# script parameters
model_id: "meta-llama/Meta-Llama-3-70b" # Hugging Face model id
# training parameters
output_dir: "./llama-3-70b-hf-no-robot" # Temporary output NOT USED
report_to: "wandb"               # report metrics to tensorboard
learning_rate: 0.00002                  # learning rate 
lr_scheduler_type: "cosine"          # learning rate scheduler
num_train_epochs: 2                   # number of training epochs
#num_steps: 1
per_device_train_batch_size: 1         # batch size per device during training
#per_device_eval_batch_size: 1          # batch size for evaluation
gradient_accumulation_steps: 1         # number of steps before performing a backward/update pass
optim: adamw_torch                     # use torch adamw optimizer
logging_steps: 1                      # log every 1 steps
save_strategy: "no"                   # save checkpoint every epoch
#evaluation_strategy: epoch             # evaluate every epoch
max_grad_norm: 1.0                     # max gradient norm
warmup_ratio: 0.05                     # warmup ratio
bf16: true                             # use bfloat16 precision
tf32: true                             # use tf32 precision
assistant_only_loss: true
gradient_checkpointing: true           # use gradient checkpointing to save memory
# FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp
fsdp: "full_shard"
fsdp_config:
  fsdp_sharding_strategy: "FULL_SHARD"
  fsdp_offload_params: true
  fsdp_backward_prefetch: "BACKWARD_PRE"
  fsdp_use_orig_params: true
  fsdp_auto_wrap_policy: "TRANSFORMER_BASED_WRAP"
  fsdp_transformer_layer_cls_to_wrap: ["LlamaDecoderLayer"]  # For LLaMA 3
  fsdp_state_dict_type: "FULL_STATE_DICT"
  fsdp_save_optimizer_state: false