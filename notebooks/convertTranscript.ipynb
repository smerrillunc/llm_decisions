{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "406795d0-8e61-4260-892a-d7347e4d6fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import difflib\n",
    "import textwrap\n",
    "import os\n",
    "\n",
    "import sys\n",
    "from rapidfuzz import process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b72fca-2108-47e3-a6e6-d5e9ddb093c6",
   "metadata": {},
   "source": [
    "### 1. Clean transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0050c442-4efd-4386-a6ec-3655c4741a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefined canonical names\n",
    "target_names = [\n",
    "    'Ellen Osborne', 'David Oberg', 'Graham Paige', 'Jonno Alcaro',\n",
    "    'Katrina Callsen', 'Kate Acuff', 'Judy Le'\n",
    "]\n",
    "\n",
    "# Known aliases → canonical name\n",
    "alias_map = {\n",
    "    \"Katherin Acuff\": \"Kate Acuff\",\n",
    "    \"Katherine Acuff\": \"Kate Acuff\",\n",
    "    \"Kate Acuff\": \"Kate Acuff\",\n",
    "    \"Jonathan Alcaro\": \"Jonno Alcaro\",\n",
    "    \"Jon Alcaro\": \"Jonno Alcaro\",\n",
    "    \"Jonno Alcaro\": \"Jonno Alcaro\"\n",
    "}\n",
    "\n",
    "# Merge all names used for matching\n",
    "known_names = list(set(target_names + list(alias_map.keys())))\n",
    "\n",
    "def normalize_name(name):\n",
    "    # Remove extra characters, insert space before capital letters if needed\n",
    "    name = re.sub(r'[^a-zA-Z ]+', '', name)  # remove punctuation\n",
    "    name = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', name)  # camelCase to spaced\n",
    "    return name.strip().title()\n",
    "\n",
    "def extract_names_from_transcript(text):\n",
    "    pattern = re.compile(r\"\\[\\d{2}:\\d{2}:\\d{2} --> \\d{2}:\\d{2}:\\d{2}\\]\\s+([^\\:]+):\")\n",
    "    return sorted(set(pattern.findall(text)))\n",
    "\n",
    "def build_name_map_auto(name_variants, known_names, alias_map, threshold=75):\n",
    "    name_map = {}\n",
    "    for name in name_variants:\n",
    "        normalized_name = normalize_name(name)\n",
    "\n",
    "        # Alias check\n",
    "        if normalized_name in alias_map:\n",
    "            name_map[name] = alias_map[normalized_name]\n",
    "            continue\n",
    "\n",
    "        # Fuzzy match\n",
    "        result = process.extractOne(normalized_name, known_names, score_cutoff=threshold)\n",
    "        if result:\n",
    "            match, _, _ = result\n",
    "            canonical = alias_map.get(match, match)\n",
    "            name_map[name] = canonical\n",
    "        else:\n",
    "            # Fallback\n",
    "            name_map[name] = normalized_name\n",
    "    return name_map\n",
    "\n",
    "def normalize_transcript(text, name_map):\n",
    "    pattern = re.compile(r\"(\\[\\d{2}:\\d{2}:\\d{2} --> \\d{2}:\\d{2}:\\d{2}\\])\\s+([^\\:]+):\")\n",
    "    \n",
    "    def replacer(match):\n",
    "        timestamp = match.group(1)\n",
    "        raw_name = match.group(2).strip()\n",
    "        canonical = name_map.get(raw_name, raw_name.title())\n",
    "        return f\"{timestamp} {canonical}:\"\n",
    "\n",
    "    return pattern.sub(replacer, text)\n",
    "\n",
    "def clean_transcript(transcript_path, out_path):\n",
    "    with open(transcript_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    detected_names = extract_names_from_transcript(text)\n",
    "    name_map = build_name_map_auto(detected_names, known_names, alias_map)\n",
    "\n",
    "    # Save name map for transparency\n",
    "    with open(\"name_map.json\", \"w\") as f:\n",
    "        json.dump(name_map, f, indent=2)\n",
    "\n",
    "    # Normalize transcript\n",
    "    cleaned = normalize_transcript(text, name_map)\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(cleaned)\n",
    "\n",
    "    print(f\"Cleaned transcript saved to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "da09400f-1414-4feb-8e84-9873c856b54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned transcript saved to: /work/users/s/m/smerrill/Albemarle/cleantranscriptsbkp/-A6v9Byfz20.txt\n",
      "Cleaned transcript saved to: /work/users/s/m/smerrill/Albemarle/cleantranscriptsbkp/5hxY73VZXok.txt\n",
      "Cleaned transcript saved to: /work/users/s/m/smerrill/Albemarle/cleantranscriptsbkp/6WYXzne6dlY.txt\n",
      "Cleaned transcript saved to: /work/users/s/m/smerrill/Albemarle/cleantranscriptsbkp/6gwrzUFYMw8.txt\n",
      "Cleaned transcript saved to: /work/users/s/m/smerrill/Albemarle/cleantranscriptsbkp/9zIGFJbKhNg.txt\n",
      "Cleaned transcript saved to: /work/users/s/m/smerrill/Albemarle/cleantranscriptsbkp/BFr19jzSYoM.txt\n",
      "Cleaned transcript saved to: /work/users/s/m/smerrill/Albemarle/cleantranscriptsbkp/HbBO4irR6AA.txt\n",
      "Cleaned transcript saved to: /work/users/s/m/smerrill/Albemarle/cleantranscriptsbkp/MgHvV_9dxdA.txt\n",
      "Cleaned transcript saved to: /work/users/s/m/smerrill/Albemarle/cleantranscriptsbkp/NSizXN8dD5g.txt\n",
      "Cleaned transcript saved to: /work/users/s/m/smerrill/Albemarle/cleantranscriptsbkp/PfwawD7a2Sg.txt\n",
      "Cleaned transcript saved to: /work/users/s/m/smerrill/Albemarle/cleantranscriptsbkp/XVE0SqP-6Zg&t.txt\n",
      "Cleaned transcript saved to: /work/users/s/m/smerrill/Albemarle/cleantranscriptsbkp/_91XXbXeQD4.txt\n",
      "Cleaned transcript saved to: /work/users/s/m/smerrill/Albemarle/cleantranscriptsbkp/acuff.txt\n",
      "Cleaned transcript saved to: /work/users/s/m/smerrill/Albemarle/cleantranscriptsbkp/blaYeycF09Q.txt\n"
     ]
    }
   ],
   "source": [
    "transcript_path = '/work/users/s/m/smerrill/Albemarle/transcriptsbkp'\n",
    "clean_transcript_path = '/work/users/s/m/smerrill/Albemarle/cleantranscriptsbkp'\n",
    "\n",
    "for transcript in all_transcripts:\n",
    "    transcript_file = os.path.join(transcript_path, transcript)\n",
    "    output_file = os.path.join(clean_transcript_path, transcript)\n",
    "    clean_transcript(transcript_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4408e25a-0262-4ba2-b3f2-d62c281cdddd",
   "metadata": {},
   "source": [
    "### 2. Make Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c5ef7b65-97e1-49bc-b46b-429d3a4871f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_transcript_to_finetune_data(\n",
    "    filename,\n",
    "    target_speaker,\n",
    "    match_threshold=0.8,\n",
    "    max_tokens=200,\n",
    "    max_context_utterances=5\n",
    "):\n",
    "    def split_long_response(response_text, max_tokens):\n",
    "        # Approximate 1 token ≈ 5 characters\n",
    "        chunks = textwrap.wrap(response_text, width=max_tokens * 5)\n",
    "        return chunks\n",
    "\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    speaker_map = {}\n",
    "    speaker_id = 1\n",
    "    dialogue = []\n",
    "    known_speakers = set()\n",
    "\n",
    "    # Step 1: Parse lines and normalize speakers\n",
    "    for line in lines:\n",
    "        match = re.match(r\"\\[\\d{2}:\\d{2}:\\d{2} --> \\d{2}:\\d{2}:\\d{2}\\] (.*?):\\s+(.*)\", line)\n",
    "        if not match:\n",
    "            continue\n",
    "        speaker, text = match.groups()\n",
    "        speaker = speaker.strip().rstrip(':')\n",
    "        text = text.strip()\n",
    "\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        known_speakers.add(speaker)\n",
    "        is_target = bool(difflib.get_close_matches(speaker, [target_speaker], n=1, cutoff=match_threshold))\n",
    "        normalized_speaker = target_speaker if is_target else speaker_map.setdefault(\n",
    "            speaker, f\"Speaker_{speaker_id}\"\n",
    "        )\n",
    "\n",
    "        if not is_target and speaker not in speaker_map:\n",
    "            speaker_id += 1\n",
    "\n",
    "        dialogue.append((normalized_speaker, text))\n",
    "\n",
    "    # Step 2: Build fine-tuning examples\n",
    "    examples = []\n",
    "    context = []\n",
    "    buffer = []\n",
    "\n",
    "    for i, (speaker, text) in enumerate(dialogue):\n",
    "        if speaker == target_speaker:\n",
    "            buffer.append(text)\n",
    "            next_speaker = dialogue[i + 1][0] if i + 1 < len(dialogue) else None\n",
    "            if next_speaker != target_speaker:\n",
    "                full_response = \" \".join(buffer)\n",
    "                response_chunks = split_long_response(full_response, max_tokens)\n",
    "\n",
    "                trimmed_context = context[-max_context_utterances:]\n",
    "\n",
    "                for chunk in response_chunks:\n",
    "                    if trimmed_context:\n",
    "                        examples.append({\n",
    "                            \"messages\": [\n",
    "                                {\"role\": \"user\", \"content\": \"\\n\".join(trimmed_context)},\n",
    "                                {\"role\": \"assistant\", \"content\": chunk}\n",
    "                            ]\n",
    "                        })\n",
    "\n",
    "                context.append(f\"{speaker}: {full_response}\")\n",
    "                buffer = []\n",
    "        else:\n",
    "            context.append(f\"{speaker}: {text}\")\n",
    "\n",
    "    return examples\n",
    "\n",
    "def save_chat_dataset(data, output_path):\n",
    "    \"\"\"\n",
    "    Save a list of chat-style message dictionaries to a JSONL or JSON file.\n",
    "    \"\"\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    print(f\"Dataset saved to: {output_path}\")\n",
    "    print(f\"Total examples: {len(data)}\")\n",
    "\n",
    "def load_jsonl_dataset(file_path):\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    print(f\"Loaded {len(data)} records from {file_path}\")\n",
    "    return data\n",
    "\n",
    "def count_context_and_content_words(dataset):\n",
    "    \"\"\"\n",
    "    Count total words in 'user' (context) and 'assistant' (content) messages.\n",
    "    \n",
    "    Args:\n",
    "        dataset (list): A list of dicts with 'messages' key, each containing role/content.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Word counts {'context_words': int, 'content_words': int}\n",
    "    \"\"\"\n",
    "    context_words = 0\n",
    "    content_words = 0\n",
    "\n",
    "    for example in dataset:\n",
    "        for msg in example.get(\"messages\", []):\n",
    "            words = re.findall(r'\\b\\w+\\b', msg[\"content\"])\n",
    "            if msg[\"role\"] == \"user\":\n",
    "                context_words += len(words)\n",
    "            elif msg[\"role\"] == \"assistant\":\n",
    "                content_words += len(words)\n",
    "\n",
    "    return {\n",
    "        \"context_words\": context_words,\n",
    "        \"content_words\": content_words\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f415a55f-8dbd-4cbf-b373-b807a2869dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_save_path = '/work/users/s/m/smerrill/Albemarle/dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0b865953-73f7-43f6-9a3f-8ab66889638a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ellen Osborne\n",
      "Dataset saved to: /work/users/s/m/smerrill/Albemarle/dataset/ellenosborne.txt\n",
      "Total examples: 45\n",
      "---------------------------------------\n",
      "David Oberg\n",
      "Dataset saved to: /work/users/s/m/smerrill/Albemarle/dataset/davidoberg.txt\n",
      "Total examples: 179\n",
      "---------------------------------------\n",
      "Graham Paige\n",
      "Dataset saved to: /work/users/s/m/smerrill/Albemarle/dataset/grahampaige.txt\n",
      "Total examples: 281\n",
      "---------------------------------------\n",
      "Jonno Alcaro\n",
      "Dataset saved to: /work/users/s/m/smerrill/Albemarle/dataset/jonnoalcaro.txt\n",
      "Total examples: 227\n",
      "---------------------------------------\n",
      "Katrina Callsen\n",
      "Dataset saved to: /work/users/s/m/smerrill/Albemarle/dataset/katrinacallsen.txt\n",
      "Total examples: 220\n",
      "---------------------------------------\n",
      "Kate Acuff\n",
      "Dataset saved to: /work/users/s/m/smerrill/Albemarle/dataset/kateacuff.txt\n",
      "Total examples: 187\n",
      "---------------------------------------\n",
      "Judy Le\n",
      "Dataset saved to: /work/users/s/m/smerrill/Albemarle/dataset/judyle.txt\n",
      "Total examples: 118\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "datasets = []\n",
    "for speaker in target_names:\n",
    "    print(speaker)\n",
    "    examples = []\n",
    "    for transcript in all_transcripts:\n",
    "        cleaned_transcript = os.path.join(clean_transcript_path, transcript)\n",
    "        examples += convert_transcript_to_finetune_data(\n",
    "                            cleaned_transcript,\n",
    "                            speaker,\n",
    "                            match_threshold=0.8, # this should be exact now...\n",
    "                            max_tokens=250,\n",
    "                            max_context_utterances=5)\n",
    "    save_name = dataset_save_path + speaker.replace(' ', '').lower() + '.txt'\n",
    "    save_chat_dataset(examples, save_name)\n",
    "    datasets.append(save_name)\n",
    "    print('---------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0a0fac-344d-4f0a-9461-078cfeed2fd1",
   "metadata": {},
   "source": [
    "### 3. Dataset Metastats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "de69558d-56f5-424e-8bde-23b4d87d635b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 45 records from /work/users/s/m/smerrill/Albemarle/dataset/ellenosborne.txt\n",
      "Loaded 179 records from /work/users/s/m/smerrill/Albemarle/dataset/davidoberg.txt\n",
      "Loaded 281 records from /work/users/s/m/smerrill/Albemarle/dataset/grahampaige.txt\n",
      "Loaded 227 records from /work/users/s/m/smerrill/Albemarle/dataset/jonnoalcaro.txt\n",
      "Loaded 220 records from /work/users/s/m/smerrill/Albemarle/dataset/katrinacallsen.txt\n",
      "Loaded 187 records from /work/users/s/m/smerrill/Albemarle/dataset/kateacuff.txt\n",
      "Loaded 118 records from /work/users/s/m/smerrill/Albemarle/dataset/judyle.txt\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "\n",
    "for speaker in target_names:\n",
    "    filename = dataset_save_path + speaker.replace(' ', '').lower() + '.txt'\n",
    "\n",
    "    try:\n",
    "        tmp = load_json_dataset(filename)\n",
    "        word_counts = count_context_and_content_words(tmp)\n",
    "\n",
    "        rows.append({\n",
    "            \"speaker\": speaker,\n",
    "            \"context_words\": word_counts[\"context_words\"],\n",
    "            \"content_words\": word_counts[\"content_words\"]\n",
    "        })\n",
    "    except FileNotFoundError:\n",
    "        print(f\"⚠️ File not found: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {filename}: {e}\")\n",
    "\n",
    "df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "08a8df0d-5157-4090-9e4d-79c6c2077dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>context_words</th>\n",
       "      <th>content_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ellen Osborne</td>\n",
       "      <td>2296</td>\n",
       "      <td>1166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>David Oberg</td>\n",
       "      <td>12156</td>\n",
       "      <td>7055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Graham Paige</td>\n",
       "      <td>19221</td>\n",
       "      <td>9311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jonno Alcaro</td>\n",
       "      <td>16432</td>\n",
       "      <td>10072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Katrina Callsen</td>\n",
       "      <td>16889</td>\n",
       "      <td>14112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Kate Acuff</td>\n",
       "      <td>12920</td>\n",
       "      <td>6008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Judy Le</td>\n",
       "      <td>9222</td>\n",
       "      <td>6077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           speaker  context_words  content_words\n",
       "0    Ellen Osborne           2296           1166\n",
       "1      David Oberg          12156           7055\n",
       "2     Graham Paige          19221           9311\n",
       "3     Jonno Alcaro          16432          10072\n",
       "4  Katrina Callsen          16889          14112\n",
       "5       Kate Acuff          12920           6008\n",
       "6          Judy Le           9222           6077"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df1217d-c2f7-4846-9b82-b5bf126d22df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c915a14b-9725-496b-9cf7-6e965af7af01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speaker",
   "language": "python",
   "name": "speaker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
