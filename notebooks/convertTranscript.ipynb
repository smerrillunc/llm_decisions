{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "406795d0-8e61-4260-892a-d7347e4d6fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import difflib\n",
    "import textwrap\n",
    "import os\n",
    "\n",
    "import sys\n",
    "from rapidfuzz import process\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b72fca-2108-47e3-a6e6-d5e9ddb093c6",
   "metadata": {},
   "source": [
    "### 1. Clean transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0050c442-4efd-4386-a6ec-3655c4741a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefined canonical names\n",
    "target_names = [\n",
    "    'Ellen Osborne', 'David Oberg', 'Graham Paige', 'Jonno Alcaro',\n",
    "    'Katrina Callsen', 'Kate Acuff', 'Judy Le'\n",
    "]\n",
    "\n",
    "# Known aliases → canonical name\n",
    "alias_map = {\n",
    "    \"Katherin Acuff\": \"Kate Acuff\",\n",
    "    \"Katherine Acuff\": \"Kate Acuff\",\n",
    "    \"Kate Acuff\": \"Kate Acuff\",\n",
    "    \"Jonathan Alcaro\": \"Jonno Alcaro\",\n",
    "    \"Jon Alcaro\": \"Jonno Alcaro\",\n",
    "    \"Jonno Alcaro\": \"Jonno Alcaro\"\n",
    "}\n",
    "\n",
    "# Merge all names used for matching\n",
    "known_names = list(set(target_names + list(alias_map.keys())))\n",
    "\n",
    "def normalize_name(name):\n",
    "    # Remove extra characters, insert space before capital letters if needed\n",
    "    name = re.sub(r'[^a-zA-Z ]+', '', name)  # remove punctuation\n",
    "    name = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', name)  # camelCase to spaced\n",
    "    return name.strip().title()\n",
    "\n",
    "def extract_names_from_transcript(text):\n",
    "    pattern = re.compile(r\"\\[\\d{2}:\\d{2}:\\d{2} --> \\d{2}:\\d{2}:\\d{2}\\]\\s+([^\\:]+):\")\n",
    "    return sorted(set(pattern.findall(text)))\n",
    "\n",
    "def build_name_map_auto(name_variants, known_names, alias_map, threshold=75):\n",
    "    name_map = {}\n",
    "    for name in name_variants:\n",
    "        normalized_name = normalize_name(name)\n",
    "\n",
    "        # Alias check\n",
    "        if normalized_name in alias_map:\n",
    "            name_map[name] = alias_map[normalized_name]\n",
    "            continue\n",
    "\n",
    "        # Fuzzy match\n",
    "        result = process.extractOne(normalized_name, known_names, score_cutoff=threshold)\n",
    "        if result:\n",
    "            match, _, _ = result\n",
    "            canonical = alias_map.get(match, match)\n",
    "            name_map[name] = canonical\n",
    "        else:\n",
    "            # Fallback\n",
    "            name_map[name] = normalized_name\n",
    "    return name_map\n",
    "\n",
    "def normalize_transcript(text, name_map):\n",
    "    pattern = re.compile(r\"(\\[\\d{2}:\\d{2}:\\d{2} --> \\d{2}:\\d{2}:\\d{2}\\])\\s+([^\\:]+):\")\n",
    "    \n",
    "    def replacer(match):\n",
    "        timestamp = match.group(1)\n",
    "        raw_name = match.group(2).strip()\n",
    "        canonical = name_map.get(raw_name, raw_name.title())\n",
    "        return f\"{timestamp} {canonical}:\"\n",
    "\n",
    "    return pattern.sub(replacer, text)\n",
    "\n",
    "def clean_transcript(transcript_path, out_path):\n",
    "    with open(transcript_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    detected_names = extract_names_from_transcript(text)\n",
    "    name_map = build_name_map_auto(detected_names, known_names, alias_map)\n",
    "\n",
    "    # Save name map for transparency\n",
    "    with open(\"name_map.json\", \"w\") as f:\n",
    "        json.dump(name_map, f, indent=2)\n",
    "\n",
    "    # Normalize transcript\n",
    "    cleaned = normalize_transcript(text, name_map)\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(cleaned)\n",
    "\n",
    "    print(f\"Cleaned transcript saved to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da09400f-1414-4feb-8e84-9873c856b54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned transcript saved to: /work/users/s/m/smerrill/Albemarle/cleantranscripts/-A6v9Byfz20.txt\n",
      "Cleaned transcript saved to: /work/users/s/m/smerrill/Albemarle/cleantranscripts/3BtZN2Tye08.txt\n",
      "Cleaned transcript saved to: /work/users/s/m/smerrill/Albemarle/cleantranscripts/5YMkxWBgdtY.txt\n",
      "Cleaned transcript saved to: /work/users/s/m/smerrill/Albemarle/cleantranscripts/5hxY73VZXok.txt\n",
      "Cleaned transcript saved to: /work/users/s/m/smerrill/Albemarle/cleantranscripts/6WYXzne6dlY.txt\n",
      "Cleaned transcript saved to: /work/users/s/m/smerrill/Albemarle/cleantranscripts/6gwrzUFYMw8.txt\n",
      "Cleaned transcript saved to: /work/users/s/m/smerrill/Albemarle/cleantranscripts/8TdTe--0CUs.txt\n",
      "Cleaned transcript saved to: /work/users/s/m/smerrill/Albemarle/cleantranscripts/9zIGFJbKhNg.txt\n",
      "Cleaned transcript saved to: /work/users/s/m/smerrill/Albemarle/cleantranscripts/BFr19jzSYoM.txt\n",
      "Cleaned transcript saved to: /work/users/s/m/smerrill/Albemarle/cleantranscripts/HbBO4irR6AA.txt\n",
      "Cleaned transcript saved to: /work/users/s/m/smerrill/Albemarle/cleantranscripts/MgHvV_9dxdA.txt\n",
      "Cleaned transcript saved to: /work/users/s/m/smerrill/Albemarle/cleantranscripts/b0dOdikDv0M.txt\n",
      "Cleaned transcript saved to: /work/users/s/m/smerrill/Albemarle/cleantranscripts/blaYeycF09Q.txt\n",
      "Cleaned transcript saved to: /work/users/s/m/smerrill/Albemarle/cleantranscripts/crHvtYkt7jA.txt\n",
      "Cleaned transcript saved to: /work/users/s/m/smerrill/Albemarle/cleantranscripts/hhvce_UUc4A.txt\n",
      "Cleaned transcript saved to: /work/users/s/m/smerrill/Albemarle/cleantranscripts/le.txt\n",
      "Cleaned transcript saved to: /work/users/s/m/smerrill/Albemarle/cleantranscripts/o-mxBguJmlU.txt\n"
     ]
    }
   ],
   "source": [
    "transcript_path = '/work/users/s/m/smerrill/Albemarle/transcripts'\n",
    "all_transcripts = os.listdir(transcript_path)\n",
    "clean_transcript_path = '/work/users/s/m/smerrill/Albemarle/cleantranscripts'\n",
    "os.makedirs(clean_transcript_path, exist_ok=True)\n",
    "\n",
    "for transcript in all_transcripts:\n",
    "    transcript_file = os.path.join(transcript_path, transcript)\n",
    "    output_file = os.path.join(clean_transcript_path, transcript)\n",
    "    clean_transcript(transcript_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4408e25a-0262-4ba2-b3f2-d62c281cdddd",
   "metadata": {},
   "source": [
    "### 2. Make Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c5ef7b65-97e1-49bc-b46b-429d3a4871f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_transcript_to_finetune_data(\n",
    "    filename,\n",
    "    target_speaker,\n",
    "    match_threshold=0.8,\n",
    "    max_tokens=200,\n",
    "    max_context_utterances=5,\n",
    "    min_response_words=3\n",
    "):\n",
    "    def split_long_response(response_text, max_tokens):\n",
    "        # Approximate 1 token ≈ 5 characters\n",
    "        chunks = textwrap.wrap(response_text, width=max_tokens * 5)\n",
    "        return chunks\n",
    "\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    speaker_map = {}\n",
    "    speaker_id = 1\n",
    "    dialogue = []\n",
    "    known_speakers = set()\n",
    "\n",
    "    # Step 1: Parse lines and normalize speakers\n",
    "    for line in lines:\n",
    "        match = re.match(r\"\\[\\d{2}:\\d{2}:\\d{2} --> \\d{2}:\\d{2}:\\d{2}\\] (.*?):\\s+(.*)\", line)\n",
    "        if not match:\n",
    "            continue\n",
    "        speaker, text = match.groups()\n",
    "        speaker = speaker.strip().rstrip(':')\n",
    "        text = text.strip()\n",
    "\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        known_speakers.add(speaker)\n",
    "        is_target = bool(difflib.get_close_matches(speaker, [target_speaker], n=1, cutoff=match_threshold))\n",
    "        normalized_speaker = target_speaker if is_target else speaker_map.setdefault(\n",
    "            speaker, f\"Speaker_{speaker_id}\"\n",
    "        )\n",
    "\n",
    "        if not is_target and speaker not in speaker_map:\n",
    "            speaker_id += 1\n",
    "\n",
    "        dialogue.append((normalized_speaker, text))\n",
    "\n",
    "    # Step 2: Build fine-tuning examples\n",
    "    examples = []\n",
    "    context = []\n",
    "    buffer = []\n",
    "\n",
    "    for i, (speaker, text) in enumerate(dialogue):\n",
    "        if speaker == target_speaker:\n",
    "            buffer.append(text)\n",
    "            next_speaker = dialogue[i + 1][0] if i + 1 < len(dialogue) else None\n",
    "            if next_speaker != target_speaker:\n",
    "                full_response = \" \".join(buffer)\n",
    "                response_chunks = split_long_response(full_response, max_tokens)\n",
    "                trimmed_context = context[-max_context_utterances:]\n",
    "\n",
    "                for chunk in response_chunks:\n",
    "                    word_count = len(re.findall(r'\\b\\w+\\b', chunk))\n",
    "                    if word_count >= min_response_words and trimmed_context:\n",
    "                        examples.append({\n",
    "                            \"messages\": [\n",
    "                                {\"role\": \"user\", \"content\": \"\\n\".join(trimmed_context)},\n",
    "                                {\"role\": \"assistant\", \"content\": chunk}\n",
    "                            ]\n",
    "                        })\n",
    "\n",
    "                context.append(f\"{speaker}: {full_response}\")\n",
    "                buffer = []\n",
    "        else:\n",
    "            context.append(f\"{speaker}: {text}\")\n",
    "\n",
    "    return examples\n",
    "\n",
    "def save_chat_dataset(data, output_path):\n",
    "    \"\"\"\n",
    "    Save a list of chat-style message dictionaries to a JSONL or JSON file.\n",
    "    \"\"\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    print(f\"Dataset saved to: {output_path}\")\n",
    "    print(f\"Total examples: {len(data)}\")\n",
    "\n",
    "def load_chat_dataset(input_path):\n",
    "    \"\"\"\n",
    "    Load a chat-style message dataset from a JSON or JSONL file.\n",
    "    \n",
    "    Returns:\n",
    "        data (list): A list of message dictionaries.\n",
    "    \"\"\"\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        if input_path.endswith(\".jsonl\"):\n",
    "            data = [json.loads(line) for line in f]\n",
    "        else:  # Assume .json\n",
    "            data = json.load(f)\n",
    "    \n",
    "    print(f\"Dataset loaded from: {input_path}\")\n",
    "    print(f\"Total examples: {len(data)}\")\n",
    "    return data\n",
    "\n",
    "def count_context_and_content_words(dataset):\n",
    "    \"\"\"\n",
    "    Analyze word statistics in 'user' (context) and 'assistant' (content) messages.\n",
    "\n",
    "    Args:\n",
    "        dataset (list): A list of dicts with 'messages' key, each containing role/content.\n",
    "\n",
    "    Returns:\n",
    "        dict: Word stats including total, average, min, and max for context and content.\n",
    "    \"\"\"\n",
    "    context_word_counts = []\n",
    "    content_word_counts = []\n",
    "\n",
    "    for example in dataset:\n",
    "        context_count = 0\n",
    "        content_count = 0\n",
    "        for msg in example.get(\"messages\", []):\n",
    "            words = re.findall(r'\\b\\w+\\b', msg[\"content\"])\n",
    "            if msg[\"role\"] == \"user\":\n",
    "                context_count += len(words)\n",
    "            elif msg[\"role\"] == \"assistant\":\n",
    "                content_count += len(words)\n",
    "        context_word_counts.append(context_count)\n",
    "        content_word_counts.append(content_count)\n",
    "\n",
    "    total_examples = len(dataset)\n",
    "\n",
    "    return {\n",
    "        \"total_context_words\": sum(context_word_counts),\n",
    "        \"total_content_words\": sum(content_word_counts),\n",
    "        \"avg_context_words\": sum(context_word_counts) / total_examples if total_examples else 0,\n",
    "        \"avg_content_words\": sum(content_word_counts) / total_examples if total_examples else 0,\n",
    "        \"min_context_words\": min(context_word_counts) if context_word_counts else 0,\n",
    "        \"max_context_words\": max(context_word_counts) if context_word_counts else 0,\n",
    "        \"min_content_words\": min(content_word_counts) if content_word_counts else 0,\n",
    "        \"max_content_words\": max(content_word_counts) if content_word_counts else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f415a55f-8dbd-4cbf-b373-b807a2869dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_save_path = '/work/users/s/m/smerrill/Albemarle/dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0b865953-73f7-43f6-9a3f-8ab66889638a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ellen Osborne\n",
      "Dataset saved to: /work/users/s/m/smerrill/Albemarle/dataset/ellenosborne.txt\n",
      "Total examples: 91\n",
      "---------------------------------------\n",
      "David Oberg\n",
      "Dataset saved to: /work/users/s/m/smerrill/Albemarle/dataset/davidoberg.txt\n",
      "Total examples: 262\n",
      "---------------------------------------\n",
      "Graham Paige\n",
      "Dataset saved to: /work/users/s/m/smerrill/Albemarle/dataset/grahampaige.txt\n",
      "Total examples: 767\n",
      "---------------------------------------\n",
      "Jonno Alcaro\n",
      "Dataset saved to: /work/users/s/m/smerrill/Albemarle/dataset/jonnoalcaro.txt\n",
      "Total examples: 351\n",
      "---------------------------------------\n",
      "Katrina Callsen\n",
      "Dataset saved to: /work/users/s/m/smerrill/Albemarle/dataset/katrinacallsen.txt\n",
      "Total examples: 345\n",
      "---------------------------------------\n",
      "Kate Acuff\n",
      "Dataset saved to: /work/users/s/m/smerrill/Albemarle/dataset/kateacuff.txt\n",
      "Total examples: 321\n",
      "---------------------------------------\n",
      "Judy Le\n",
      "Dataset saved to: /work/users/s/m/smerrill/Albemarle/dataset/judyle.txt\n",
      "Total examples: 226\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "datasets = []\n",
    "for speaker in target_names:\n",
    "    print(speaker)\n",
    "    examples = []\n",
    "    for transcript in all_transcripts:\n",
    "        cleaned_transcript = os.path.join(clean_transcript_path, transcript)\n",
    "        examples += convert_transcript_to_finetune_data(\n",
    "                            cleaned_transcript,\n",
    "                            speaker,\n",
    "                            match_threshold=0.8, # this should be exact now...\n",
    "                            max_tokens=250,\n",
    "                            max_context_utterances=5)\n",
    "    save_name = dataset_save_path + speaker.replace(' ', '').lower() + '.txt'\n",
    "    save_chat_dataset(examples, save_name)\n",
    "    datasets.append(save_name)\n",
    "    print('---------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0a0fac-344d-4f0a-9461-078cfeed2fd1",
   "metadata": {},
   "source": [
    "### 3. Dataset Metastats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "de69558d-56f5-424e-8bde-23b4d87d635b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded from: /work/users/s/m/smerrill/Albemarle/dataset/ellenosborne.txt\n",
      "Total examples: 91\n",
      "Dataset loaded from: /work/users/s/m/smerrill/Albemarle/dataset/davidoberg.txt\n",
      "Total examples: 262\n",
      "Dataset loaded from: /work/users/s/m/smerrill/Albemarle/dataset/grahampaige.txt\n",
      "Total examples: 767\n",
      "Dataset loaded from: /work/users/s/m/smerrill/Albemarle/dataset/jonnoalcaro.txt\n",
      "Total examples: 351\n",
      "Dataset loaded from: /work/users/s/m/smerrill/Albemarle/dataset/katrinacallsen.txt\n",
      "Total examples: 345\n",
      "Dataset loaded from: /work/users/s/m/smerrill/Albemarle/dataset/kateacuff.txt\n",
      "Total examples: 321\n",
      "Dataset loaded from: /work/users/s/m/smerrill/Albemarle/dataset/judyle.txt\n",
      "Total examples: 226\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "\n",
    "for speaker in target_names:\n",
    "    filename = dataset_save_path + speaker.replace(' ', '').lower() + '.txt'\n",
    "\n",
    "    try:\n",
    "        tmp = load_chat_dataset(filename)\n",
    "        word_counts = count_context_and_content_words(tmp)\n",
    "\n",
    "        rows.append({\n",
    "            \"speaker\": speaker,\n",
    "            \"total_context_words\": word_counts[\"total_context_words\"],\n",
    "            \"total_content_words\": word_counts[\"total_content_words\"],\n",
    "            \"avg_context_words\": word_counts[\"avg_context_words\"],\n",
    "            \"avg_content_words\": word_counts[\"avg_content_words\"],\n",
    "            \"min_context_words\": word_counts[\"min_context_words\"],\n",
    "            \"max_context_words\": word_counts[\"max_context_words\"],\n",
    "            \"min_content_words\": word_counts[\"min_content_words\"],\n",
    "            \"max_content_words\": word_counts[\"max_content_words\"]\n",
    "        })\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "df = pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "08a8df0d-5157-4090-9e4d-79c6c2077dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>total_context_words</th>\n",
       "      <th>total_content_words</th>\n",
       "      <th>avg_context_words</th>\n",
       "      <th>avg_content_words</th>\n",
       "      <th>min_context_words</th>\n",
       "      <th>max_context_words</th>\n",
       "      <th>min_content_words</th>\n",
       "      <th>max_content_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ellen Osborne</td>\n",
       "      <td>3978</td>\n",
       "      <td>1917</td>\n",
       "      <td>43.714286</td>\n",
       "      <td>21.065934</td>\n",
       "      <td>10</td>\n",
       "      <td>192</td>\n",
       "      <td>3</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>David Oberg</td>\n",
       "      <td>16260</td>\n",
       "      <td>9752</td>\n",
       "      <td>62.061069</td>\n",
       "      <td>37.221374</td>\n",
       "      <td>10</td>\n",
       "      <td>371</td>\n",
       "      <td>3</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Graham Paige</td>\n",
       "      <td>43744</td>\n",
       "      <td>21268</td>\n",
       "      <td>57.032595</td>\n",
       "      <td>27.728814</td>\n",
       "      <td>10</td>\n",
       "      <td>590</td>\n",
       "      <td>3</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jonno Alcaro</td>\n",
       "      <td>24554</td>\n",
       "      <td>14737</td>\n",
       "      <td>69.954416</td>\n",
       "      <td>41.985755</td>\n",
       "      <td>8</td>\n",
       "      <td>921</td>\n",
       "      <td>3</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Katrina Callsen</td>\n",
       "      <td>25495</td>\n",
       "      <td>17090</td>\n",
       "      <td>73.898551</td>\n",
       "      <td>49.536232</td>\n",
       "      <td>12</td>\n",
       "      <td>435</td>\n",
       "      <td>3</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Kate Acuff</td>\n",
       "      <td>19471</td>\n",
       "      <td>9360</td>\n",
       "      <td>60.657321</td>\n",
       "      <td>29.158879</td>\n",
       "      <td>10</td>\n",
       "      <td>292</td>\n",
       "      <td>3</td>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Judy Le</td>\n",
       "      <td>14684</td>\n",
       "      <td>8299</td>\n",
       "      <td>64.973451</td>\n",
       "      <td>36.721239</td>\n",
       "      <td>10</td>\n",
       "      <td>419</td>\n",
       "      <td>3</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           speaker  total_context_words  total_content_words  \\\n",
       "0    Ellen Osborne                 3978                 1917   \n",
       "1      David Oberg                16260                 9752   \n",
       "2     Graham Paige                43744                21268   \n",
       "3     Jonno Alcaro                24554                14737   \n",
       "4  Katrina Callsen                25495                17090   \n",
       "5       Kate Acuff                19471                 9360   \n",
       "6          Judy Le                14684                 8299   \n",
       "\n",
       "   avg_context_words  avg_content_words  min_context_words  max_context_words  \\\n",
       "0          43.714286          21.065934                 10                192   \n",
       "1          62.061069          37.221374                 10                371   \n",
       "2          57.032595          27.728814                 10                590   \n",
       "3          69.954416          41.985755                  8                921   \n",
       "4          73.898551          49.536232                 12                435   \n",
       "5          60.657321          29.158879                 10                292   \n",
       "6          64.973451          36.721239                 10                419   \n",
       "\n",
       "   min_content_words  max_content_words  \n",
       "0                  3                206  \n",
       "1                  3                270  \n",
       "2                  3                251  \n",
       "3                  3                263  \n",
       "4                  3                255  \n",
       "5                  3                243  \n",
       "6                  3                247  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f0dcb3-1c9c-4ae8-8534-c4ead79fcedd",
   "metadata": {},
   "source": [
    "### Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c0f15ea0-1efc-46cd-967e-7d18a1927374",
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_path = '/nas/longleaf/home/smerrill/notebooks/LLM/data/final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ae76af9c-9775-4034-85dc-989d94161a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to: /work/users/s/m/smerrill/Albemarle/dataset/synth_kateacuff.txt\n",
      "Total examples: 40\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(synth_path, 'acuff_qa_pairs_ft.json'), 'r', encoding='utf-8') as f:\n",
    "    acuff1 = json.load(f)\n",
    "\n",
    "with open(os.path.join(synth_path, 'YT_acuff_qa_pairs_ft.json'), 'r', encoding='utf-8') as f:\n",
    "    acuff2 = json.load(f)\n",
    "acuff = acuff1 + acuff2\n",
    "\n",
    "save_chat_dataset(acuff, dataset_save_path + 'synth_kateacuff.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5870d6df-8204-46aa-9045-b09faf383d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to: /work/users/s/m/smerrill/Albemarle/dataset/synth_ellenosborne.txt\n",
      "Total examples: 41\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(synth_path, 'osborne_qa_pairs_ft.json'), 'r', encoding='utf-8') as f:\n",
    "    osborne1 = json.load(f)\n",
    "\n",
    "with open(os.path.join(synth_path, 'YT_osborne_qa_pairs_ft.json'), 'r', encoding='utf-8') as f:\n",
    "    osborne2 = json.load(f)\n",
    "osborne = osborne1 + osborne2\n",
    "\n",
    "save_chat_dataset(osborne, dataset_save_path + 'synth_ellenosborne.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9350a437-e249-4ab9-aec8-8a7d40a23286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to: /work/users/s/m/smerrill/Albemarle/dataset/synth_grahampaige.txt\n",
      "Total examples: 50\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(synth_path, 'paige_qa_pairs_ft.json'), 'r', encoding='utf-8') as f:\n",
    "    paige1 = json.load(f)\n",
    "\n",
    "with open(os.path.join(synth_path, 'YT_paige_qa_pairs_ft.json'), 'r', encoding='utf-8') as f:\n",
    "    paige2 = json.load(f)\n",
    "paige = paige1 + paige2\n",
    "\n",
    "save_chat_dataset(paige, dataset_save_path + 'synth_grahampaige.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9388f914-7990-49c4-9909-1083bee84ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to: /work/users/s/m/smerrill/Albemarle/dataset/synth_judyle.txt\n",
      "Total examples: 43\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(synth_path, 'le_qa_pairs_ft.json'), 'r', encoding='utf-8') as f:\n",
    "    le1 = json.load(f)\n",
    "\n",
    "with open(os.path.join(synth_path, 'YT_le_qa_pairs_ft.json'), 'r', encoding='utf-8') as f:\n",
    "    le2 = json.load(f)\n",
    "le = le1 + le2\n",
    "\n",
    "save_chat_dataset(le, dataset_save_path + 'synth_judyle.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4c0466ac-0f8e-4cd4-b0b3-99c0eb83898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bd800922-d677-427b-92e3-d67203e2c9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = count_context_and_content_words(acuff)\n",
    "\n",
    "rows.append({\n",
    "    \"speaker\": 'Kate Acuff',\n",
    "    \"total_context_words\": word_counts[\"total_context_words\"],\n",
    "    \"total_content_words\": word_counts[\"total_content_words\"],\n",
    "    \"avg_context_words\": word_counts[\"avg_context_words\"],\n",
    "    \"avg_content_words\": word_counts[\"avg_content_words\"],\n",
    "    \"min_context_words\": word_counts[\"min_context_words\"],\n",
    "    \"max_context_words\": word_counts[\"max_context_words\"],\n",
    "    \"min_content_words\": word_counts[\"min_content_words\"],\n",
    "    \"max_content_words\": word_counts[\"max_content_words\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a606a74a-8106-4c5f-a015-5bbb98815e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = count_context_and_content_words(osborne)\n",
    "\n",
    "rows.append({\n",
    "    \"speaker\": 'Ellen Osborne',\n",
    "    \"total_context_words\": word_counts[\"total_context_words\"],\n",
    "    \"total_content_words\": word_counts[\"total_content_words\"],\n",
    "    \"avg_context_words\": word_counts[\"avg_context_words\"],\n",
    "    \"avg_content_words\": word_counts[\"avg_content_words\"],\n",
    "    \"min_context_words\": word_counts[\"min_context_words\"],\n",
    "    \"max_context_words\": word_counts[\"max_context_words\"],\n",
    "    \"min_content_words\": word_counts[\"min_content_words\"],\n",
    "    \"max_content_words\": word_counts[\"max_content_words\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "af154555-29f7-4b07-8f37-cef217d12e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = count_context_and_content_words(paige)\n",
    "\n",
    "rows.append({\n",
    "    \"speaker\": 'Graham Paige',\n",
    "    \"total_context_words\": word_counts[\"total_context_words\"],\n",
    "    \"total_content_words\": word_counts[\"total_content_words\"],\n",
    "    \"avg_context_words\": word_counts[\"avg_context_words\"],\n",
    "    \"avg_content_words\": word_counts[\"avg_content_words\"],\n",
    "    \"min_context_words\": word_counts[\"min_context_words\"],\n",
    "    \"max_context_words\": word_counts[\"max_context_words\"],\n",
    "    \"min_content_words\": word_counts[\"min_content_words\"],\n",
    "    \"max_content_words\": word_counts[\"max_content_words\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "824add64-913a-4039-a26e-ec6cf8904223",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = count_context_and_content_words(le)\n",
    "\n",
    "rows.append({\n",
    "    \"speaker\": 'Judy Le',\n",
    "    \"total_context_words\": word_counts[\"total_context_words\"],\n",
    "    \"total_content_words\": word_counts[\"total_content_words\"],\n",
    "    \"avg_context_words\": word_counts[\"avg_context_words\"],\n",
    "    \"avg_content_words\": word_counts[\"avg_content_words\"],\n",
    "    \"min_context_words\": word_counts[\"min_context_words\"],\n",
    "    \"max_context_words\": word_counts[\"max_context_words\"],\n",
    "    \"min_content_words\": word_counts[\"min_content_words\"],\n",
    "    \"max_content_words\": word_counts[\"max_content_words\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "678328ba-bb30-4d57-9290-2f8ea744a8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ea843837-f37e-47cc-9968-1590f3795268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>total_context_words</th>\n",
       "      <th>total_content_words</th>\n",
       "      <th>avg_context_words</th>\n",
       "      <th>avg_content_words</th>\n",
       "      <th>min_context_words</th>\n",
       "      <th>max_context_words</th>\n",
       "      <th>min_content_words</th>\n",
       "      <th>max_content_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Judy Le</td>\n",
       "      <td>462</td>\n",
       "      <td>445</td>\n",
       "      <td>10.744186</td>\n",
       "      <td>10.348837</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Graham Paige</td>\n",
       "      <td>529</td>\n",
       "      <td>354</td>\n",
       "      <td>10.580000</td>\n",
       "      <td>7.080000</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ellen Osborne</td>\n",
       "      <td>422</td>\n",
       "      <td>409</td>\n",
       "      <td>10.292683</td>\n",
       "      <td>9.975610</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kate Acuff</td>\n",
       "      <td>468</td>\n",
       "      <td>216</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>5.400000</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         speaker  total_context_words  total_content_words  avg_context_words  \\\n",
       "0        Judy Le                  462                  445          10.744186   \n",
       "1   Graham Paige                  529                  354          10.580000   \n",
       "2  Ellen Osborne                  422                  409          10.292683   \n",
       "3     Kate Acuff                  468                  216          11.700000   \n",
       "\n",
       "   avg_content_words  min_context_words  max_context_words  min_content_words  \\\n",
       "0          10.348837                  6                 17                  1   \n",
       "1           7.080000                  4                 16                  1   \n",
       "2           9.975610                  4                 19                  1   \n",
       "3           5.400000                  7                 17                  1   \n",
       "\n",
       "   max_content_words  \n",
       "0                 27  \n",
       "1                 17  \n",
       "2                 35  \n",
       "3                 17  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e971c1-211b-443e-877e-b64fd4ece3a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speaker",
   "language": "python",
   "name": "speaker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
