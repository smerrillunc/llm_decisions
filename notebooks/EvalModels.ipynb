{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "72fb4989-2236-4cfc-b2c2-77d474d9f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "from unsloth import FastLanguageModel\n",
    "from typing import List, Tuple\n",
    "import random\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import json\n",
    "import evaluate  # instead of datasets.load_metric\n",
    "from tqdm import tqdm\n",
    "from unsloth import FastLanguageModel\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e8d0aa-f1f2-4ed2-bc30-51414b1be83f",
   "metadata": {},
   "source": [
    "### Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "895b7bed-8e4e-4663-b3ec-1e2e620d5017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name, max_seq_length=2048, dtype=None, load_in_4bit=True):\n",
    "    \"\"\"\n",
    "    Load a fine-tuned model and tokenizer with optional quantization and inference setup.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name (str): Path to the model directory.\n",
    "    - max_seq_length (int): Maximum sequence length the model should support.\n",
    "    - dtype (torch.dtype or None): Set to None for auto detection, or specify torch.float16 or torch.bfloat16.\n",
    "    - load_in_4bit (bool): Whether to use 4-bit quantization for memory efficiency.\n",
    "\n",
    "    Returns:\n",
    "    - model (PreTrainedModel): The loaded and prepared model.\n",
    "    - tokenizer (PreTrainedTokenizer): The corresponding tokenizer.\n",
    "    \"\"\"\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=model_name,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "994364d6-e2f0-4a21-ae2f-8cf0b8332ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = {\n",
    "    \"ellenosborne\": \"/work/users/s/m/smerrill/Albemarle/trained_models/ellenosborne\",\n",
    "    \"grahampaige\": \"/work/users/s/m/smerrill/Albemarle/trained_models/grahampaige\",\n",
    "    \"judyle\": \"/work/users/s/m/smerrill/Albemarle/trained_models/judyle\",\n",
    "    \"kateacuff\": \"/work/users/s/m/smerrill/Albemarle/trained_models/kateacuff\",\n",
    "    \"katrinacallsen\": \"/work/users/s/m/smerrill/Albemarle/trained_models/katrinacallsen\",\n",
    "    \"davidoberg\": \"/work/users/s/m/smerrill/Albemarle/trained_models/davidoberg\",\n",
    "    \"jonnoalcaro\": \"/work/users/s/m/smerrill/Albemarle/trained_models/jonnoalcaro\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "be52b45e-954e-4b95-a322-cab20be51439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: ellenosborne\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unsloth: Failed to load model. Both AutoConfig and PeftConfig loading failed.\n\nAutoConfig error: Unrecognized model in /work/users/s/m/smerrill/Albemarle/trained_models/ellenosborne. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v3, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glm4, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth\n\nPeftConfig error: Can't find 'adapter_config.json' at '/work/users/s/m/smerrill/Albemarle/trained_models/ellenosborne'\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, path \u001b[38;5;129;01min\u001b[39;00m model_paths.items():\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     model, tokenizer = \u001b[43mload_model_and_tokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_in_4bit\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     loaded_models[name] = (model, tokenizer)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Baseline not fine-tuned\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mload_model_and_tokenizer\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_model_and_tokenizer\u001b[39m(model_name, max_seq_length=\u001b[32m2048\u001b[39m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, load_in_4bit=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m    Load a fine-tuned model and tokenizer with optional quantization and inference setup.\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[33;03m    - tokenizer (PreTrainedTokenizer): The corresponding tokenizer.\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     model, tokenizer = \u001b[43mFastLanguageModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_in_4bit\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     FastLanguageModel.for_inference(model)\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model, tokenizer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/users/s/m/smerrill/.conda/envs/unsloth_env/lib/python3.11/site-packages/unsloth/models/loader.py:232\u001b[39m, in \u001b[36mFastLanguageModel.from_pretrained\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, *args, **kwargs)\u001b[39m\n\u001b[32m    226\u001b[39m     \u001b[38;5;66;03m# Create a combined error message showing both failures\u001b[39;00m\n\u001b[32m    227\u001b[39m     combined_error = (\n\u001b[32m    228\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnsloth: Failed to load model. Both AutoConfig and PeftConfig loading failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    229\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAutoConfig error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mautoconfig_error\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    230\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPeftConfig error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeft_error\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    231\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(combined_error)\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    235\u001b[39m \u001b[38;5;66;03m# Get base model for PEFT:\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Unsloth: Failed to load model. Both AutoConfig and PeftConfig loading failed.\n\nAutoConfig error: Unrecognized model in /work/users/s/m/smerrill/Albemarle/trained_models/ellenosborne. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v3, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glm4, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth\n\nPeftConfig error: Can't find 'adapter_config.json' at '/work/users/s/m/smerrill/Albemarle/trained_models/ellenosborne'\n\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "loaded_models = {}\n",
    "\n",
    "for name, path in model_paths.items():\n",
    "    print(f\"Loading model: {name}\")\n",
    "    model, tokenizer = load_model_and_tokenizer(\n",
    "        model_name=path,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit\n",
    "    )\n",
    "    loaded_models[name] = (model, tokenizer)\n",
    "\n",
    "\n",
    "# Baseline not fine-tuned\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "loaded_models['baseline'] = (model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380f2296-32a3-47cf-aff8-3d8aa059d197",
   "metadata": {},
   "source": [
    "### Dataprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2243d081-096b-48a0-93f7-f204101243d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(\n",
    "    member: str,\n",
    "    test_size: float = 0.2,\n",
    "    seed: int = 42,\n",
    "    data_path: str = \"/work/users/s/m/smerrill/Albemarle/dataset\",\n",
    ") -> Tuple[List[dict], List[dict]]:\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and test sets, including synthetic data in training.\n",
    "\n",
    "    Args:\n",
    "        member: Board member name to select dataset files.\n",
    "        test_size: Fraction of real data for testing (between 0 and 1).\n",
    "        seed: Random seed for reproducibility.\n",
    "        data_path: Base path for dataset files.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (train_data, test_data).\n",
    "    \"\"\"\n",
    "    real_data, synth_data = [], []\n",
    "\n",
    "    member_files = {\n",
    "        \"kateacuff\": (\"kateacuff.txt\", \"synth_kateacuff.txt\"),\n",
    "        \"ellenosborne\": (\"ellenosborne.txt\", \"synth_ellenosborne.txt\"),\n",
    "        \"grahampaige\": (\"grahampaige.txt\", \"synth_grahampaige.txt\"),\n",
    "        \"judyle\": (\"judyle.txt\", \"synth_judyle.txt\"),\n",
    "        \"katrinacallsen\": (\"katrinacallsen.txt\", None),\n",
    "        \"davidoberg\": (\"davidoberg.txt\", None),\n",
    "        \"jonnoalcaro\": (\"jonnoalcaro.txt\", None),\n",
    "    }\n",
    "\n",
    "    if member not in member_files:\n",
    "        raise ValueError(f\"Unknown member: {member}\")\n",
    "\n",
    "    real_file, synth_file = member_files[member]\n",
    "\n",
    "    real_data = load_chat_dataset(os.path.join(data_path, real_file))\n",
    "    if synth_file:\n",
    "        synth_data = load_chat_dataset(os.path.join(data_path, synth_file))\n",
    "\n",
    "    if not (0 < test_size < 1):\n",
    "        raise ValueError(\"test_size must be between 0 and 1.\")\n",
    "\n",
    "    random.seed(seed)\n",
    "    shuffled_real = real_data.copy()\n",
    "    random.shuffle(shuffled_real)\n",
    "\n",
    "    split_index = int(len(shuffled_real) * (1 - test_size))\n",
    "    train_data = shuffled_real[:split_index] + synth_data\n",
    "    test_data = shuffled_real[split_index:]\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "def combine_conversations(dataset: List[dict]) -> Dataset:\n",
    "    \"\"\"\n",
    "    Combine messages into conversation pairs (user + assistant).\n",
    "\n",
    "    Args:\n",
    "        dataset: List of dicts with keys 'role' and 'content'.\n",
    "\n",
    "    Returns:\n",
    "        Huggingface Dataset with combined conversations.\n",
    "    \"\"\"\n",
    "    new_data = []\n",
    "    i = 0\n",
    "    while i < len(dataset):\n",
    "        if i + 1 < len(dataset):\n",
    "            convo = [\n",
    "                {\"role\": dataset[i][\"role\"], \"content\": dataset[i][\"content\"]},\n",
    "                {\"role\": dataset[i + 1][\"role\"], \"content\": dataset[i + 1][\"content\"]},\n",
    "            ]\n",
    "            new_data.append({\"conversations\": convo})\n",
    "            i += 2\n",
    "        else:\n",
    "            convo = [{\"role\": dataset[i][\"role\"], \"content\": dataset[i][\"content\"]}]\n",
    "            new_data.append({\"conversations\": convo})\n",
    "            i += 1\n",
    "    return Dataset.from_list(new_data)\n",
    "\n",
    "\n",
    "def replace_system_message(example: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Replace system message in text if format allows.\n",
    "\n",
    "    Args:\n",
    "        example: Single dataset example with \"text\" field.\n",
    "        custom_system_message: The system message string to replace with.\n",
    "\n",
    "    Returns:\n",
    "        Modified example dict.\n",
    "    \"\"\"\n",
    "    parts = example[\"text\"].split(\"<|eot_id|>\", 1)\n",
    "    if len(parts) == 2:\n",
    "        new_text = custom_system_message + parts[1]\n",
    "    else:\n",
    "        new_text = example[\"text\"]\n",
    "    return {\"text\": new_text}\n",
    "\n",
    "\n",
    "def formatting_prompts_func(examples: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Apply chat template formatting to conversations.\n",
    "\n",
    "    Args:\n",
    "        examples: Batch of examples from the dataset.\n",
    "        tokenizer: Tokenizer instance with apply_chat_template method.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with formatted \"text\".\n",
    "    \"\"\"\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False).removeprefix(tokenizer.bos_token)\n",
    "        for convo in convos\n",
    "    ]\n",
    "    return {\"text\": texts}\n",
    "\n",
    "def load_chat_dataset(input_path: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Load a chat-style message dataset from a JSON or JSONL file.\n",
    "\n",
    "    Args:\n",
    "        input_path: Path to the dataset file.\n",
    "\n",
    "    Returns:\n",
    "        List of message dicts.\n",
    "    \"\"\"\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        if input_path.endswith(\".jsonl\"):\n",
    "            data = [json.loads(line) for line in f]\n",
    "        else:  # Assume .json\n",
    "            data = json.load(f)\n",
    "\n",
    "    #logger.info(f\"Loaded dataset from {input_path} with {len(data)} examples\")\n",
    "    return data\n",
    "\n",
    "def convert_to_chat_format(data: List[dict]) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Convert dataset entries to chat format with alternating roles user/assistant.\n",
    "\n",
    "    Args:\n",
    "        data: List of dicts with keys 'prompt' and 'response'.\n",
    "\n",
    "    Returns:\n",
    "        List of dicts with 'role' and 'content'.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for item in data:\n",
    "        result.append({\"role\": \"user\", \"content\": item[\"prompt\"]})\n",
    "        result.append({\"role\": \"assistant\", \"content\": item[\"response\"]})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240e4d7e-eaa2-4f30-8634-a806d53e9961",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7039c381-427c-46aa-9b70-ed8f7e9296b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 60/60 [00:00<00:00, 8659.66 examples/s]\n",
      "Map: 100%|██████████| 60/60 [00:00<00:00, 13493.74 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded for ellenosborne\n",
      "Evaluating 60 Examples of kateacuff\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [02:26<00:00,  2.44s/it]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Results ---\n",
      "Dataset: kateacuff\n",
      "Agent Name: Kate Acuff\n",
      "BLEU: 0.0102\n",
      "ROUGE-L: 0.0975\n",
      "BERTScore F1: 0.8259\n",
      "Model Loaded for grahampaige\n",
      "Evaluating 60 Examples of kateacuff\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [02:54<00:00,  2.90s/it]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Results ---\n",
      "Dataset: kateacuff\n",
      "Agent Name: Kate Acuff\n",
      "BLEU: 0.0000\n",
      "ROUGE-L: 0.0662\n",
      "BERTScore F1: 0.7944\n",
      "Model Loaded for judyle\n",
      "Evaluating 60 Examples of kateacuff\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [02:36<00:00,  2.62s/it]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Results ---\n",
      "Dataset: kateacuff\n",
      "Agent Name: Kate Acuff\n",
      "BLEU: 0.0000\n",
      "ROUGE-L: 0.0988\n",
      "BERTScore F1: 0.8176\n",
      "Model Loaded for kateacuff\n",
      "Evaluating 60 Examples of kateacuff\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [02:39<00:00,  2.66s/it]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Results ---\n",
      "Dataset: kateacuff\n",
      "Agent Name: Kate Acuff\n",
      "BLEU: 0.0106\n",
      "ROUGE-L: 0.0819\n",
      "BERTScore F1: 0.7960\n",
      "Model Loaded for katrinacallsen\n",
      "Evaluating 60 Examples of kateacuff\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [02:33<00:00,  2.55s/it]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Results ---\n",
      "Dataset: kateacuff\n",
      "Agent Name: Kate Acuff\n",
      "BLEU: 0.0000\n",
      "ROUGE-L: 0.0794\n",
      "BERTScore F1: 0.8120\n",
      "Model Loaded for baseline\n",
      "Evaluating 60 Examples of kateacuff\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:32<00:00,  1.54s/it]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Results ---\n",
      "Dataset: kateacuff\n",
      "Agent Name: Kate Acuff\n",
      "BLEU: 0.0077\n",
      "ROUGE-L: 0.1021\n",
      "BERTScore F1: 0.8260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 14/14 [00:00<00:00, 3542.27 examples/s]\n",
      "Map: 100%|██████████| 14/14 [00:00<00:00, 5329.97 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded for ellenosborne\n",
      "Evaluating 14 Examples of ellenosborne\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:35<00:00,  2.52s/it]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Results ---\n",
      "Dataset: ellenosborne\n",
      "Agent Name: Ellen Osborne\n",
      "BLEU: 0.0127\n",
      "ROUGE-L: 0.1166\n",
      "BERTScore F1: 0.8221\n",
      "Model Loaded for grahampaige\n",
      "Evaluating 14 Examples of ellenosborne\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:36<00:00,  2.62s/it]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Results ---\n",
      "Dataset: ellenosborne\n",
      "Agent Name: Ellen Osborne\n",
      "BLEU: 0.0092\n",
      "ROUGE-L: 0.0680\n",
      "BERTScore F1: 0.8038\n",
      "Model Loaded for judyle\n",
      "Evaluating 14 Examples of ellenosborne\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:29<00:00,  2.10s/it]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Results ---\n",
      "Dataset: ellenosborne\n",
      "Agent Name: Ellen Osborne\n",
      "BLEU: 0.0000\n",
      "ROUGE-L: 0.0820\n",
      "BERTScore F1: 0.8100\n",
      "Model Loaded for kateacuff\n",
      "Evaluating 14 Examples of ellenosborne\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:40<00:00,  2.88s/it]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Results ---\n",
      "Dataset: ellenosborne\n",
      "Agent Name: Ellen Osborne\n",
      "BLEU: 0.0000\n",
      "ROUGE-L: 0.0772\n",
      "BERTScore F1: 0.8029\n",
      "Model Loaded for katrinacallsen\n",
      "Evaluating 14 Examples of ellenosborne\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:33<00:00,  2.42s/it]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Results ---\n",
      "Dataset: ellenosborne\n",
      "Agent Name: Ellen Osborne\n",
      "BLEU: 0.0000\n",
      "ROUGE-L: 0.0813\n",
      "BERTScore F1: 0.8047\n",
      "Model Loaded for baseline\n",
      "Evaluating 14 Examples of ellenosborne\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:21<00:00,  1.51s/it]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Results ---\n",
      "Dataset: ellenosborne\n",
      "Agent Name: Ellen Osborne\n",
      "BLEU: 0.0000\n",
      "ROUGE-L: 0.1053\n",
      "BERTScore F1: 0.8323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 180/180 [00:00<00:00, 12877.37 examples/s]\n",
      "Map: 100%|██████████| 180/180 [00:00<00:00, 20383.24 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded for ellenosborne\n",
      "Evaluating 180 Examples of grahampaige\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [05:48<00:00,  1.93s/it]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Results ---\n",
      "Dataset: grahampaige\n",
      "Agent Name: Graham Paige\n",
      "BLEU: 0.0102\n",
      "ROUGE-L: 0.0893\n",
      "BERTScore F1: 0.8225\n",
      "Model Loaded for grahampaige\n",
      "Evaluating 180 Examples of grahampaige\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [06:59<00:00,  2.33s/it]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Results ---\n",
      "Dataset: grahampaige\n",
      "Agent Name: Graham Paige\n",
      "BLEU: 0.0092\n",
      "ROUGE-L: 0.0659\n",
      "BERTScore F1: 0.8055\n",
      "Model Loaded for judyle\n",
      "Evaluating 180 Examples of grahampaige\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 11/180 [00:26<06:45,  2.40s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Store metrics for all evaluations\n",
    "metrics_list = []\n",
    "\n",
    "# This first loop is to loop over datasets\n",
    "for agent in agents:\n",
    "    agent_lower = agent.replace(' ', '').lower()\n",
    "    dataset_name = agent_lower\n",
    "    message = f\"\"\"You are {agent}. Fill in the next response to the conversation by continuing the dialogue naturally. Only write what {agent} would say next — do not write for other speakers.\n",
    "\n",
    "            Example:\n",
    "            user:  \n",
    "            {agent_lower}: Good evening, everyone. Let's get started with the agenda.  \n",
    "            Speaker_1: Thanks, {agent_lower}. I just had a quick question about the minutes from last time.  \n",
    "            assistant:  \n",
    "            {agent_lower}: Sure, go ahead with your question.\"\"\"\n",
    "\n",
    "    custom_system_message = f\"<|start_header_id|>system<|end_header_id|>\\n\\n{message}\\n<|eot_id|>\"\n",
    "\n",
    "    # Dataset preparation\n",
    "    _, test_data = train_test_split(dataset_name)\n",
    "    test_data = convert_to_chat_format(test_data)\n",
    "    test_data = combine_conversations(test_data)\n",
    "    test_data = test_data.map(formatting_prompts_func, batched=True)\n",
    "    test_data = test_data.map(replace_system_message)\n",
    "\n",
    "    # For each agent evaluate performance on this dataset\n",
    "    for model_name in loaded_models.keys():\n",
    "\n",
    "        try:\n",
    "            model, tokenizer= loaded_models[model_name]\n",
    "            print(f\"Model Loaded for {model_name}\")\n",
    "        except:\n",
    "            print(f\"COULD NOT LOAD MODEL FOR {model_name}\")\n",
    "            continue\n",
    "\n",
    "        # Initialize metrics\n",
    "        bleu = evaluate.load(\"bleu\")\n",
    "        rouge = evaluate.load(\"rouge\")\n",
    "        bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "        # For this dataset the speaker prefix we want corresponds to the dataset\n",
    "        speaker_prefix = tokenizer.encode(f\"{agent_lower}:\", return_tensors=\"pt\").to(\"cuda\")[:, 1:]\n",
    "\n",
    "        generated_texts = []\n",
    "        reference_texts = []\n",
    "\n",
    "        print(f\"Evaluating {len(test_data)} Examples of {dataset_name}\")\n",
    "        for example in tqdm(test_data):\n",
    "            messages = [example['conversations'][:-1]]\n",
    "            reference = example['conversations'][-1]['content']\n",
    "\n",
    "            inputs = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=True,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(\"cuda\")\n",
    "\n",
    "            inputs = inputs[:, 26:]  # Remove old system prompt\n",
    "            custom_prompt = \"<|begin_of_text|>\" + custom_system_message\n",
    "            custom_prompt_tokens = tokenizer.encode(custom_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "            inputs = torch.cat([custom_prompt_tokens, inputs], dim=1)\n",
    "            inputs = torch.cat([inputs, speaker_prefix], dim=1)\n",
    "\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs,\n",
    "                max_new_tokens=128,\n",
    "                use_cache=True,\n",
    "                temperature=1.5,\n",
    "                min_p=0.1,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "            generated = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "            generated_texts.append(generated.strip())\n",
    "            reference_texts.append(reference.strip())\n",
    "\n",
    "        # Compute metrics\n",
    "        bleu_score = bleu.compute(predictions=generated_texts, references=[[r] for r in reference_texts])\n",
    "        rouge_score = rouge.compute(predictions=generated_texts, references=reference_texts)\n",
    "        bertscore_result = bertscore.compute(predictions=generated_texts, references=reference_texts, lang=\"en\")\n",
    "\n",
    "        # Average BERTScore F1\n",
    "        avg_bertscore_f1 = sum(bertscore_result['f1']) / len(bertscore_result['f1'])\n",
    "\n",
    "        # Save metrics\n",
    "        metrics_list.append({\n",
    "            \"ModelName\": model_name,\n",
    "            \"dataset\": dataset_name,\n",
    "            \"BLEU\": bleu_score[\"bleu\"],\n",
    "            \"ROUGE-L\": rouge_score[\"rougeL\"],\n",
    "            \"BERTScore_F1\": avg_bertscore_f1\n",
    "        })\n",
    "\n",
    "        print(\"\\n--- Evaluation Results ---\")\n",
    "        print(f\"Dataset: {dataset_name}\")\n",
    "        print(f\"BLEU: {bleu_score['bleu']:.4f}\")\n",
    "        print(f\"ROUGE-L: {rouge_score['rougeL']:.4f}\")\n",
    "        print(f\"BERTScore F1: {avg_bertscore_f1:.4f}\")\n",
    "\n",
    "# Create DataFrame from metrics\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "print(\"\\n=== All Evaluation Metrics ===\")\n",
    "print(metrics_df)\n",
    "\n",
    "# Optionally save to CSV\n",
    "# metrics_df.to_csv(\"evaluation_metrics.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d0385af1-cd71-472c-9d6c-cc0206b900d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(metrics_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b5f1a610-2b27-46fe-9ca6-6dcd70d6a2b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ModelName</th>\n",
       "      <th>dataset</th>\n",
       "      <th>BLEU</th>\n",
       "      <th>ROUGE-L</th>\n",
       "      <th>BERTScore_F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ellenosborne</td>\n",
       "      <td>ellenosborne</td>\n",
       "      <td>0.012726</td>\n",
       "      <td>0.116645</td>\n",
       "      <td>0.822130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>grahampaige</td>\n",
       "      <td>ellenosborne</td>\n",
       "      <td>0.009202</td>\n",
       "      <td>0.067962</td>\n",
       "      <td>0.803843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>judyle</td>\n",
       "      <td>ellenosborne</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081982</td>\n",
       "      <td>0.810025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kateacuff</td>\n",
       "      <td>ellenosborne</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077208</td>\n",
       "      <td>0.802936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>katrinacallsen</td>\n",
       "      <td>ellenosborne</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081305</td>\n",
       "      <td>0.804726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>baseline</td>\n",
       "      <td>ellenosborne</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105304</td>\n",
       "      <td>0.832277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ModelName       dataset      BLEU   ROUGE-L  BERTScore_F1\n",
       "6     ellenosborne  ellenosborne  0.012726  0.116645      0.822130\n",
       "7      grahampaige  ellenosborne  0.009202  0.067962      0.803843\n",
       "8           judyle  ellenosborne  0.000000  0.081982      0.810025\n",
       "9        kateacuff  ellenosborne  0.000000  0.077208      0.802936\n",
       "10  katrinacallsen  ellenosborne  0.000000  0.081305      0.804726\n",
       "11        baseline  ellenosborne  0.000000  0.105304      0.832277"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df[metrics_df.dataset == 'ellenosborne']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2510cf66-87bb-4534-8c82-b14e4f2beb6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def88d6e-c644-49e0-9383-6a3200ba1f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "unsloth_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
