{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imwJhjjYPSr0"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9uwqwgkzkC_"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "    !pip install synthetic-data-kit==0.0.3\n",
        "else:\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    !pip install --no-deps unsloth vllm\n",
        "    !pip install synthetic-data-kit==0.0.3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGMgEY2GzkDA"
      },
      "outputs": [],
      "source": [
        "#@title Colab Extra Install { display-mode: \"form\" }\n",
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    !pip install --no-deps unsloth vllm\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    # Skip restarting message in Colab\n",
        "    import sys, re, requests; modules = list(sys.modules.keys())\n",
        "    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft \"trl==0.15.2\" triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "\n",
        "    # vLLM requirements - vLLM breaks Colab due to reinstalling numpy\n",
        "    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
        "    with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
        "        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
        "    !pip install -r vllm_requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTkQP7ryzkDB"
      },
      "source": [
        "## Start VLM server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ym8UA1PRiXsa",
        "outputId": "f6943a36-4570-40f4-b3c5-921ea3c352e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Your GPU cannot handle sequence lengths of 256 due to limited GPU memory.\n",
            "Unsloth: Your GPU can only handle approximately the maximum sequence length of 256.\n",
            "Unsloth: Using dtype = torch.float16 for vLLM.\n",
            "Unsloth: vLLM loading unsloth/Llama-3.2-3B-Instruct with actual GPU utilization = 11.91%\n",
            "Unsloth: Your GPU has CUDA compute capability 7.5 with VRAM = 14.74 GB.\n",
            "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 256. Num Sequences = 128.\n",
            "Unsloth: vLLM's KV Cache can use up to 0.0 GB. Also swap space = 0 GB.\n",
            "vLLM STDOUT: INFO 05-21 18:36:42 [__init__.py:239] Automatically detected platform cuda.\n",
            "vLLM STDOUT: INFO 05-21 18:36:53 [api_server.py:1043] vLLM API server version 0.8.5.post1\n",
            "vLLM STDOUT: INFO 05-21 18:36:53 [api_server.py:1044] args: Namespace(subparser='serve', model_tag='unsloth/Llama-3.2-3B-Instruct', config='', host=None, port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='unsloth/Llama-3.2-3B-Instruct', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, load_format='auto', download_dir=None, model_loader_extra_config={}, use_tqdm_on_load=True, config_format=<ConfigFormat.AUTO: 'auto'>, dtype='float16', max_model_len=256, guided_decoding_backend='auto', reasoning_parser=None, logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, disable_custom_all_reduce=False, block_size=None, gpu_memory_utilization=0.11913249779522278, swap_space=0.0, kv_cache_dtype='auto', num_gpu_blocks_override=None, enable_prefix_caching=True, prefix_caching_hash_algo='builtin', cpu_offload_gb=0, calculate_kv_scales=False, disable_sliding_window=False, use_v2_block_manager=True, seed=0, max_logprobs=0, disable_log_stats=True, quantization=None, rope_scaling=None, rope_theta=None, hf_token=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config={}, limit_mm_per_prompt={}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=None, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=None, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', speculative_config=None, ignore_patterns=[], served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, max_num_batched_tokens=256, max_num_seqs=128, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, num_lookahead_slots=0, scheduler_delay_factor=0.0, preemption_mode=None, num_scheduler_steps=1, multi_step_stream_outputs=True, scheduling_policy='fcfs', enable_chunked_prefill=None, disable_chunked_mm_input=False, scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config={\"level\":3,\"splitting_ops\":[]}, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, additional_config=None, enable_reasoning=False, disable_cascade_attn=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False, dispatch_function=<function ServeSubcommand.cmd at 0x7d5d5e4f2de0>)\n",
            "vLLM STDOUT: WARNING 05-21 18:36:54 [config.py:2972] Casting torch.bfloat16 to torch.float16.\n",
            "vLLM STDOUT: INFO 05-21 18:37:09 [config.py:717] This model supports multiple tasks: {'score', 'embed', 'classify', 'reward', 'generate'}. Defaulting to 'generate'.\n",
            "vLLM STDOUT: WARNING 05-21 18:37:09 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0.\n",
            "vLLM STDOUT: INFO 05-21 18:37:10 [api_server.py:246] Started engine process with PID 24373\n",
            "vLLM STDOUT: INFO 05-21 18:37:16 [__init__.py:239] Automatically detected platform cuda.\n",
            "vLLM STDOUT: INFO 05-21 18:37:22 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='unsloth/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='unsloth/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=256, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/Llama-3.2-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":128}, use_cached_outputs=True,\n",
            "vLLM STDOUT: INFO 05-21 18:37:24 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
            "vLLM STDOUT: INFO 05-21 18:37:24 [cuda.py:289] Using XFormers backend.\n",
            "vLLM STDOUT: INFO 05-21 18:37:25 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
            "vLLM STDOUT: INFO 05-21 18:37:25 [model_runner.py:1108] Starting to load model unsloth/Llama-3.2-3B-Instruct...\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448] CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 80.12 MiB is free. Process 7378 has 100.00 MiB memory in use. Process 120878 has 12.69 GiB memory in use. Process 313115 has 1.87 GiB memory in use. Of the allocated memory 1.75 GiB is allocated by PyTorch, and 9.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448] Traceback (most recent call last):\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]   File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/multiprocessing/engine.py\", line 436, in run_mp_engine\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]     engine = MQLLMEngine.from_vllm_config(\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]   File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/multiprocessing/engine.py\", line 128, in from_vllm_config\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]     return cls(\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]            ^^^^\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]   File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/multiprocessing/engine.py\", line 82, in __init__\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]     self.engine = LLMEngine(*args, **kwargs)\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]   File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/llm_engine.py\", line 275, in __init__\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]     self.model_executor = executor_class(vllm_config=vllm_config)\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]   File \"/usr/local/lib/python3.11/dist-packages/vllm/executor/executor_base.py\", line 52, in __init__\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]     self._init_executor()\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]   File \"/usr/local/lib/python3.11/dist-packages/vllm/executor/uniproc_executor.py\", line 47, in _init_executor\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]     self.collective_rpc(\"load_model\")\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]   File \"/usr/local/lib/python3.11/dist-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]     answer = run_method(self.driver_worker, method, args, kwargs)\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]   File \"/usr/local/lib/python3.11/dist-packages/vllm/utils.py\", line 2456, in run_method\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]     return func(*args, **kwargs)\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]            ^^^^^^^^^^^^^^^^^^^^^\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]   File \"/usr/local/lib/python3.11/dist-packages/vllm/worker/worker.py\", line 203, in load_model\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]     self.model_runner.load_model()\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]   File \"/usr/local/lib/python3.11/dist-packages/vllm/worker/model_runner.py\", line 1111, in load_model\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]     self.model = get_model(vllm_config=self.vllm_config)\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]     return loader.load_model(vllm_config=vllm_config)\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/loader.py\", line 452, in load_model\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]     model = _initialize_model(vllm_config=vllm_config)\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/loader.py\", line 133, in _initialize_model\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]     return model_class(vllm_config=vllm_config, prefix=prefix)\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/llama.py\", line 496, in __init__\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]     self.model = self._init_model(vllm_config=vllm_config,\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/llama.py\", line 542, in _init_model\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]     return LlamaModel(vllm_config=vllm_config,\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]   File \"/usr/local/lib/python3.11/dist-packages/vllm/compilation/decorators.py\", line 151, in __init__\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/llama.py\", line 321, in __init__\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]     self.start_layer, self.end_layer, self.layers = make_layers(\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]                                                     ^^^^^^^^^^^^\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/utils.py\", line 609, in make_layers\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]     [PPMissingLayer() for _ in range(start_layer)] + [\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]                                                      ^\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/utils.py\", line 610, in <listcomp>\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/llama.py\", line 323, in <lambda>\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]     lambda prefix: layer_type(config=config,\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/llama.py\", line 254, in __init__\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]     self.mlp = LlamaMLP(\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]                ^^^^^^^^^\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/llama.py\", line 70, in __init__\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]     self.gate_up_proj = MergedColumnParallelLinear(\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/linear.py\", line 544, in __init__\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]     super().__init__(input_size=input_size,\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/linear.py\", line 409, in __init__\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]     self.quant_method.create_weights(\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]   File \"/usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/linear.py\", line 189, in create_weights\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]     weight = Parameter(torch.empty(sum(output_partition_sizes),\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_device.py\", line 104, in __torch_function__\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]     return func(*args, **kwargs)\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448]            ^^^^^^^^^^^^^^^^^^^^^\n",
            "vLLM STDOUT: ERROR 05-21 18:37:25 [engine.py:448] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 80.12 MiB is free. Process 7378 has 100.00 MiB memory in use. Process 120878 has 12.69 GiB memory in use. Process 313115 has 1.87 GiB memory in use. Of the allocated memory 1.75 GiB is allocated by PyTorch, and 9.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Stdout stream ended before readiness message detected.\n"
          ]
        }
      ],
      "source": [
        "from unsloth.dataprep import SyntheticDataKit\n",
        "\n",
        "generator = SyntheticDataKit.from_pretrained(\n",
        "    # Choose any model from https://huggingface.co/unsloth\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    max_seq_length = 2048, # Longer sequence lengths will be slower!\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef_MnK575tr2"
      },
      "source": [
        "## Generate QA Pairs + Auto clean data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q487TNby-nwT"
      },
      "outputs": [],
      "source": [
        "generator.prepare_qa_generation(\n",
        "    output_folder = \"data\", # Output location of synthetic data\n",
        "    temperature = 0.7, # Higher temp makes more diverse datases\n",
        "    top_p = 0.95,\n",
        "    overlap = 64, # Overlap portion during chunking\n",
        "    max_generation_tokens = 512, # Can increase for longer QA pairs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sanity Checks"
      ],
      "metadata": {
        "id": "Dub1F7r7M1B1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2gQZcr_Wp94",
        "outputId": "a6e1ff9d-819d-4155-b75d-b1cf172cc4dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K\u001b[31mL VLLM server is not available at \u001b[0m\u001b[4;94mhttp://localhost:8000/v1\u001b[0m\n",
            "\u001b[2KError: \u001b[1;35mHTTPConnectionPool\u001b[0m\u001b[1m(\u001b[0m\u001b[33mhost\u001b[0m=\u001b[32m'localhost'\u001b[0m, \u001b[33mport\u001b[0m=\u001b[1;36m8000\u001b[0m\u001b[1m)\u001b[0m: Read timed out. \u001b[1m(\u001b[0mread \n",
            "\u001b[33mtimeout\u001b[0m=\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2K\n",
            "\u001b[33mTo start the server, run:\u001b[0m\n",
            "\u001b[2K\u001b[1;34mvllm serve meta-llama/Llama-\u001b[0m\u001b[1;36m3.3\u001b[0m\u001b[1;34m-70B-Instruct --port \u001b[0m\u001b[1;36m8000\u001b[0m\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Checking VLLM server at http://localhost:8000/v1...\n",
            "\u001b[1A\u001b[2K"
          ]
        }
      ],
      "source": [
        "!synthetic-data-kit system-check"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import requests\n",
        "\n",
        "url = \"http://localhost:8000/v1/chat/completions\"\n",
        "\n",
        "payload = {\n",
        "    \"model\": \"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    \"messages\": [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
        "    ],\n",
        "    \"temperature\": 0.7,\n",
        "    \"max_tokens\": 100\n",
        "}\n",
        "\n",
        "response = requests.post(url, json=payload)\n",
        "\n",
        "# Pretty-print the result\n",
        "if response.status_code == 200:\n",
        "    print(response.json()[\"choices\"][0][\"message\"][\"content\"])\n",
        "else:\n",
        "    print(f\"Error {response.status_code}: {response.text}\")\n"
      ],
      "metadata": {
        "id": "qvWo1SCEM0fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ingest"
      ],
      "metadata": {
        "id": "Vqyzd_kqM8se"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!synthetic-data-kit ingest /content/acuff.pdf\n",
        "!synthetic-data-kit ingest /content/le.pdf\n",
        "!synthetic-data-kit ingest /content/paige.pdf\n",
        "!synthetic-data-kit ingest /content/osborne.pdf\n",
        "\n",
        "!synthetic-data-kit ingest /content/YT_acuff.pdf\n",
        "!synthetic-data-kit ingest /content/YT_le.pdf\n",
        "!synthetic-data-kit ingest /content/YT_paige.pdf\n",
        "!synthetic-data-kit ingest /content/YT_osborne.pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uV7PDLkAAQrK",
        "outputId": "71be733b-1cbe-4f85-cd7f-97083e57607c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\u001b[32m⠋\u001b[0m Processing /content/acuff.pdf...\r\u001b[2K\u001b[32m⠙\u001b[0m Processing /content/acuff.pdf...\r\u001b[2K\u001b[32m⠹\u001b[0m Processing /content/acuff.pdf...\n",
            "\u001b[1A\u001b[2K\u001b[32m Text successfully extracted to \u001b[0m\u001b[1;32mdata/output/acuff.txt\u001b[0m\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Processing /content/le.pdf...\n",
            "\u001b[1A\u001b[2K\u001b[32m Text successfully extracted to \u001b[0m\u001b[1;32mdata/output/le.txt\u001b[0m\n",
            "\u001b[2K\u001b[32m⠙\u001b[0m Processing /content/paige.pdf...\n",
            "\u001b[1A\u001b[2K\u001b[32m Text successfully extracted to \u001b[0m\u001b[1;32mdata/output/paige.txt\u001b[0m\n",
            "\u001b[2K\u001b[32m⠹\u001b[0m Processing /content/osborne.pdf...\n",
            "\u001b[1A\u001b[2K\u001b[32m Text successfully extracted to \u001b[0m\u001b[1;32mdata/output/osborne.txt\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate"
      ],
      "metadata": {
        "id": "LxjCc2ObM_Qz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Custom Bios text"
      ],
      "metadata": {
        "id": "qzx4uKjoQTzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!synthetic-data-kit \\\n",
        "        -c synthetic_data_kit_config.yaml \\\n",
        "        create /content/data/output/acuff.txt  \\\n",
        "        --num-pairs 25 \\\n",
        "        --type \"qa\"\n",
        "!synthetic-data-kit \\\n",
        "        -c synthetic_data_kit_config.yaml \\\n",
        "        create /content/data/output/le.txt  \\\n",
        "        --num-pairs 25 \\\n",
        "        --type \"qa\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SkxDiqdFzqq",
        "outputId": "aca6647f-da8a-4027-beb8-49a9cd011f77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2KProcessing 2 chunks to generate QA pairs...\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 25 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/acuff_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/acuff_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from /content/data/output/acuff.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/acuff_qa_pairs.json\u001b[0m\n",
            "\u001b[2KProcessing 2 chunks to generate QA pairs...\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 25 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/le_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/le_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from /content/data/output/le.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/le_qa_pairs.json\u001b[0m\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from /content/data/output/paige.txt...\n",
            "\u001b[1A\u001b[2K\u001b[31mL Error: Failed to get completion after \u001b[0m\u001b[1;36m3\u001b[0m\u001b[31m attempts: \u001b[0m\n",
            "\u001b[1;35mHTTPConnectionPool\u001b[0m\u001b[1;31m(\u001b[0m\u001b[33mhost\u001b[0m\u001b[31m=\u001b[0m\u001b[32m'localhost'\u001b[0m\u001b[31m, \u001b[0m\u001b[33mport\u001b[0m\u001b[31m=\u001b[0m\u001b[1;36m8000\u001b[0m\u001b[1;31m)\u001b[0m\u001b[31m: Read timed out. \u001b[0m\u001b[1;31m(\u001b[0m\u001b[31mread \u001b[0m\n",
            "\u001b[33mtimeout\u001b[0m\u001b[31m=\u001b[0m\u001b[1;36m180\u001b[0m\u001b[1;31m)\u001b[0m\n",
            "\u001b[31mL Error: VLLM server not available at \u001b[0m\u001b[4;94mhttp://localhost:8000/v1\u001b[0m\n",
            "\u001b[33mPlease start the VLLM server with:\u001b[0m\n",
            "\u001b[1;34mvllm serve unsloth/Llama-\u001b[0m\u001b[1;36m3.2\u001b[0m\u001b[1;34m-3B-Instruct\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!synthetic-data-kit \\\n",
        "        -c synthetic_data_kit_config.yaml \\\n",
        "        create /content/data/output/paige.txt  \\\n",
        "        --num-pairs 25 \\\n",
        "        --type \"qa\"\n",
        "!synthetic-data-kit \\\n",
        "        -c synthetic_data_kit_config.yaml \\\n",
        "        create /content/data/output/osborne.txt  \\\n",
        "        --num-pairs 25 \\\n",
        "        --type \"qa\""
      ],
      "metadata": {
        "id": "jnrh9hhdAQkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Youtube Transcripts"
      ],
      "metadata": {
        "id": "Tz8dAqEnQRoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!synthetic-data-kit \\\n",
        "        -c synthetic_data_kit_config.yaml \\\n",
        "        create /content/data/output/YT_acuff.txt  \\\n",
        "        --num-pairs 25 \\\n",
        "        --type \"qa\"\n",
        "!synthetic-data-kit \\\n",
        "        -c synthetic_data_kit_config.yaml \\\n",
        "        create /content/data/output/YT_le.txt  \\\n",
        "        --num-pairs 25 \\\n",
        "        --type \"qa\""
      ],
      "metadata": {
        "id": "zpRsWKPuQEeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!synthetic-data-kit \\\n",
        "        -c synthetic_data_kit_config.yaml \\\n",
        "        create /content/data/output/YT_paige.txt  \\\n",
        "        --num-pairs 25 \\\n",
        "        --type \"qa\"\n",
        "!synthetic-data-kit \\\n",
        "        -c synthetic_data_kit_config.yaml \\\n",
        "        create /content/data/output/YT_osborne.txt  \\\n",
        "        --num-pairs 25 \\\n",
        "        --type \"qa\""
      ],
      "metadata": {
        "id": "m1Y37FksQKec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AScJ5-vAOjYj"
      },
      "source": [
        "### Convert to HF format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9Um4Z8SqUTB",
        "outputId": "d7880fb6-7548-45f0-9112-9837094285b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l\u001b[32m⠋\u001b[0m Converting data/generated/arxiv_org_0_qa_pairs.json to ft format with json \n",
            "storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/arxiv_org_0_qa_pairs_ft.json\u001b[0m\n",
            "\u001b[?25l\u001b[32m⠋\u001b[0m Converting data/generated/arxiv_org_1_qa_pairs.json to ft format with json \n",
            "storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/arxiv_org_1_qa_pairs_ft.json\u001b[0m\n",
            "\u001b[?25l\u001b[32m⠋\u001b[0m Converting data/generated/arxiv_org_2_qa_pairs.json to ft format with json \n",
            "storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/arxiv_org_2_qa_pairs_ft.json\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "qa_pairs_filenames = os.listdir('/content/data/generated')\n",
        "for filename in qa_pairs_filenames:\n",
        "    !synthetic-data-kit \\\n",
        "        -c synthetic_data_kit_config.yaml \\\n",
        "        save-as {filename} -f ft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8qgTjywzgl6",
        "outputId": "f54b04cd-9586-42ff-8276-c4c54496560b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to terminate the VLLM server gracefully...\n",
            "Server terminated gracefully.\n"
          ]
        }
      ],
      "source": [
        "#generator.cleanup()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}