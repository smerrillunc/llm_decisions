{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "118ef1e5-3de6-4da6-be29-e3b196dac443",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/smerrill/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pyDataverse.api import NativeApi\n",
    "from pyDataverse.api import DataAccessApi\n",
    "import requests\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "from pyannote.audio import Pipeline\n",
    "import os\n",
    "from pyannote.core import Segment\n",
    "from IPython.display import YouTubeVideo\n",
    "import whisperx\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"PATH\"] = \"/work/users/s/m/smerrill/ffmpeg-7.0.2-amd64-static:\" + os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c13d47-f897-4cfd-89e2-2a12d73b3943",
   "metadata": {},
   "source": [
    "### Download Interview Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ed60acf-aeb8-45c2-80fd-df191d4a5125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_youtube_audio(video_id: str, output_dir: str = \".\") -> str:\n",
    "    \"\"\"\n",
    "    Downloads and converts the best available audio stream from a YouTube video to MP3 format.\n",
    "    Saves the output as {video_id}.mp3.\n",
    "    Requires yt-dlp and ffmpeg.\n",
    "    \"\"\"\n",
    "    url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "    if '.mp3' in output_dir:\n",
    "        output_path = output_dir\n",
    "    else:\n",
    "        output_path = os.path.join(output_dir, f\"{video_id}.mp3\")\n",
    "\n",
    "    command = [\n",
    "        \"yt-dlp\",\n",
    "        \"-f\", \"bestaudio\",\n",
    "        \"--extract-audio\",\n",
    "        \"--audio-format\", \"mp3\",\n",
    "        \"-o\", output_path,\n",
    "        url\n",
    "    ]\n",
    "\n",
    "    print(f\"⬇️ Downloading and converting to MP3 from {url}...\")\n",
    "    subprocess.run(command, check=True)\n",
    "    print(f\"✅ MP3 saved to: {output_path}\")\n",
    "\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c0e21e-c1c9-492d-a98a-d6dd428abffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "jon = 'oAgf7qMT-Gw'\n",
    "jon_filename = \"audio/jon.mp3\"\n",
    "\n",
    "maria = 'fpD-9jPo8Gk'\n",
    "maria_filename = \"audio/maria.mp3\"\n",
    "\n",
    "laurie = 'Ohj_pmO2a4g'\n",
    "laurie_filename = \"audio/laurie.mp3\"\n",
    "\n",
    "alicia = 'XAridnH_PYs'\n",
    "alicia_filename = \"audio/alicia.mp3\"\n",
    "\n",
    "stacy = '58w3rU9U8Aw'\n",
    "stacy_filename = \"audio/stacy.mp3\"\n",
    "\n",
    "jennifer = 'zE2fxKo2WwE'\n",
    "jennifer_filename = \"audio/jennifer.mp3\"\n",
    "\n",
    "richard = 'qdZ4dRg76Gc'\n",
    "richard_filename = \"audio/richard.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7437f6c7-1757-4f7e-8484-b0d920293d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬇️ Downloading and converting to MP3 from https://www.youtube.com/watch?v=oAgf7qMT-Gw...\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=oAgf7qMT-Gw\n",
      "[youtube] oAgf7qMT-Gw: Downloading webpage\n",
      "[youtube] oAgf7qMT-Gw: Downloading tv client config\n",
      "[youtube] oAgf7qMT-Gw: Downloading player b2858d36-main\n",
      "[youtube] oAgf7qMT-Gw: Downloading tv player API JSON\n",
      "[youtube] oAgf7qMT-Gw: Downloading ios player API JSON\n",
      "[youtube] oAgf7qMT-Gw: Downloading m3u8 information\n",
      "[info] oAgf7qMT-Gw: Downloading 1 format(s): 251\n",
      "[download] Destination: audio/jon.webm\n",
      "[download] 100% of   21.14MiB in 00:00:02 at 7.70MiB/s   \n",
      "[ExtractAudio] Destination: audio/jon.mp3\n",
      "Deleting original file audio/jon.webm (pass -k to keep)\n",
      "✅ MP3 saved to: audio/jon.mp3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'audio/jon.mp3'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_youtube_audio(jon, output_dir= jon_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af604387-88da-4fa5-948a-3552534e6e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬇️ Downloading and converting to MP3 from https://www.youtube.com/watch?v=fpD-9jPo8Gk...\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=fpD-9jPo8Gk\n",
      "[youtube] fpD-9jPo8Gk: Downloading webpage\n",
      "[youtube] fpD-9jPo8Gk: Downloading tv client config\n",
      "[youtube] fpD-9jPo8Gk: Downloading player b2858d36-main\n",
      "[youtube] fpD-9jPo8Gk: Downloading tv player API JSON\n",
      "[youtube] fpD-9jPo8Gk: Downloading ios player API JSON\n",
      "[youtube] fpD-9jPo8Gk: Downloading m3u8 information\n",
      "[info] fpD-9jPo8Gk: Downloading 1 format(s): 251\n",
      "[download] Destination: audio/maria.webm\n",
      "[download] 100% of   22.16MiB in 00:00:03 at 6.55MiB/s   \n",
      "[ExtractAudio] Destination: audio/maria.mp3\n",
      "Deleting original file audio/maria.webm (pass -k to keep)\n",
      "✅ MP3 saved to: audio/maria.mp3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'audio/maria.mp3'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_youtube_audio(maria, output_dir= maria_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54eac6d6-70ea-4af6-841d-1e13c319d59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬇️ Downloading and converting to MP3 from https://www.youtube.com/watch?v=Ohj_pmO2a4g...\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=Ohj_pmO2a4g\n",
      "[youtube] Ohj_pmO2a4g: Downloading webpage\n",
      "[youtube] Ohj_pmO2a4g: Downloading tv client config\n",
      "[youtube] Ohj_pmO2a4g: Downloading player b2858d36-main\n",
      "[youtube] Ohj_pmO2a4g: Downloading tv player API JSON\n",
      "[youtube] Ohj_pmO2a4g: Downloading ios player API JSON\n",
      "[youtube] Ohj_pmO2a4g: Downloading m3u8 information\n",
      "[info] Ohj_pmO2a4g: Downloading 1 format(s): 251\n",
      "[download] Destination: audio/laurie.webm\n",
      "[download] 100% of   15.04MiB in 00:00:01 at 8.92MiB/s   \n",
      "[ExtractAudio] Destination: audio/laurie.mp3\n",
      "Deleting original file audio/laurie.webm (pass -k to keep)\n",
      "✅ MP3 saved to: audio/laurie.mp3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'audio/laurie.mp3'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_youtube_audio(laurie, output_dir= laurie_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "362273de-a7b2-4cb6-b0ae-e2bcff68c62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬇️ Downloading and converting to MP3 from https://www.youtube.com/watch?v=XAridnH_PYs...\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=XAridnH_PYs\n",
      "[youtube] XAridnH_PYs: Downloading webpage\n",
      "[youtube] XAridnH_PYs: Downloading tv client config\n",
      "[youtube] XAridnH_PYs: Downloading player b2858d36-main\n",
      "[youtube] XAridnH_PYs: Downloading tv player API JSON\n",
      "[youtube] XAridnH_PYs: Downloading ios player API JSON\n",
      "[youtube] XAridnH_PYs: Downloading m3u8 information\n",
      "[info] XAridnH_PYs: Downloading 1 format(s): 251\n",
      "[download] Destination: audio/alicia.webm\n",
      "[download] 100% of   14.05MiB in 00:00:01 at 7.70MiB/s   \n",
      "[ExtractAudio] Destination: audio/alicia.mp3\n",
      "Deleting original file audio/alicia.webm (pass -k to keep)\n",
      "✅ MP3 saved to: audio/alicia.mp3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'audio/alicia.mp3'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_youtube_audio(alicia, output_dir= alicia_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95d31728-0c2e-411e-8696-9da07c69e3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬇️ Downloading and converting to MP3 from https://www.youtube.com/watch?v=58w3rU9U8Aw...\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=58w3rU9U8Aw\n",
      "[youtube] 58w3rU9U8Aw: Downloading webpage\n",
      "[youtube] 58w3rU9U8Aw: Downloading tv client config\n",
      "[youtube] 58w3rU9U8Aw: Downloading player b2858d36-main\n",
      "[youtube] 58w3rU9U8Aw: Downloading tv player API JSON\n",
      "[youtube] 58w3rU9U8Aw: Downloading ios player API JSON\n",
      "[youtube] 58w3rU9U8Aw: Downloading m3u8 information\n",
      "[info] 58w3rU9U8Aw: Downloading 1 format(s): 251\n",
      "[download] Destination: audio/stacy.webm\n",
      "[download] 100% of   20.34MiB in 00:00:04 at 4.53MiB/s   \n",
      "[ExtractAudio] Destination: audio/stacy.mp3\n",
      "Deleting original file audio/stacy.webm (pass -k to keep)\n",
      "✅ MP3 saved to: audio/stacy.mp3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'audio/stacy.mp3'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_youtube_audio(stacy, output_dir= stacy_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78030d81-27b0-49e7-98b7-10a1bef540e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬇️ Downloading and converting to MP3 from https://www.youtube.com/watch?v=zE2fxKo2WwE...\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=zE2fxKo2WwE\n",
      "[youtube] zE2fxKo2WwE: Downloading webpage\n",
      "[youtube] zE2fxKo2WwE: Downloading tv client config\n",
      "[youtube] zE2fxKo2WwE: Downloading player b2858d36-main\n",
      "[youtube] zE2fxKo2WwE: Downloading tv player API JSON\n",
      "[youtube] zE2fxKo2WwE: Downloading ios player API JSON\n",
      "[youtube] zE2fxKo2WwE: Downloading m3u8 information\n",
      "[info] zE2fxKo2WwE: Downloading 1 format(s): 251\n",
      "[download] Destination: audio/jennifer.webm\n",
      "[download] 100% of   21.14MiB in 00:00:02 at 7.85MiB/s   \n",
      "[ExtractAudio] Destination: audio/jennifer.mp3\n",
      "Deleting original file audio/jennifer.webm (pass -k to keep)\n",
      "✅ MP3 saved to: audio/jennifer.mp3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'audio/jennifer.mp3'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_youtube_audio(jennifer, output_dir= jennifer_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e65c74-1130-4faa-bd56-7785c43b324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_youtube_audio(richard, output_dir= richard_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4a2ccd-ee6c-4ff0-8ed0-c25de1ea2ea3",
   "metadata": {},
   "source": [
    "### Pre-Process Interview Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb8b1fb2-8e07-4801-8093-f794632fdb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_with_speaker_labels(\n",
    "    audio_path: str,\n",
    "    save_dir: str,\n",
    "    hf_token: str,\n",
    "    min_speakers: int = 4,\n",
    "    max_speakers: int = 10,\n",
    "    device: str = \"cuda\") -> list:\n",
    "    \"\"\"\n",
    "    Transcribes audio, aligns words, performs speaker diarization, and adds speaker labels.\n",
    "\n",
    "    Args:\n",
    "        audio_path (str): Path to the input audio file.\n",
    "        save_dir (str): Directory to cache models and files.\n",
    "        hf_token (str): HuggingFace token for accessing diarization model.\n",
    "        min_speakers (int): Minimum number of speakers expected.\n",
    "        max_speakers (int): Maximum number of speakers expected.\n",
    "        device (str): Device to run the models on (\"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "        List of segments with speaker labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load WhisperX model and transcribe\n",
    "    model = whisperx.load_model(\"large-v2\", device=device, download_root=save_dir)\n",
    "    result = model.transcribe(audio_path)\n",
    "\n",
    "    # Align segments (optional but useful)\n",
    "    model_a, metadata = whisperx.load_align_model(\n",
    "        language_code=result[\"language\"], device=device\n",
    "    )\n",
    "    result_aligned = whisperx.align(\n",
    "        result[\"segments\"], model_a, metadata, audio_path, device\n",
    "    )\n",
    "\n",
    "    # Perform speaker diarization\n",
    "    pipeline = Pipeline.from_pretrained(\n",
    "        \"pyannote/speaker-diarization\", use_auth_token=hf_token\n",
    "    )\n",
    "    diarization = pipeline(audio_path, min_speakers=min_speakers, max_speakers=max_speakers)\n",
    "\n",
    "    # Add speaker labels to aligned segments\n",
    "    result_segments = result_aligned[\"segments\"]\n",
    "\n",
    "    for segment in result_segments:\n",
    "        segment_start = segment[\"start\"]\n",
    "        segment_end = segment[\"end\"]\n",
    "\n",
    "        for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "            if turn.start <= segment_start <= turn.end or turn.start <= segment_end <= turn.end:\n",
    "                segment[\"speaker\"] = speaker\n",
    "                break\n",
    "        else:\n",
    "            segment[\"speaker\"] = \"unknown\"\n",
    "\n",
    "    return result_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26c22c20-a729-4211-aa79-dd26ab9260d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63259ce0-95c7-4a1e-b695-1d94290fe15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "jon_segments = transcribe_with_speaker_labels(\n",
    "    audio_path=jon_filename,\n",
    "    save_dir=\".\",\n",
    "    hf_token=hf_token,\n",
    "    min_speakers=2,\n",
    "    max_speakers=10)\n",
    "np.save('transcripts/jon.npy', jon_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6bbd2a2-e7d4-4dcd-bca0-df45782ad770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.conda/envs/llm/lib/python3.9/site-packages/whisperx/assets/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n",
      ">>Performing voice activity detection using Pyannote...\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.7.0+cu126. Bad things might happen unless you revert torch to 1.x.\n",
      "Detected language: en (1.00) in first 30s of audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.7.0+cu126. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/smerrill/.conda/envs/llm/lib/python3.9/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "alicia_segments = transcribe_with_speaker_labels(\n",
    "    audio_path=alicia_filename,\n",
    "    save_dir=\".\",\n",
    "    hf_token=hf_token,\n",
    "    min_speakers=2,\n",
    "    max_speakers=10)\n",
    "np.save('transcripts/alicia.npy', alicia_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0fe2699-225c-419a-b093-48d7aa356a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.conda/envs/llm/lib/python3.9/site-packages/whisperx/assets/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n",
      ">>Performing voice activity detection using Pyannote...\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.7.0+cu126. Bad things might happen unless you revert torch to 1.x.\n",
      "Detected language: en (1.00) in first 30s of audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.7.0+cu126. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/smerrill/.conda/envs/llm/lib/python3.9/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "jennifer_segments = transcribe_with_speaker_labels(\n",
    "    audio_path=jennifer_filename,\n",
    "    save_dir=\".\",\n",
    "    hf_token=hf_token,\n",
    "    min_speakers=2,\n",
    "    max_speakers=10)\n",
    "np.save('transcripts/jennifer.npy', jennifer_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "269c85ed-6a88-498e-a8b0-8df53b98923e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.conda/envs/llm/lib/python3.9/site-packages/whisperx/assets/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n",
      ">>Performing voice activity detection using Pyannote...\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.7.0+cu126. Bad things might happen unless you revert torch to 1.x.\n",
      "Detected language: en (1.00) in first 30s of audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.7.0+cu126. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/smerrill/.conda/envs/llm/lib/python3.9/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "laurie_segments = transcribe_with_speaker_labels(\n",
    "    audio_path=laurie_filename,\n",
    "    save_dir=\".\",\n",
    "    hf_token=hf_token,\n",
    "    min_speakers=2,\n",
    "    max_speakers=10)\n",
    "np.save('transcripts/laurie.npy', laurie_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5492cfcf-9132-4db6-9262-dfd1bb6343ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n",
      ">>Performing voice activity detection using Pyannote...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.conda/envs/llm/lib/python3.9/site-packages/whisperx/assets/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.7.0+cu126. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/smerrill/.conda/envs/llm/lib/python3.9/site-packages/pyannote/audio/utils/reproducibility.py:74: ReproducibilityWarning: TensorFloat-32 (TF32) has been disabled as it might lead to reproducibility issues and lower accuracy.\n",
      "It can be re-enabled by calling\n",
      "   >>> import torch\n",
      "   >>> torch.backends.cuda.matmul.allow_tf32 = True\n",
      "   >>> torch.backends.cudnn.allow_tf32 = True\n",
      "See https://github.com/pyannote/pyannote-audio/issues/1370 for more details.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: en (1.00) in first 30s of audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n",
      "/nas/longleaf/home/smerrill/.conda/envs/llm/lib/python3.9/site-packages/speechbrain/utils/autocast.py:188: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.7.0+cu126. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/smerrill/.conda/envs/llm/lib/python3.9/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "maria_segments = transcribe_with_speaker_labels(\n",
    "    audio_path=maria_filename,\n",
    "    save_dir=\".\",\n",
    "    hf_token=hf_token,\n",
    "    min_speakers=2,\n",
    "    max_speakers=10)\n",
    "np.save('transcripts/maria.npy', maria_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcd29b2-966e-4276-a69d-dd55d5b0eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "richard_segments = transcribe_with_speaker_labels(\n",
    "    audio_path=richard_filename,\n",
    "    save_dir=\".\",\n",
    "    hf_token=hf_token,\n",
    "    min_speakers=2,\n",
    "    max_speakers=10)\n",
    "np.save('transcripts/richard.npy', richard_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763170d3-5917-478b-8ccc-0154343bb716",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacy_segments = transcribe_with_speaker_labels(\n",
    "    audio_path=stacy_filename,\n",
    "    save_dir=\".\",\n",
    "    hf_token=hf_token,\n",
    "    min_speakers=2,\n",
    "    max_speakers=10)\n",
    "np.save('transcripts/stacy.npy', stacy_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20ec0e0e-d0e4-4cca-b0be-feaca1986e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ac640e3-d2fe-4a40-87c6-435a997c024e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0342fd61-e166-4d43-835e-eedf74c88600",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
