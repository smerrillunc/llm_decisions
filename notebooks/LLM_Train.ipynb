{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6ba2b0-80d1-4198-9011-9ff95941d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append('/playpen-ssd/smerrill/llm_decisions')\n",
    "from dataclasses import dataclass, field\n",
    "from datasets import (Dataset, IterableDataset,)\n",
    "import torch\n",
    "from transformers import AutoTokenizer, TrainingArguments\n",
    "from trl.commands.cli_utils import  TrlParser\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    PreTrainedTokenizerBase,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "from trl import setup_chat_format\n",
    "from peft import LoraConfig\n",
    "import numpy as np\n",
    "\n",
    "from trl import (\n",
    "   SFTTrainer)\n",
    "from collections import Counter\n",
    "import re\n",
    "import wandb\n",
    "from utils import train_on_responses_only, train_test_split, get_speaker_special_tokens, debug_tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55f92b2-54d7-4c3a-8659-b8989e448413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_data, test_data, train_completion_data = train_test_split('kateacuff')\n",
    "\n",
    "tmp = [{\"text\": text} for text in train_data]\n",
    "train_data = Dataset.from_list(tmp)\n",
    "\n",
    "special_tokens = get_speaker_special_tokens(train_data)\n",
    "tokenizer.add_special_tokens(special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daee384e-d764-4c96-8b5a-88ea7afd50c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "kateacuff: I would agree that that the online retreat and half a day, but I would keep it open to having part to either online or with a delayed larger group. I think having all day long online, you would see people ducking under their desk. Probably I'm entering.<|eot_id|>\n",
      "\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "rossholden: I'm entering our seven of Zoom. I can tell you I'm. My attention is drifting.\n",
      "katrinacallsen: I move or do we have to make a motion you just we just have to tell you.\n",
      "patrickmclau: I think we just need some consensus from the board, it sounds like we might have that around a half day. zoom meeting would we want to keep the the June 5 date, which was our original date for the retreat.\n",
      "jonnoalcaro: I would be in favor of keeping the June 5th date. And on the half day issue, I agree with Kate that we need to potentially set another date for a part two, because at least some of the things that I'd like to talk about would take some time, but it doesn't have to be all on the same day.\n",
      "unknownspeaker: So then maybe then, so if there's no objection from the board, I propose that we move forward with the plan for a half-day board retreat on June 5th, and we can kind of shift the discussion now to what may be the topics that we want to have on there. So the ones that we have put in front of you already are the evaluations, both the board self-evaluation and the superintendent evaluation, the recovery of learning phase and the logistics and planning behind the new strategic plan that's going to come up. But again, the board really should play a significant role in determining the agenda for this retreat. And so I guess I would open it, I'd like to open it back up to the board for thoughts on that. Are these the right agenda items right now? Are there other things that you would like to get on there? And if we're going for a half day part one, what would you like to see in part one? Dr. Acuff.<|eot_id|>\n",
      "\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "kateacuff: Last year and perhaps even the year before that, we moved the evaluations out of the retreat. And I would be in favor of doing that. I think recovery of learning phase and the strategic plan are the two more important. And I would maybe put recovery of the learning phase in the June 5 meeting and then strategic plan later. But it could be either way. I just don't want an hour and a half of learning phase and an hour and a half of strategic plan. I think we need to do a deep dive into each of these.<|eot_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_data['text'][19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d5a18f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tokenized Input ===\n",
      "000: 128000  ->  '<|begin_of_text|>'\n",
      "001: 128006  ->  '<|start_header_id|>'\n",
      "002: 78191  ->  'assistant'\n",
      "003: 128007  ->  '<|end_header_id|>'\n",
      "004:   271  ->  '\\n\\n'\n",
      "005: 128260  ->  'kateacuff:'\n",
      "006:   358  ->  ' I'\n",
      "007:  1053  ->  ' would'\n",
      "008:  7655  ->  ' agree'\n",
      "009:   430  ->  ' that'\n",
      "010:   430  ->  ' that'\n",
      "011:   279  ->  ' the'\n",
      "012:  2930  ->  ' online'\n",
      "013: 31114  ->  ' retreat'\n",
      "014:   323  ->  ' and'\n",
      "015:  4376  ->  ' half'\n",
      "016:   264  ->  ' a'\n",
      "017:  1938  ->  ' day'\n",
      "018:    11  ->  ','\n",
      "019:   719  ->  ' but'\n",
      "020:   358  ->  ' I'\n",
      "021:  1053  ->  ' would'\n",
      "022:  2567  ->  ' keep'\n",
      "023:   433  ->  ' it'\n",
      "024:  1825  ->  ' open'\n",
      "025:   311  ->  ' to'\n",
      "026:  3515  ->  ' having'\n",
      "027:   961  ->  ' part'\n",
      "028:   311  ->  ' to'\n",
      "029:  3060  ->  ' either'\n",
      "030:  2930  ->  ' online'\n",
      "031:   477  ->  ' or'\n",
      "032:   449  ->  ' with'\n",
      "033:   264  ->  ' a'\n",
      "034: 23540  ->  ' delayed'\n",
      "035:  8294  ->  ' larger'\n",
      "036:  1912  ->  ' group'\n",
      "037:    13  ->  '.'\n",
      "038:   358  ->  ' I'\n",
      "039:  1781  ->  ' think'\n",
      "040:  3515  ->  ' having'\n",
      "041:   682  ->  ' all'\n",
      "042:  1938  ->  ' day'\n",
      "043:  1317  ->  ' long'\n",
      "044:  2930  ->  ' online'\n",
      "045:    11  ->  ','\n",
      "046:   499  ->  ' you'\n",
      "047:  1053  ->  ' would'\n",
      "048:  1518  ->  ' see'\n",
      "049:  1274  ->  ' people'\n",
      "050: 37085  ->  ' duck'\n",
      "051:   287  ->  'ing'\n",
      "052:  1234  ->  ' under'\n",
      "053:   872  ->  ' their'\n",
      "054: 18496  ->  ' desk'\n",
      "055:    13  ->  '.'\n",
      "056: 38254  ->  ' Probably'\n",
      "057:   358  ->  ' I'\n",
      "058:  2846  ->  \"'m\"\n",
      "059: 16661  ->  ' entering'\n",
      "060:    13  ->  '.'\n",
      "061: 128009  ->  '<|eot_id|>'\n",
      "062:   271  ->  '\\n\\n'\n",
      "063: 128006  ->  '<|start_header_id|>'\n",
      "064:   882  ->  'user'\n",
      "065: 128007  ->  '<|end_header_id|>'\n",
      "066:   271  ->  '\\n\\n'\n",
      "067: 128281  ->  'rossholden:'\n",
      "068:   358  ->  ' I'\n",
      "069:  2846  ->  \"'m\"\n",
      "070: 16661  ->  ' entering'\n",
      "071:  1057  ->  ' our'\n",
      "072:  8254  ->  ' seven'\n",
      "073:   315  ->  ' of'\n",
      "074: 38629  ->  ' Zoom'\n",
      "075:    13  ->  '.'\n",
      "076:   358  ->  ' I'\n",
      "077:   649  ->  ' can'\n",
      "078:  3371  ->  ' tell'\n",
      "079:   499  ->  ' you'\n",
      "080:   358  ->  ' I'\n",
      "081:  2846  ->  \"'m\"\n",
      "082:    13  ->  '.'\n",
      "083:  3092  ->  ' My'\n",
      "084:  6666  ->  ' attention'\n",
      "085:   374  ->  ' is'\n",
      "086: 85353  ->  ' drifting'\n",
      "087:   627  ->  '.\\n'\n",
      "088: 128267  ->  'katrinacallsen:'\n",
      "089:   358  ->  ' I'\n",
      "090:  3351  ->  ' move'\n",
      "091:   477  ->  ' or'\n",
      "092:   656  ->  ' do'\n",
      "093:   584  ->  ' we'\n",
      "094:   617  ->  ' have'\n",
      "095:   311  ->  ' to'\n",
      "096:  1304  ->  ' make'\n",
      "097:   264  ->  ' a'\n",
      "098: 11633  ->  ' motion'\n",
      "099:   499  ->  ' you'\n",
      "100:  1120  ->  ' just'\n",
      "101:   584  ->  ' we'\n",
      "102:  1120  ->  ' just'\n",
      "103:   617  ->  ' have'\n",
      "104:   311  ->  ' to'\n",
      "105:  3371  ->  ' tell'\n",
      "106:   499  ->  ' you'\n",
      "107:   627  ->  '.\\n'\n",
      "108: 128282  ->  'patrickmclau:'\n",
      "109:   358  ->  ' I'\n",
      "110:  1781  ->  ' think'\n",
      "111:   584  ->  ' we'\n",
      "112:  1120  ->  ' just'\n",
      "113:  1205  ->  ' need'\n",
      "114:  1063  ->  ' some'\n",
      "115: 24811  ->  ' consensus'\n",
      "116:   505  ->  ' from'\n",
      "117:   279  ->  ' the'\n",
      "118:  4580  ->  ' board'\n",
      "119:    11  ->  ','\n",
      "120:   433  ->  ' it'\n",
      "121: 10578  ->  ' sounds'\n",
      "122:  1093  ->  ' like'\n",
      "123:   584  ->  ' we'\n",
      "124:  2643  ->  ' might'\n",
      "125:   617  ->  ' have'\n",
      "126:   430  ->  ' that'\n",
      "127:  2212  ->  ' around'\n",
      "128:   264  ->  ' a'\n",
      "129:  4376  ->  ' half'\n",
      "130:  1938  ->  ' day'\n",
      "131:    13  ->  '.'\n",
      "132: 15932  ->  ' zoom'\n",
      "133:  6574  ->  ' meeting'\n",
      "134:  1053  ->  ' would'\n",
      "135:   584  ->  ' we'\n",
      "136:  1390  ->  ' want'\n",
      "137:   311  ->  ' to'\n",
      "138:  2567  ->  ' keep'\n",
      "139:   279  ->  ' the'\n",
      "140:   279  ->  ' the'\n",
      "141:  5651  ->  ' June'\n",
      "142:   220  ->  ' '\n",
      "143:    20  ->  '5'\n",
      "144:  2457  ->  ' date'\n",
      "145:    11  ->  ','\n",
      "146:   902  ->  ' which'\n",
      "147:   574  ->  ' was'\n",
      "148:  1057  ->  ' our'\n",
      "149:  4113  ->  ' original'\n",
      "150:  2457  ->  ' date'\n",
      "151:   369  ->  ' for'\n",
      "152:   279  ->  ' the'\n",
      "153: 31114  ->  ' retreat'\n",
      "154:   627  ->  '.\\n'\n",
      "155: 128268  ->  'jonnoalcaro:'\n",
      "156:   358  ->  ' I'\n",
      "157:  1053  ->  ' would'\n",
      "158:   387  ->  ' be'\n",
      "159:   304  ->  ' in'\n",
      "160:  4799  ->  ' favor'\n",
      "161:   315  ->  ' of'\n",
      "162: 10494  ->  ' keeping'\n",
      "163:   279  ->  ' the'\n",
      "164:  5651  ->  ' June'\n",
      "165:   220  ->  ' '\n",
      "166:    20  ->  '5'\n",
      "167:   339  ->  'th'\n",
      "168:  2457  ->  ' date'\n",
      "169:    13  ->  '.'\n",
      "170:  1628  ->  ' And'\n",
      "171:   389  ->  ' on'\n",
      "172:   279  ->  ' the'\n",
      "173:  4376  ->  ' half'\n",
      "174:  1938  ->  ' day'\n",
      "175:  4360  ->  ' issue'\n",
      "176:    11  ->  ','\n",
      "177:   358  ->  ' I'\n",
      "178:  7655  ->  ' agree'\n",
      "179:   449  ->  ' with'\n",
      "180: 30301  ->  ' Kate'\n",
      "181:   430  ->  ' that'\n",
      "182:   584  ->  ' we'\n",
      "183:  1205  ->  ' need'\n",
      "184:   311  ->  ' to'\n",
      "185: 13893  ->  ' potentially'\n",
      "186:   743  ->  ' set'\n",
      "187:  2500  ->  ' another'\n",
      "188:  2457  ->  ' date'\n",
      "189:   369  ->  ' for'\n",
      "190:   264  ->  ' a'\n",
      "191:   961  ->  ' part'\n",
      "192:  1403  ->  ' two'\n",
      "193:    11  ->  ','\n",
      "194:  1606  ->  ' because'\n",
      "195:   520  ->  ' at'\n",
      "196:  3325  ->  ' least'\n",
      "197:  1063  ->  ' some'\n",
      "198:   315  ->  ' of'\n",
      "199:   279  ->  ' the'\n",
      "200:  2574  ->  ' things'\n",
      "201:   430  ->  ' that'\n",
      "202:   358  ->  ' I'\n",
      "203:  4265  ->  \"'d\"\n",
      "204:  1093  ->  ' like'\n",
      "205:   311  ->  ' to'\n",
      "206:  3137  ->  ' talk'\n",
      "207:   922  ->  ' about'\n",
      "208:  1053  ->  ' would'\n",
      "209:  1935  ->  ' take'\n",
      "210:  1063  ->  ' some'\n",
      "211:   892  ->  ' time'\n",
      "212:    11  ->  ','\n",
      "213:   719  ->  ' but'\n",
      "214:   433  ->  ' it'\n",
      "215:  3250  ->  ' doesn'\n",
      "216:   956  ->  \"'t\"\n",
      "217:   617  ->  ' have'\n",
      "218:   311  ->  ' to'\n",
      "219:   387  ->  ' be'\n",
      "220:   682  ->  ' all'\n",
      "221:   389  ->  ' on'\n",
      "222:   279  ->  ' the'\n",
      "223:  1890  ->  ' same'\n",
      "224:  1938  ->  ' day'\n",
      "225:   627  ->  '.\\n'\n",
      "226: 128259  ->  'unknownspeaker:'\n",
      "227:  2100  ->  ' So'\n",
      "228:  1243  ->  ' then'\n",
      "229:  7344  ->  ' maybe'\n",
      "230:  1243  ->  ' then'\n",
      "231:    11  ->  ','\n",
      "232:   779  ->  ' so'\n",
      "233:   422  ->  ' if'\n",
      "234:  1070  ->  ' there'\n",
      "235:   596  ->  \"'s\"\n",
      "236:   912  ->  ' no'\n",
      "237: 46655  ->  ' objection'\n",
      "238:   505  ->  ' from'\n",
      "239:   279  ->  ' the'\n",
      "240:  4580  ->  ' board'\n",
      "241:    11  ->  ','\n",
      "242:   358  ->  ' I'\n",
      "243: 30714  ->  ' propose'\n",
      "244:   430  ->  ' that'\n",
      "245:   584  ->  ' we'\n",
      "246:  3351  ->  ' move'\n",
      "247:  4741  ->  ' forward'\n",
      "248:   449  ->  ' with'\n",
      "249:   279  ->  ' the'\n",
      "250:  3197  ->  ' plan'\n",
      "251:   369  ->  ' for'\n",
      "252:   264  ->  ' a'\n",
      "253:  4376  ->  ' half'\n",
      "254: 11477  ->  '-day'\n",
      "255:  4580  ->  ' board'\n",
      "256: 31114  ->  ' retreat'\n",
      "257:   389  ->  ' on'\n",
      "258:  5651  ->  ' June'\n",
      "259:   220  ->  ' '\n",
      "260:    20  ->  '5'\n",
      "261:   339  ->  'th'\n",
      "262:    11  ->  ','\n",
      "263:   323  ->  ' and'\n",
      "264:   584  ->  ' we'\n",
      "265:   649  ->  ' can'\n",
      "266:  3169  ->  ' kind'\n",
      "267:   315  ->  ' of'\n",
      "268:  6541  ->  ' shift'\n",
      "269:   279  ->  ' the'\n",
      "270: 10430  ->  ' discussion'\n",
      "271:  1457  ->  ' now'\n",
      "272:   311  ->  ' to'\n",
      "273:  1148  ->  ' what'\n",
      "274:  1253  ->  ' may'\n",
      "275:   387  ->  ' be'\n",
      "276:   279  ->  ' the'\n",
      "277: 13650  ->  ' topics'\n",
      "278:   430  ->  ' that'\n",
      "279:   584  ->  ' we'\n",
      "280:  1390  ->  ' want'\n",
      "281:   311  ->  ' to'\n",
      "282:   617  ->  ' have'\n",
      "283:   389  ->  ' on'\n",
      "284:  1070  ->  ' there'\n",
      "285:    13  ->  '.'\n",
      "286:  2100  ->  ' So'\n",
      "287:   279  ->  ' the'\n",
      "288:  6305  ->  ' ones'\n",
      "289:   430  ->  ' that'\n",
      "290:   584  ->  ' we'\n",
      "291:   617  ->  ' have'\n",
      "292:  2231  ->  ' put'\n",
      "293:   304  ->  ' in'\n",
      "294:  4156  ->  ' front'\n",
      "295:   315  ->  ' of'\n",
      "296:   499  ->  ' you'\n",
      "297:  2736  ->  ' already'\n",
      "298:   527  ->  ' are'\n",
      "299:   279  ->  ' the'\n",
      "300: 56181  ->  ' evaluations'\n",
      "301:    11  ->  ','\n",
      "302:  2225  ->  ' both'\n",
      "303:   279  ->  ' the'\n",
      "304:  4580  ->  ' board'\n",
      "305:   659  ->  ' self'\n",
      "306:  5773  ->  '-e'\n",
      "307: 24756  ->  'valuation'\n",
      "308:   323  ->  ' and'\n",
      "309:   279  ->  ' the'\n",
      "310: 74133  ->  ' superintendent'\n",
      "311: 16865  ->  ' evaluation'\n",
      "312:    11  ->  ','\n",
      "313:   279  ->  ' the'\n",
      "314: 13654  ->  ' recovery'\n",
      "315:   315  ->  ' of'\n",
      "316:  6975  ->  ' learning'\n",
      "317: 10474  ->  ' phase'\n",
      "318:   323  ->  ' and'\n",
      "319:   279  ->  ' the'\n",
      "320: 43257  ->  ' logistics'\n",
      "321:   323  ->  ' and'\n",
      "322:  9293  ->  ' planning'\n",
      "323:  4920  ->  ' behind'\n",
      "324:   279  ->  ' the'\n",
      "325:   502  ->  ' new'\n",
      "326: 19092  ->  ' strategic'\n",
      "327:  3197  ->  ' plan'\n",
      "328:   430  ->  ' that'\n",
      "329:   596  ->  \"'s\"\n",
      "330:  2133  ->  ' going'\n",
      "331:   311  ->  ' to'\n",
      "332:  2586  ->  ' come'\n",
      "333:   709  ->  ' up'\n",
      "334:    13  ->  '.'\n",
      "335:  2030  ->  ' But'\n",
      "336:  1578  ->  ' again'\n",
      "337:    11  ->  ','\n",
      "338:   279  ->  ' the'\n",
      "339:  4580  ->  ' board'\n",
      "340:  2216  ->  ' really'\n",
      "341:  1288  ->  ' should'\n",
      "342:  1514  ->  ' play'\n",
      "343:   264  ->  ' a'\n",
      "344:  5199  ->  ' significant'\n",
      "345:  3560  ->  ' role'\n",
      "346:   304  ->  ' in'\n",
      "347: 26679  ->  ' determining'\n",
      "348:   279  ->  ' the'\n",
      "349: 18909  ->  ' agenda'\n",
      "350:   369  ->  ' for'\n",
      "351:   420  ->  ' this'\n",
      "352: 31114  ->  ' retreat'\n",
      "353:    13  ->  '.'\n",
      "354:  1628  ->  ' And'\n",
      "355:   779  ->  ' so'\n",
      "356:   358  ->  ' I'\n",
      "357:  8101  ->  ' guess'\n",
      "358:   358  ->  ' I'\n",
      "359:  1053  ->  ' would'\n",
      "360:  1825  ->  ' open'\n",
      "361:   433  ->  ' it'\n",
      "362:    11  ->  ','\n",
      "363:   358  ->  ' I'\n",
      "364:  4265  ->  \"'d\"\n",
      "365:  1093  ->  ' like'\n",
      "366:   311  ->  ' to'\n",
      "367:  1825  ->  ' open'\n",
      "368:   433  ->  ' it'\n",
      "369:  1203  ->  ' back'\n",
      "370:   709  ->  ' up'\n",
      "371:   311  ->  ' to'\n",
      "372:   279  ->  ' the'\n",
      "373:  4580  ->  ' board'\n",
      "374:   369  ->  ' for'\n",
      "375: 11555  ->  ' thoughts'\n",
      "376:   389  ->  ' on'\n",
      "377:   430  ->  ' that'\n",
      "378:    13  ->  '.'\n",
      "379:  8886  ->  ' Are'\n",
      "380:  1521  ->  ' these'\n",
      "381:   279  ->  ' the'\n",
      "382:  1314  ->  ' right'\n",
      "383: 18909  ->  ' agenda'\n",
      "384:  3673  ->  ' items'\n",
      "385:  1314  ->  ' right'\n",
      "386:  1457  ->  ' now'\n",
      "387:    30  ->  '?'\n",
      "388:  8886  ->  ' Are'\n",
      "389:  1070  ->  ' there'\n",
      "390:  1023  ->  ' other'\n",
      "391:  2574  ->  ' things'\n",
      "392:   430  ->  ' that'\n",
      "393:   499  ->  ' you'\n",
      "394:  1053  ->  ' would'\n",
      "395:  1093  ->  ' like'\n",
      "396:   311  ->  ' to'\n",
      "397:   636  ->  ' get'\n",
      "398:   389  ->  ' on'\n",
      "399:  1070  ->  ' there'\n",
      "400:    30  ->  '?'\n",
      "401:  1628  ->  ' And'\n",
      "402:   422  ->  ' if'\n",
      "403:   584  ->  ' we'\n",
      "404:  2351  ->  \"'re\"\n",
      "405:  2133  ->  ' going'\n",
      "406:   369  ->  ' for'\n",
      "407:   264  ->  ' a'\n",
      "408:  4376  ->  ' half'\n",
      "409:  1938  ->  ' day'\n",
      "410:   961  ->  ' part'\n",
      "411:   832  ->  ' one'\n",
      "412:    11  ->  ','\n",
      "413:  1148  ->  ' what'\n",
      "414:  1053  ->  ' would'\n",
      "415:   499  ->  ' you'\n",
      "416:  1093  ->  ' like'\n",
      "417:   311  ->  ' to'\n",
      "418:  1518  ->  ' see'\n",
      "419:   304  ->  ' in'\n",
      "420:   961  ->  ' part'\n",
      "421:   832  ->  ' one'\n",
      "422:    30  ->  '?'\n",
      "423:  2999  ->  ' Dr'\n",
      "424:    13  ->  '.'\n",
      "425:  6515  ->  ' Ac'\n",
      "426:  1386  ->  'uff'\n",
      "427:    13  ->  '.'\n",
      "428: 128009  ->  '<|eot_id|>'\n",
      "429:   271  ->  '\\n\\n'\n",
      "430: 128006  ->  '<|start_header_id|>'\n",
      "431: 78191  ->  'assistant'\n",
      "432: 128007  ->  '<|end_header_id|>'\n",
      "433:   271  ->  '\\n\\n'\n",
      "434: 128260  ->  'kateacuff:'\n",
      "435:  8155  ->  ' Last'\n",
      "436:  1060  ->  ' year'\n",
      "437:   323  ->  ' and'\n",
      "438:  8530  ->  ' perhaps'\n",
      "439:  1524  ->  ' even'\n",
      "440:   279  ->  ' the'\n",
      "441:  1060  ->  ' year'\n",
      "442:  1603  ->  ' before'\n",
      "443:   430  ->  ' that'\n",
      "444:    11  ->  ','\n",
      "445:   584  ->  ' we'\n",
      "446:  7882  ->  ' moved'\n",
      "447:   279  ->  ' the'\n",
      "448: 56181  ->  ' evaluations'\n",
      "449:   704  ->  ' out'\n",
      "450:   315  ->  ' of'\n",
      "451:   279  ->  ' the'\n",
      "452: 31114  ->  ' retreat'\n",
      "453:    13  ->  '.'\n",
      "454:  1628  ->  ' And'\n",
      "455:   358  ->  ' I'\n",
      "456:  1053  ->  ' would'\n",
      "457:   387  ->  ' be'\n",
      "458:   304  ->  ' in'\n",
      "459:  4799  ->  ' favor'\n",
      "460:   315  ->  ' of'\n",
      "461:  3815  ->  ' doing'\n",
      "462:   430  ->  ' that'\n",
      "463:    13  ->  '.'\n",
      "464:   358  ->  ' I'\n",
      "465:  1781  ->  ' think'\n",
      "466: 13654  ->  ' recovery'\n",
      "467:   315  ->  ' of'\n",
      "468:  6975  ->  ' learning'\n",
      "469: 10474  ->  ' phase'\n",
      "470:   323  ->  ' and'\n",
      "471:   279  ->  ' the'\n",
      "472: 19092  ->  ' strategic'\n",
      "473:  3197  ->  ' plan'\n",
      "474:   527  ->  ' are'\n",
      "475:   279  ->  ' the'\n",
      "476:  1403  ->  ' two'\n",
      "477:   810  ->  ' more'\n",
      "478:  3062  ->  ' important'\n",
      "479:    13  ->  '.'\n",
      "480:  1628  ->  ' And'\n",
      "481:   358  ->  ' I'\n",
      "482:  1053  ->  ' would'\n",
      "483:  7344  ->  ' maybe'\n",
      "484:  2231  ->  ' put'\n",
      "485: 13654  ->  ' recovery'\n",
      "486:   315  ->  ' of'\n",
      "487:   279  ->  ' the'\n",
      "488:  6975  ->  ' learning'\n",
      "489: 10474  ->  ' phase'\n",
      "490:   304  ->  ' in'\n",
      "491:   279  ->  ' the'\n",
      "492:  5651  ->  ' June'\n",
      "493:   220  ->  ' '\n",
      "494:    20  ->  '5'\n",
      "495:  6574  ->  ' meeting'\n",
      "496:   323  ->  ' and'\n",
      "497:  1243  ->  ' then'\n",
      "498: 19092  ->  ' strategic'\n",
      "499:  3197  ->  ' plan'\n",
      "500:  3010  ->  ' later'\n",
      "501:    13  ->  '.'\n",
      "502:  2030  ->  ' But'\n",
      "503:   433  ->  ' it'\n",
      "504:  1436  ->  ' could'\n",
      "505:   387  ->  ' be'\n",
      "506:  3060  ->  ' either'\n",
      "507:  1648  ->  ' way'\n",
      "508:    13  ->  '.'\n",
      "509:   358  ->  ' I'\n",
      "510:  1120  ->  ' just'\n",
      "511:  1541  ->  ' don'\n",
      "512:   956  ->  \"'t\"\n",
      "513:  1390  ->  ' want'\n",
      "514:   459  ->  ' an'\n",
      "515:  6596  ->  ' hour'\n",
      "516:   323  ->  ' and'\n",
      "517:   264  ->  ' a'\n",
      "518:  4376  ->  ' half'\n",
      "519:   315  ->  ' of'\n",
      "520:  6975  ->  ' learning'\n",
      "521: 10474  ->  ' phase'\n",
      "522:   323  ->  ' and'\n",
      "523:   459  ->  ' an'\n",
      "524:  6596  ->  ' hour'\n",
      "525:   323  ->  ' and'\n",
      "526:   264  ->  ' a'\n",
      "527:  4376  ->  ' half'\n",
      "528:   315  ->  ' of'\n",
      "529: 19092  ->  ' strategic'\n",
      "530:  3197  ->  ' plan'\n",
      "531:    13  ->  '.'\n",
      "532:   358  ->  ' I'\n",
      "533:  1781  ->  ' think'\n",
      "534:   584  ->  ' we'\n",
      "535:  1205  ->  ' need'\n",
      "536:   311  ->  ' to'\n",
      "537:   656  ->  ' do'\n",
      "538:   264  ->  ' a'\n",
      "539:  5655  ->  ' deep'\n",
      "540: 30963  ->  ' dive'\n",
      "541:  1139  ->  ' into'\n",
      "542:  1855  ->  ' each'\n",
      "543:   315  ->  ' of'\n",
      "544:  1521  ->  ' these'\n",
      "545:    13  ->  '.'\n",
      "546: 128009  ->  '<|eot_id|>'\n",
      "547:   271  ->  '\\n\\n'\n"
     ]
    }
   ],
   "source": [
    "debug_tokenization(train_data['text'][19], tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f107fb0a-a9ca-4ce7-8e1d-28680da5612d",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5f46db-8456-4b2a-84fb-2d3af30fe94d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`rope_scaling` must be a dictionary with two fields, `type` and `factor`, got {'factor': 8.0, 'low_freq_factor': 1.0, 'high_freq_factor': 4.0, 'original_max_position_embeddings': 8192, 'rope_type': 'llama3'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 13\u001b[0m\n\u001b[1;32m      3\u001b[0m quant_storage_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbfloat16\n\u001b[1;32m      5\u001b[0m quantization_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m      6\u001b[0m                 load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m                 bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m                 bnb_4bit_quant_storage\u001b[38;5;241m=\u001b[39mquant_storage_dtype,\n\u001b[1;32m     11\u001b[0m                 )\n\u001b[0;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/playpen-ssd/smerrill/hf_env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:523\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    521\u001b[0m     _ \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 523\u001b[0m config, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/playpen-ssd/smerrill/hf_env/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:952\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    947\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    948\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    949\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    950\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    951\u001b[0m         )\n\u001b[0;32m--> 952\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconfig_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munused_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[1;32m    955\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n\u001b[1;32m    956\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(CONFIG_MAPPING\u001b[38;5;241m.\u001b[39mkeys(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/playpen-ssd/smerrill/hf_env/lib/python3.10/site-packages/transformers/configuration_utils.py:761\u001b[0m, in \u001b[0;36mPretrainedConfig.from_dict\u001b[0;34m(cls, config_dict, **kwargs)\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;66;03m# We remove it from kwargs so that it does not appear in `return_unused_kwargs`.\u001b[39;00m\n\u001b[1;32m    759\u001b[0m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn_implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn_implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 761\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpruned_heads\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    764\u001b[0m     config\u001b[38;5;241m.\u001b[39mpruned_heads \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mint\u001b[39m(key): value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mpruned_heads\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m/playpen-ssd/smerrill/hf_env/lib/python3.10/site-packages/transformers/models/llama/configuration_llama.py:161\u001b[0m, in \u001b[0;36mLlamaConfig.__init__\u001b[0;34m(self, vocab_size, hidden_size, intermediate_size, num_hidden_layers, num_attention_heads, num_key_value_heads, hidden_act, max_position_embeddings, initializer_range, rms_norm_eps, use_cache, pad_token_id, bos_token_id, eos_token_id, pretraining_tp, tie_word_embeddings, rope_theta, rope_scaling, attention_bias, attention_dropout, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrope_theta \u001b[38;5;241m=\u001b[39m rope_theta\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrope_scaling \u001b[38;5;241m=\u001b[39m rope_scaling\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rope_scaling_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_bias \u001b[38;5;241m=\u001b[39m attention_bias\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_dropout \u001b[38;5;241m=\u001b[39m attention_dropout\n",
      "File \u001b[0;32m/playpen-ssd/smerrill/hf_env/lib/python3.10/site-packages/transformers/models/llama/configuration_llama.py:181\u001b[0m, in \u001b[0;36mLlamaConfig._rope_scaling_validation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrope_scaling, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrope_scaling) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`rope_scaling` must be a dictionary with two fields, `type` and `factor`, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrope_scaling\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    184\u001b[0m rope_scaling_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrope_scaling\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m rope_scaling_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrope_scaling\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfactor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: `rope_scaling` must be a dictionary with two fields, `type` and `factor`, got {'factor': 8.0, 'low_freq_factor': 1.0, 'high_freq_factor': 4.0, 'original_max_position_embeddings': 8192, 'rope_type': 'llama3'}"
     ]
    }
   ],
   "source": [
    "# Model    \n",
    "torch_dtype = torch.bfloat16\n",
    "quant_storage_dtype = torch.bfloat16\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_quant_storage=quant_storage_dtype,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "model_name,\n",
    "quantization_config=quantization_config,\n",
    "attn_implementation=\"sdpa\", # use sdpa, alternatively use \"flash_attention_2\"\n",
    "torch_dtype=quant_storage_dtype,\n",
    "use_cache=False if training_args.gradient_checkpointing else True,  # this is needed for gradient checkpointing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f11201f6-02aa-4c9c-a435-955a7fbb6977",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|██████████| 202/202 [00:00<00:00, 203.88 examples/s]\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Map (num_proc=128): 100%|██████████| 202/202 [00:01<00:00, 137.99 examples/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_data,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 300,\n",
    "        learning_rate = 1e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.1,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "988864bf-76f4-4e8e-a194-c2ba06e28ee2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|><|begin_of_text|><|start_header_id|>assistant<|end_header_id|>\\n\\nkateacuff: Well, this. Let's see, will this be open to the public or school board members? Yes.<|eot_id|>\\n\\n<|start_header_id|>user<|end_header_id|>\\n\\nkwicks: You will have to request an invitation and get the Zoom link sent directly to you.\\njonnoalcaro: Karen, I don't have a question, but I've already put it on my calendar, and I'd love to get an invitation.\\nkwicks: Fantastic. We will get one out to each of you. That would be great.\\nunknownspeaker: Yeah, I'd like to have one, too.\\nbernardhairston: You can actually click on the link in the PowerPoint in front of you, and it will allow you to register.\\njenniferjohnston: Okay, doesn't look like it's working, but all right. It's located on the announcement section of the electronic school board.\\nbernardhairston: We will also be working with our communications department to advertise this through the media and also our sources of communicating, such as Blackboard Connect with All About Families.\\nkwicks: Oh, also wanted to just update you. The students have named themselves the Student Equity and Advisory Team, otherwise known as SEAT. And the motto that they came up with is that they want to give everyone a seat at the table. So pretty proud of them.\\nunknownspeaker: That's great work, Karen. Thank you.\\nkwicks: Thank you, Dr. Haas. Thanks for your time, everybody. Have a good evening and stay safe. Thank you.\\nunknownspeaker: Got a couple of other announcements from Debbie.\\ndeboracollins: Good evening. I'm Debbie Collins, Deputy Superintendent. I have two updates for you. One is we have opened our summer school registration for our high school students, just to let you know that that will continue as we have in the past. Our high schools are, those courses are virtual, so we have no real changes there. The first day of registration, we had more students than we've ever had before on our first day. At this point, we have 360 students registered. That registration is open until June 5th. Typically, we have about 400 students that come to summer high school, summer school. Most of the students that have registered so far are in PE and health, which is typically our trend. And we have about 35 students registered for econ and personal finance. So just wanted to let you know, if you hear about summer school for high school, that's, we're continuing in the fashion we've been doing for the last several years. The second item I have is just to let you know that the Return to School Task Force has convened and met twice this week. We are each, there's four areas of focus. We're looking at healthy students, healthy environments, healthy staff, and healthy instruction. There is a key central office person and a principal that are leading each of those smaller work groups. And at this point, it looks like about 12 educators on each of the work panels. So it looks at this point, we probably have over 60 educators, now Markham Public Schools, working on this document that we will have finished by June 1st. And then we'll begin to vet with multiple groups, health advisory, teacher advisory, et cetera. And then we'll be bringing that to you at your June 11th meeting. And this is a guidance document that will help us make sure that we're understanding what our steps are when we return to school. Thank you, Dr. Haas.<|eot_id|>\\n\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f31cd3e-31f1-416b-97cb-59223b48a4ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"      kateacuff: Well, this. Let's see, will this be open to the public or school board members? Yes.<|eot_id|>\\n\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b0938c3-96d6-4fd8-8009-64422e5c6406",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 202 | Num Epochs = 6 | Total steps = 300\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 4 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 10,485,760/8,000,000,000 (0.13% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/300 00:01 < 06:27, 0.77 it/s, Epoch 0.04/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.287700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399b01dd-d94b-44f9-8bdc-3e37c136555c",
   "metadata": {},
   "source": [
    "## Single Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d364562c-cf36-4ebf-b96f-6b935db9fad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" She's down there. Oh, she's there with her. There she is.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_completion_data[0]['completion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "920b367b-1060-4cc1-a1b9-e4c2338b7175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\ngrahampaige: Virginia Freedom of Information Act, Section 2-23711A of the Code of Virginia, under subsection 1, for the discussion, consideration, or interviews of prospective candidates for employment in the assignment, appointment, promotion, performance, demotion, salaries, disciplining, or resignation of public officers, appointees, or employees of any public body. Do I have a second?\\nunknownspeaker: Second. We have a motion and a second. Ms. Johnston? Ms. Colson? Yes. Ms. Osborne? Yes. Dr. Acuff? Yes. Mr. Page? Yes. Mr. Alcaro? Yes.\\njonnoalcaro: The motion passes. We are in closed session. See you on the other side.\\nunknownspeaker: Bye. Hello again. Is that Dave's dog?\\ngrahampaige: No, actually, I think that one of those is mine. Oh, OK. We have four 19 and 20-year-olds hanging out doing a bonfire tonight.\\nunknownspeaker: Oh, OK.\\njonnoalcaro: And so this is going to excite the dogs all evening long. So I'm going to keep myself on mute as much as I can.\\ngrahampaige: I hope you don't have to go out to fight a forest fire or anything. Me too.\\nunknownspeaker: I'm so on board with that. Right. So I'm starting my fifth hour on Zoom today. Wow. You're muted, Kate. What are the courts doing during this time? Hi, Karen. Exceptional question. Basically, we're 100,000 cases behind or something. We are just missing Katrina.<|eot_id|>\\n\\n<|start_header_id|>assistant<|end_header_id|>\\n\\nkateacuff:\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_completion_data[0]['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38715777-8fb5-4285-b188-48aa6e1a60a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The goal in making a prediction about vaccine development is a fascinating area of modeling, isn't it? So, I'd love to help you dive deeper, perhaps to explore modeling complex interactions or making predictive decisions with some level of uncertainty. I'm more than just a statistical tool - I'm here to assist in various domains!\n",
      "\n",
      "When modeling vaccination outcomes, a crucial approach is to use machine learning or simulate dynamic models using techniques, such as, say agent based modeling using techniques like ABM in MATLAB.\n",
      "\n",
      "A crucial area of concern here might be how many people to vaccinate, to stop any particular infection from spreading. So, a model could\n"
     ]
    }
   ],
   "source": [
    "input_text = train_completion_data[5]['prompt']\n",
    "\n",
    "inputs = tokenizer(\n",
    "    input_text,  # your full prompt string\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "# Create a streamer to print output as it's generated (optional)\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "# Generate a response\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=128,\n",
    "    use_cache=True,\n",
    "    temperature=1.5,\n",
    "    top_p=0.9  # (use `top_p`, not `min_p`)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a20cc8-789d-4aa5-a30e-78333351b102",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "626da8fe-9e63-44da-9f46-bb79539fa8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "def compute_perplexity(\n",
    "    model,\n",
    "    dataset,\n",
    "    tokenizer,\n",
    "    max_length=1024,\n",
    "    device=None,\n",
    "    verbose=True):\n",
    "    \"\"\"\n",
    "    Compute average perplexity of model predicting 'completion' given 'prompt'.\n",
    "\n",
    "    Parameters:\n",
    "    - model: causal LM\n",
    "    - dataset: list of dicts with keys 'prompt' and 'completion'\n",
    "    - tokenizer: tokenizer matching model\n",
    "    - max_length: max tokens to feed into model\n",
    "    - device: torch device\n",
    "    - verbose: if True, print per-example details\n",
    "\n",
    "    Returns:\n",
    "    - perplexity (float)\n",
    "    - generated_texts (list of dicts with 'prompt', 'completion', 'decoded_completion')\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = device or (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "    model.to(device)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    generated_texts = []\n",
    "    reference_texts = []\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        print(\"No pad token set, assigning eos_token as pad_token\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    for idx, example in enumerate(tqdm(dataset, desc=\"Computing perplexity\")):\n",
    "        prompt_text = example['prompt']\n",
    "        completion_text = example['completion']\n",
    "\n",
    "        prompt_tokens = tokenizer(prompt_text, return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "        completion_tokens = tokenizer(completion_text, return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "\n",
    "        input_ids = torch.cat([prompt_tokens, completion_tokens], dim=1)\n",
    "\n",
    "        if input_ids.size(1) > max_length:\n",
    "            input_ids = input_ids[:, -max_length:]\n",
    "\n",
    "        prompt_len = prompt_tokens.size(1)\n",
    "        if input_ids.size(1) < prompt_len:\n",
    "            prompt_len = input_ids.size(1)\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        labels[:, :prompt_len] = -100\n",
    "\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "        attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
    "\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Skipping example {idx + 1} due to model error: {e}\")\n",
    "            print(f\"input_ids shape: {input_ids.shape}, prompt_len: {prompt_len}\")\n",
    "            continue\n",
    "\n",
    "        completion_token_count = (labels != -100).sum().item()\n",
    "        total_loss += loss.item() * completion_token_count\n",
    "        total_tokens += completion_token_count\n",
    "\n",
    "        decoded_completion = tokenizer.decode(input_ids[0, prompt_len:], skip_special_tokens=True)\n",
    "\n",
    "        # Save to generated_texts\n",
    "        generated_texts.append(decoded_completion)\n",
    "        reference_texts.append(completion_text)\n",
    "\n",
    "\n",
    "    if total_tokens == 0:\n",
    "        print(\"⚠️ No valid completion tokens found, returning inf perplexity\")\n",
    "        return float('inf'), generated_texts\n",
    "\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = math.exp(avg_loss)\n",
    "\n",
    "    print(f\"\\n✅ Overall average loss per token: {avg_loss:.4f}\")\n",
    "    print(f\"✅ Overall perplexity: {perplexity:.2f}\")\n",
    "\n",
    "    return perplexity, generated_texts, reference_texts\n",
    "\n",
    "def compute_metrics(generated_texts, reference_texts):\n",
    "\n",
    "    # Compute metrics\n",
    "    bleu_score = bleu.compute(predictions=generated_texts, references=[[r] for r in reference_texts])\n",
    "    rouge_score = rouge.compute(predictions=generated_texts, references=reference_texts)\n",
    "    bertscore_result = bertscore.compute(predictions=generated_texts, references=reference_texts, lang=\"en\")\n",
    "\n",
    "    # Average BERTScore F1\n",
    "    avg_bertscore_f1 = sum(bertscore_result['f1']) / len(bertscore_result['f1'])\n",
    "            \n",
    "    return bleu_score, rouge_score, bertscore_result, avg_bertscore_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95d0ea53-d13a-4a79-b597-987fb8e01e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing perplexity:  24%|██▍       | 59/242 [00:07<00:22,  8.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "❌ Skipping example 58 due to model error: \n",
      "input_ids shape: torch.Size([1, 863]), prompt_len: 837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing perplexity:  28%|██▊       | 68/242 [00:08<00:21,  8.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "❌ Skipping example 67 due to model error: \n",
      "input_ids shape: torch.Size([1, 851]), prompt_len: 740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing perplexity: 100%|██████████| 242/242 [00:28<00:00,  8.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Overall average loss per token: 0.6880\n",
      "✅ Overall perplexity: 1.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#test_data, train_completion_data\n",
    "perplexity, generated_texts, reference_texts = compute_perplexity(model,\n",
    "                                                                train_completion_data,\n",
    "                                                                tokenizer,\n",
    "                                                                max_length=1024,\n",
    "                                                                device=None,\n",
    "                                                                verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bf7a9b-c215-4cd2-bc1c-3652967e0c07",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find a module script at /playpen-ssd/smerrill/llm_decisions/notebooks/bertscore/bertscore.py. Module 'bertscore' doesn't exist on the Hugging Face Hub either.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevaluate\u001b[39;00m\u001b[38;5;66;03m# Initialize metrics\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#bleu = evaluate.load(\"bleu\")\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#rouge = evaluate.load(\"rouge\")\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m bertscore \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbertscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m bleu_score, rouge_score, bertscore_result, avg_bertscore_f1 \u001b[38;5;241m=\u001b[39m compute_metrics(generated_texts, reference_texts)\n",
      "File \u001b[0;32m/playpen-ssd/smerrill/hf_env/lib/python3.10/site-packages/evaluate/loading.py:748\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, config_name, module_type, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, revision, **init_kwargs)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load a [`~evaluate.EvaluationModule`].\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \n\u001b[1;32m    705\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    747\u001b[0m download_mode \u001b[38;5;241m=\u001b[39m DownloadMode(download_mode \u001b[38;5;129;01mor\u001b[39;00m DownloadMode\u001b[38;5;241m.\u001b[39mREUSE_DATASET_IF_EXISTS)\n\u001b[0;32m--> 748\u001b[0m evaluation_module \u001b[38;5;241m=\u001b[39m \u001b[43mevaluation_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m evaluation_cls \u001b[38;5;241m=\u001b[39m import_main_class(evaluation_module\u001b[38;5;241m.\u001b[39mmodule_path)\n\u001b[1;32m    752\u001b[0m evaluation_instance \u001b[38;5;241m=\u001b[39m evaluation_cls(\n\u001b[1;32m    753\u001b[0m     config_name\u001b[38;5;241m=\u001b[39mconfig_name,\n\u001b[1;32m    754\u001b[0m     process_id\u001b[38;5;241m=\u001b[39mprocess_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs,\n\u001b[1;32m    761\u001b[0m )\n",
      "File \u001b[0;32m/playpen-ssd/smerrill/hf_env/lib/python3.10/site-packages/evaluate/loading.py:681\u001b[0m, in \u001b[0;36mevaluation_module_factory\u001b[0;34m(path, module_type, revision, download_config, download_mode, force_local_path, dynamic_modules_path, **download_kwargs)\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, (\u001b[38;5;167;01mConnectionError\u001b[39;00m, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m)):\n\u001b[1;32m    680\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m    682\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a module script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    683\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist on the Hugging Face Hub either.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    684\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a module script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find a module script at /playpen-ssd/smerrill/llm_decisions/notebooks/bertscore/bertscore.py. Module 'bertscore' doesn't exist on the Hugging Face Hub either."
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "bleu_score, rouge_score, bertscore_result, avg_bertscore_f1 = compute_metrics(generated_texts, reference_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f49a8d4-c73d-4c37-bf0e-13146b66e0c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
