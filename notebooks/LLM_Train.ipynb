{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb166938-4de5-4673-bbc8-2cb4ff6458d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87964f3-d6aa-48cf-b3d0-04b1bdc05918",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e78d60f-a9c0-4771-b9bf-9a0b1c761305",
   "metadata": {},
   "outputs": [],
   "source": [
    "alicia_dataset = np.load('datasets/alicia_dataset.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12c6d4f0-312a-4652-a42f-2feaa791de24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['context', 'response'],\n",
      "    num_rows: 12\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "hf_dataset = Dataset.from_list(alicia_dataset.tolist())\n",
    "print(hf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4217dc08-fe61-42ca-9e5a-c7a67f2be60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_qwen_chat(example):\n",
    "    return {\n",
    "        \"text\": f\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n{example['context']}<|im_end|>\\n<|im_start|>assistant\\n{example['response']}<|im_end|>\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b58a1e-c8c4-4060-bf49-72bffa181003",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_dataset = hf_dataset.map(format_qwen_chat)\n",
    "print(formatted_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142ed193-be6b-4a99-a7a1-48b2d2fd167a",
   "metadata": {},
   "source": [
    "### Load Model LoRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575e6830-1fa4-449a-8ae8-1405c182fe7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Could not load bitsandbytes native library: /lib64/libc.so.6: version `GLIBC_2.34' not found (required by /nas/longleaf/home/smerrill/.conda/envs/llm/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda126.so)\n",
      "Traceback (most recent call last):\n",
      "  File \"/nas/longleaf/home/smerrill/.conda/envs/llm/lib/python3.9/site-packages/bitsandbytes/cextension.py\", line 85, in <module>\n",
      "    lib = get_native_library()\n",
      "  File \"/nas/longleaf/home/smerrill/.conda/envs/llm/lib/python3.9/site-packages/bitsandbytes/cextension.py\", line 72, in get_native_library\n",
      "    dll = ct.cdll.LoadLibrary(str(binary_path))\n",
      "  File \"/nas/longleaf/home/smerrill/.conda/envs/llm/lib/python3.9/ctypes/__init__.py\", line 460, in LoadLibrary\n",
      "    return self._dlltype(name)\n",
      "  File \"/nas/longleaf/home/smerrill/.conda/envs/llm/lib/python3.9/ctypes/__init__.py\", line 382, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "OSError: /lib64/libc.so.6: version `GLIBC_2.34' not found (required by /nas/longleaf/home/smerrill/.conda/envs/llm/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda126.so)\n",
      "\n",
      "CUDA Setup failed despite CUDA being available. Please run the following command to get more information:\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      "Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n",
      "to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n",
      "and open an issue at: https://github.com/bitsandbytes-foundation/bitsandbytes/issues\n",
      "\n",
      "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,  # or use 4bit for more savings\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d80a7d-c46c-4404-a466-3ddfc1c3294e",
   "metadata": {},
   "source": [
    "### Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fca64cd-4f16-45d2-b956-ed3f53bd4fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, max_length=1024, padding=\"max_length\")\n",
    "\n",
    "tokenized_dataset = formatted_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a01959-43cf-42e8-9aab-a80a83ff91f4",
   "metadata": {},
   "source": [
    "### Set up Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161d04d6-7340-4ffe-84a9-cbdd80833829",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora-alicia-model\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b6f188-fd94-4a4f-93e9-2650854f3449",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained(\"./lora-alicia-model\")\n",
    "tokenizer.save_pretrained(\"./lora-alicia-model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
