{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2a6ba2b0-80d1-4198-9011-9ff95941d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "from transformers import TextStreamer\n",
    "from unsloth.chat_templates import train_on_responses_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "fc0adaa6-1f1e-4a20-b49a-d4b6555c1862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(member: str, test_size: float = 0.2, seed: int = 42, data_path: str = '/work/users/s/m/smerrill/Albemarle/dataset') -> Tuple[List[dict], List[dict]]:\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and test sets. Synthetic data is always added to the training set.\n",
    "\n",
    "    Parameters:\n",
    "    - member: The name identifier for the board member.\n",
    "    - test_size: Proportion of the real (non-synthetic) data to include in the test split.\n",
    "    - seed: Random seed for reproducibility.\n",
    "    - data_path: Base directory for the dataset files.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple (train_data, test_data)\n",
    "    \"\"\"\n",
    "    real_data, synth_data = [], []\n",
    "\n",
    "    if member == 'acuff':\n",
    "        real_data = load_chat_dataset(os.path.join(data_path, 'kateacuff.txt'))\n",
    "        synth_data = load_chat_dataset(os.path.join(data_path, 'synth_kateacuff.txt'))\n",
    "    elif member == 'osborne':\n",
    "        real_data = load_chat_dataset(os.path.join(data_path, 'ellenosborne.txt'))\n",
    "        synth_data = load_chat_dataset(os.path.join(data_path, 'synth_ellenosborne.txt'))\n",
    "    elif member == 'paige':\n",
    "        real_data = load_chat_dataset(os.path.join(data_path, 'grahampaige.txt'))\n",
    "        synth_data = load_chat_dataset(os.path.join(data_path, 'synth_grahampaige.txt'))\n",
    "    elif member == 'le':\n",
    "        real_data = load_chat_dataset(os.path.join(data_path, 'judyle.txt'))\n",
    "        synth_data = load_chat_dataset(os.path.join(data_path, 'synth_judyle.txt'))\n",
    "    elif member == 'callsen':\n",
    "        real_data = load_chat_dataset(os.path.join(data_path, 'katrinacallsen.txt'))\n",
    "    elif member == 'oberg':\n",
    "        real_data = load_chat_dataset(os.path.join(data_path, 'davidoberg.txt'))\n",
    "    elif member == 'alcaro':\n",
    "        real_data = load_chat_dataset(os.path.join(data_path, 'jonnoalcaro.txt'))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown member: {member}\")\n",
    "\n",
    "    if not 0 < test_size < 1:\n",
    "        raise ValueError(\"test_size must be a float between 0 and 1.\")\n",
    "\n",
    "    # Shuffle and split only the real data\n",
    "    random.seed(seed)\n",
    "    shuffled_real = real_data.copy()\n",
    "    random.shuffle(shuffled_real)\n",
    "\n",
    "    split_index = int(len(shuffled_real) * (1 - test_size))\n",
    "    train_data = shuffled_real[:split_index] + synth_data\n",
    "    test_data = shuffled_real[split_index:]\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def load_chat_dataset(input_path):\n",
    "    \"\"\"\n",
    "    Load a chat-style message dataset from a JSON or JSONL file.\n",
    "    \n",
    "    Returns:\n",
    "        data (list): A list of message dictionaries.\n",
    "    \"\"\"\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        if input_path.endswith(\".jsonl\"):\n",
    "            data = [json.loads(line) for line in f]\n",
    "        else:  # Assume .json\n",
    "            data = json.load(f)\n",
    "    \n",
    "    print(f\"Dataset loaded from: {input_path}\")\n",
    "    print(f\"Total examples: {len(data)}\")\n",
    "    return data\n",
    "\n",
    "def convert_to_chat_format(data):\n",
    "    result = []\n",
    "    for item in data:\n",
    "        result.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": item['prompt']\n",
    "        })\n",
    "        result.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": item['response']\n",
    "        })\n",
    "    return result\n",
    "\n",
    "def combine_conversations(dataset):\n",
    "    # Combine role and content into a list of dicts (conversation format)\n",
    "    conversations = []\n",
    "    # Assuming each row corresponds to a single message, group by conversation if possible\n",
    "    # If it's just single messages in sequence, we can pair messages by user-assistant or whatever fits your logic\n",
    "    # Here, I'll assume consecutive rows alternate roles and should be paired into conversations\n",
    "\n",
    "    new_data = []\n",
    "    i = 0\n",
    "    while i < len(dataset):\n",
    "        # Take two rows at a time: user + assistant\n",
    "        if i + 1 < len(dataset):\n",
    "            convo = [\n",
    "                #{\"role\": 'system', \"content\": \"A user will pass you dialog history for a conversation.  Your job is to predict the next response by Graham Paige by filling in the response after grahampaige:\"},\n",
    "                {\"role\": dataset[i]['role'], \"content\": dataset[i]['content']},\n",
    "                {\"role\": dataset[i+1]['role'], \"content\": dataset[i+1]['content']}\n",
    "            ]\n",
    "            new_data.append({\"conversations\": convo})\n",
    "            i += 2\n",
    "        else:\n",
    "            # If odd number of rows, last one alone\n",
    "            convo = [{\"role\": dataset[i]['role'], \"content\": dataset[i]['content']}]\n",
    "            new_data.append({\"conversations\": convo})\n",
    "            i += 1\n",
    "\n",
    "    return Dataset.from_list(new_data)\n",
    "\n",
    "\n",
    "# Function to replace the system message in the text\n",
    "def replace_system_message(example):\n",
    "    parts = example[\"text\"].split(\"<|eot_id|>\", 1)\n",
    "    if len(parts) == 2:\n",
    "        # Replace system block only if the format is correct\n",
    "        new_text = custom_system_message + parts[1]\n",
    "    else:\n",
    "        new_text = example[\"text\"]  # Fallback: leave unchanged\n",
    "    return {\"text\": new_text}\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False).removeprefix(tokenizer.bos_token) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "8bde8812-08f4-4457-8da9-7cbd9876599e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded from: /work/users/s/m/smerrill/Albemarle/dataset/grahampaige.txt\n",
      "Total examples: 896\n",
      "Dataset loaded from: /work/users/s/m/smerrill/Albemarle/dataset/synth_grahampaige.txt\n",
      "Total examples: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 766/766 [00:00<00:00, 13936.02 examples/s]\n",
      "Map: 100%|██████████| 180/180 [00:00<00:00, 11231.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split('paige')\n",
    "\n",
    "train_data = convert_to_chat_format(train_data)\n",
    "test_data = convert_to_chat_format(test_data)\n",
    "\n",
    "train_data = combine_conversations(train_data)\n",
    "test_data = combine_conversations(test_data)\n",
    "\n",
    "train_data = train_data.map(formatting_prompts_func, batched = True,)\n",
    "test_data = test_data.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "8c0fcf1b-f15a-4bac-8f03-ec2617140b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 766/766 [00:00<00:00, 16750.54 examples/s]\n",
      "Map: 100%|██████████| 180/180 [00:00<00:00, 17192.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "agent_name = \"Graham Paige\"\n",
    "agent_name_lower = agent_name.replace(' ', '').lower()\n",
    "\n",
    "message = \"\"\"You are {agent_name}. Fill in the next response to the conversation by continuing the dialogue naturally. Only write what {agent_name} would say next — do not write for other speakers.\n",
    "\n",
    "Example:\n",
    "user:  \n",
    "{agent_name_lower}: Good evening, everyone. Let's get started with the agenda.  \n",
    "Speaker_1: Thanks, {agent_name}. I just had a quick question about the minutes from last time.  \n",
    "assistant:  \n",
    "{agent_name_lower}: Sure, go ahead with your question.\"\"\".format(\n",
    "    agent_name=agent_name, agent_name_lower=agent_name_lower\n",
    ")\n",
    "\n",
    "# Define your custom system message\n",
    "custom_system_message = f\"<|start_header_id|>system<|end_header_id|>\\n\\n{message}\\n<|eot_id|>\"\n",
    "train_data = train_data.map(replace_system_message)\n",
    "test_data = test_data.map(replace_system_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "3939759d-e803-4bda-9173-fc6144059939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|start_header_id|>system<|end_header_id|>\\n\\nYou are Graham Paige. Fill in the next response to the conversation by continuing the dialogue naturally. Only write what Graham Paige would say next — do not write for other speakers.\\n\\nExample:\\nuser:  \\ngrahampaige: Good evening, everyone. Let's get started with the agenda.  \\nSpeaker_1: Thanks, Graham Paige. I just had a quick question about the minutes from last time.  \\nassistant:  \\ngrahampaige: Sure, go ahead with your question.\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nSpeaker_2: OK. I'm interested in that. Are you? Yes, I mean, if you want to be back on it. I mean, it's three years, so who knows? But I mean, I don't know if that's a concern for these appointments.\\nSpeaker_2: OK. So Ms. Lee, are you volunteering for that?\\nSpeaker_2: I would like to volunteer for that.\\nSpeaker_2: OK.\\ngrahampaige: Are there any other volunteers for the three-year term on the KTEC board? If not, thank you very much, Ms. Lee, for stepping up for that position, thank you so much.\\nSpeaker_2: Thank you.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\ngrahampaige: Okay, we now move to the prep committee. And currently, Ms. Lee and me, the two of us are currently on our board. So we could have two new people if we have two volunteers, unless Ms. Lee wants to volunteer again.<|eot_id|>\""
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f107fb0a-a9ca-4ce7-8e1d-28680da5612d",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "a40ea729-35a8-4096-b1ac-e7fc71d5c325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.7: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA A100-PCIE-40GB. Num GPUs = 1. Max memory: 39.495 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu118. CUDA: 8.0. CUDA Toolkit: 11.8. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 2048\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 4, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "9df4c343-1230-4780-aa32-4b16744ee5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|██████████| 766/766 [00:01<00:00, 456.97 examples/s]\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Map (num_proc=128): 100%|██████████| 766/766 [00:02<00:00, 264.68 examples/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_data,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 1e-5,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.15,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "988864bf-76f4-4e8e-a194-c2ba06e28ee2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are Graham Paige. Fill in the next response to the conversation by continuing the dialogue naturally. Only write what Graham Paige would say next — do not write for other speakers.\\n\\nExample:\\nuser:  \\ngrahampaige: Good evening, everyone. Let's get started with the agenda.  \\nSpeaker_1: Thanks, Graham Paige. I just had a quick question about the minutes from last time.  \\nassistant:  \\ngrahampaige: Sure, go ahead with your question.\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\ngrahampaige: We now have a break. It's about 805, no 807, so we'll break until 817. Okay.\\nSpeaker_1: Hi, Mr. Page.\\ngrahampaige: Hey, Dr. Heston, how are you? Doing well. Good, good. OK, it is now 8.17, and we will proceed with our next item on the agenda. Anti-racism policy update, Dr. Bernard Harrison. Dr. Harrison.\\nSpeaker_1: Ms. Johnson, shall I share my screen? Yes. Thank you, ma'am. You see a full screen?\\nSpeaker_1: Yeah. Very good.\\nSpeaker_1: So are we seeing a full screen? All right. Sorry. You see a full screen? Yep. Yes. OK. So good evening board members, Dr. Haas as well, and the community. I am Bernard Hairston, your assistant superintendent of School and Community Empowerment. I will be joined by Jasmine Fernandez this evening, the project manager of the Anti-Racism Steering Committee and Policy Implementation Team. We will present an overview and update to the Anti-Racism Policy Implementation Plan since our last presentation to the board on February 13th, 2020. We will share how the policy regulations were strategically outlined into a logic model design that had the built-in key activities, key strategies, and progressive outline measurable designs built in. In February, we presented examples of startup initiatives, such as kicking off a student advisory group, launching a review of the equity survey interviews, from teachers of color and steps to identify equity needs assessment tools. Today, what we would like to do is to provide you with some updates on those action steps to these startup initiatives that we presented back in February, as well as some other initiatives. Sorry about that. The anti-racism policy statement is being promoted as part of our division level staff meetings. It is presented with equal value. to our mission and vision statement, our equity mission statement, and the strategic priorities, values, and goals of our school division. And it's presented as a top priority message to staff and the community. It reads, Albemarle County Public Schools is committed to establishing and sustaining an equitable community that achieves the school division's equity mission to end the predictive value of race and ensure each individual student's and staff's success. Yes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\ngrahampaige: That's not appearing. The slide's a little bit off. OK, there it is. It's there now. OK.<|eot_id|>\""
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "e17b7e3f-2cd2-4484-8031-724bbde68d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             grahampaige: That's not appearing. The slide's a little bit off. OK, there it is. It's there now. OK.<|eot_id|>\""
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "6b0938c3-96d6-4fd8-8009-64422e5c6406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 766 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 6,078,464/3,000,000,000 (0.20% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 01:08, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.428300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.287800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.404600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.557700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.514000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.003700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.488800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.730500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.334600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.533300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.590700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.271500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.452600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.189200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.577400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.653400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.501800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.271400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.741200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.570800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.441900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.419400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.278300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.323100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.533300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.530300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.213700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.154700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.196700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.530400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.159700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.450100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.128600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.179900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.113700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.099800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.142400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.173500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.154100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.358600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.132900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.181100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.081600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.111400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.080800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.127900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.163100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.238100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.062100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.089000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.132700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.142400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.402900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.103800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.076900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.257200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.273600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.070300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.070300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "890ec7b4-0bc6-483e-8971-5f491b8721d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "399b01dd-d94b-44f9-8bdc-3e37c136555c",
   "metadata": {},
   "source": [
    "## Single Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "007a0c3b-0ec7-4dfe-a32c-595da055a53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ah, Leila Barnes from Albemarle High. Well, Leila, thank you for coming. And welcome to all our guests. Welcome, everyone! Now that we have our representatives, let's move on to the agenda...<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "# Step 1: Apply chat template\n",
    "messages = [train_data['conversations'][1][:-1]]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,  # Required for generation\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Step 2: Replace old system message (first 26 tokens) with new one\n",
    "custom_prompt = \"<|begin_of_text|>\" + custom_system_message\n",
    "custom_prompt_tokens = tokenizer.encode(custom_prompt, return_tensors=\"pt\").to(inputs.device)\n",
    "\n",
    "# Slice off the original system message (~first 26 tokens)\n",
    "inputs = inputs[:, 26:]\n",
    "\n",
    "# Prepend the new system prompt\n",
    "inputs = torch.cat([custom_prompt_tokens, inputs], dim=1)\n",
    "\n",
    "# Step 3: Append your speaker prefix if needed\n",
    "new_tokens = torch.tensor(tokenizer.encode('grahampaige:')[1:], device=inputs.device).unsqueeze(0)\n",
    "inputs = torch.cat([inputs, new_tokens], dim=1)\n",
    "\n",
    "# Step 4: Generate\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "_ = model.generate(\n",
    "    input_ids=inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=128,\n",
    "    use_cache=True,\n",
    "    temperature=1.5,\n",
    "    min_p=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "ba602cfb-443a-4a8c-b295-f219998da2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Text: <|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are Graham Paige. Fill in the next response to the conversation by continuing the dialogue naturally. Only write what Graham Paige would say next — do not write for other speakers.\n",
      "\n",
      "Example:\n",
      "user:  \n",
      "grahampaige: Good evening, everyone. Let's get started with the agenda.  \n",
      "Speaker_1: Thanks, Graham Paige. I just had a quick question about the minutes from last time.  \n",
      "assistant:  \n",
      "grahampaige: Sure, go ahead with your question.\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "grahampaige: Mr. John O. Alcaro. John O. Alcaro at large. Ms. Colson is absent at this time. Ms. Judy Lee.\n",
      "Speaker_1: Judy Lee, Rivanna District.\n",
      "Speaker_3: Mr. Dave Oberg. Dave Oberg, Whitehall Magisterial District. Ms. Ellen Osborne.\n",
      "Speaker_1: Ellen Osborne, Scottsville District.\n",
      "grahampaige: And I'm Graham Page, Samuel Miller District. Our student rep, I don't think, is here at this time. Is she? I'm here. Hi. Sorry, Leila. I'm so very sorry. OK, so introduce yourself as our rep.\n",
      "Speaker_4: Hi, my name is Leila Barnes, and I'm the student rep from Albemarle High School.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "grahampaige:\n"
     ]
    }
   ],
   "source": [
    "# Get first example token IDs as a list\n",
    "input_ids_list = inputs[0].tolist()\n",
    "\n",
    "# Decode back to text\n",
    "decoded_text = tokenizer.decode(input_ids_list)\n",
    "\n",
    "print(\"Decoded Text:\", decoded_text)\n",
    "\n",
    "# Show tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids_list)\n",
    "#print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861c7f9c-7615-4178-9dbd-92139f281ef0",
   "metadata": {},
   "source": [
    "### Full evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "624b8c1e-b0dd-4522-9441-ad9168a226dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [05:17<00:00,  1.76s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Predictions and/or references don't match the expected format.\nExpected format:\nFeature option 0: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}\nFeature option 1: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')},\nInput predictions: [['Thank', 'you,', 'thank', 'you.', \"That's\", 'all', 'for', 'now.']],\nInput references: [['grahampaige:', 'Okay,', 'we', 'now', 'move', 'to', 'the', 'prep', 'committee.', 'And', 'currently,', 'Ms.', 'Lee', 'and', 'me,', 'the', 'two', 'of', 'us', 'are', 'currently', 'on', 'our', 'board.', 'So', 'we', 'could', 'have', 'two', 'new', 'people', 'if', 'we', 'have', 'two', 'volunteers,', 'unless', 'Ms.', 'Lee', 'wants', 'to', 'volunteer', 'again.']]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[333]\u001b[39m\u001b[32m, line 64\u001b[39m\n\u001b[32m     62\u001b[39m bleu_preds = [[pred.split()] \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m generated_texts]\n\u001b[32m     63\u001b[39m bleu_refs = [[ref.split()] \u001b[38;5;28;01mfor\u001b[39;00m ref \u001b[38;5;129;01min\u001b[39;00m reference_texts]\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m bleu_score = \u001b[43mbleu\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbleu_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbleu_refs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# ROUGE\u001b[39;00m\n\u001b[32m     67\u001b[39m rouge_score = rouge.compute(predictions=generated_texts, references=reference_texts)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/users/s/m/smerrill/.conda/envs/unsloth_env/lib/python3.11/site-packages/evaluate/module.py:455\u001b[39m, in \u001b[36mEvaluationModule.compute\u001b[39m\u001b[34m(self, predictions, references, **kwargs)\u001b[39m\n\u001b[32m    452\u001b[39m compute_kwargs = {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._feature_names()}\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs.values()):\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[38;5;28mself\u001b[39m._finalize()\n\u001b[32m    458\u001b[39m \u001b[38;5;28mself\u001b[39m.cache_file_name = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/users/s/m/smerrill/.conda/envs/unsloth_env/lib/python3.11/site-packages/evaluate/module.py:514\u001b[39m, in \u001b[36mEvaluationModule.add_batch\u001b[39m\u001b[34m(self, predictions, references, **kwargs)\u001b[39m\n\u001b[32m    512\u001b[39m batch = {input_name: batch[input_name] \u001b[38;5;28;01mfor\u001b[39;00m input_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._feature_names()}\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m514\u001b[39m     \u001b[38;5;28mself\u001b[39m.selected_feature_format = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_infer_feature_from_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    515\u001b[39m     \u001b[38;5;28mself\u001b[39m._init_writer()\n\u001b[32m    516\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/users/s/m/smerrill/.conda/envs/unsloth_env/lib/python3.11/site-packages/evaluate/module.py:596\u001b[39m, in \u001b[36mEvaluationModule._infer_feature_from_batch\u001b[39m\u001b[34m(self, batch)\u001b[39m\n\u001b[32m    594\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    595\u001b[39m     example = \u001b[38;5;28mdict\u001b[39m([(k, v[\u001b[32m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch.items()])\n\u001b[32m--> \u001b[39m\u001b[32m596\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_infer_feature_from_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/users/s/m/smerrill/.conda/envs/unsloth_env/lib/python3.11/site-packages/evaluate/module.py:616\u001b[39m, in \u001b[36mEvaluationModule._infer_feature_from_example\u001b[39m\u001b[34m(self, example)\u001b[39m\n\u001b[32m    609\u001b[39m feature_strings = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join([\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFeature option \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.features)])\n\u001b[32m    610\u001b[39m error_msg = (\n\u001b[32m    611\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPredictions and/or references don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt match the expected format.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    612\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected format:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfeature_strings\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    613\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput predictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(example[\u001b[33m'\u001b[39m\u001b[33mpredictions\u001b[39m\u001b[33m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    614\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput references: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(example[\u001b[33m'\u001b[39m\u001b[33mreferences\u001b[39m\u001b[33m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    615\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Predictions and/or references don't match the expected format.\nExpected format:\nFeature option 0: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}\nFeature option 1: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')},\nInput predictions: [['Thank', 'you,', 'thank', 'you.', \"That's\", 'all', 'for', 'now.']],\nInput references: [['grahampaige:', 'Okay,', 'we', 'now', 'move', 'to', 'the', 'prep', 'committee.', 'And', 'currently,', 'Ms.', 'Lee', 'and', 'me,', 'the', 'two', 'of', 'us', 'are', 'currently', 'on', 'our', 'board.', 'So', 'we', 'could', 'have', 'two', 'new', 'people', 'if', 'we', 'have', 'two', 'volunteers,', 'unless', 'Ms.', 'Lee', 'wants', 'to', 'volunteer', 'again.']]"
     ]
    }
   ],
   "source": [
    "import evaluate  # instead of datasets.load_metric\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize metrics\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "FastLanguageModel.for_inference(model)  # Enable fast inference\n",
    "\n",
    "agent_name = \"Graham Paige\"\n",
    "agent_name_lower = agent_name.replace(' ', '').lower()\n",
    "speaker_prefix = tokenizer.encode(f\"{agent_name_lower}:\", return_tensors=\"pt\").to(\"cuda\")[:, 1:]  # remove BOS\n",
    "\n",
    "# Store predictions and references\n",
    "generated_texts = []\n",
    "reference_texts = []\n",
    "\n",
    "for example in tqdm(test_data):  # assuming test_set is a list of dicts with 'conversations'\n",
    "    # === Step 1: Prepare messages ===\n",
    "    messages = [example['conversations'][:-1]]  # all but final message\n",
    "    reference = example['conversations'][-1]['content']  # the target response\n",
    "\n",
    "    # === Step 2: Format prompt ===\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Remove old system prompt (~first 26 tokens)\n",
    "    inputs = inputs[:, 26:]\n",
    "\n",
    "    # Insert new system prompt\n",
    "    custom_prompt = \"<|begin_of_text|>\" + custom_system_message\n",
    "    custom_prompt_tokens = tokenizer.encode(custom_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    inputs = torch.cat([custom_prompt_tokens, inputs], dim=1)\n",
    "\n",
    "    # Append agent speaker token\n",
    "    inputs = torch.cat([inputs, speaker_prefix], dim=1)\n",
    "\n",
    "    # === Step 3: Generate response ===\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        max_new_tokens=128,\n",
    "        use_cache=True,\n",
    "        temperature=1.5,\n",
    "        min_p=0.1,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    generated = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "    generated_texts.append(generated.strip())\n",
    "    reference_texts.append(reference.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "953b8183-5a09-464b-b0c1-7f3ee96ae349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Results ---\n",
      "BLEU: 0.0135\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.float64' object has no attribute 'mid'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[334]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Evaluation Results ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBLEU: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbleu_score[\u001b[33m'\u001b[39m\u001b[33mbleu\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mROUGE-L: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mrouge_score\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrougeL\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmid\u001b[49m.fmeasure\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBERTScore F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(bertscore_result[\u001b[33m'\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m'\u001b[39m])\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(bertscore_result[\u001b[33m'\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m'\u001b[39m])\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'numpy.float64' object has no attribute 'mid'"
     ]
    }
   ],
   "source": [
    "# BLEU expects list of strings (predictions) and list of lists (references)\n",
    "bleu_preds = generated_texts  # list of strings\n",
    "bleu_refs = [[ref] for ref in reference_texts]  # list of lists of strings\n",
    "\n",
    "bleu_score = bleu.compute(predictions=bleu_preds, references=bleu_refs)\n",
    "\n",
    "# ROUGE (expects strings)\n",
    "rouge_score = rouge.compute(predictions=generated_texts, references=reference_texts)\n",
    "\n",
    "# BERTScore (expects strings)\n",
    "bertscore_result = bertscore.compute(predictions=generated_texts, references=reference_texts, lang=\"en\")\n",
    "\n",
    "print(\"\\n--- Evaluation Results ---\")\n",
    "print(f\"BLEU: {bleu_score['bleu']:.4f}\")\n",
    "print(f\"ROUGE-L: {rouge_score['rougeL']:.4f}\")\n",
    "print(f\"BERTScore F1: {sum(bertscore_result['f1']) / len(bertscore_result['f1']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "ddea09b7-5379-4b27-81b0-fb2b8e2cc8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Results ---\n",
      "BLEU: 0.0135\n",
      "ROUGE-L: 0.0924\n",
      "BERTScore F1: 0.8318\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Evaluation Results ---\")\n",
    "print(f\"BLEU: {bleu_score['bleu']:.4f}\")\n",
    "print(f\"ROUGE-L: {rouge_score['rougeL']:.4f}\")\n",
    "print(f\"BERTScore F1: {sum(bertscore_result['f1']) / len(bertscore_result['f1']):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6640eac-ab83-40a8-98c2-6eae8a774e26",
   "metadata": {},
   "source": [
    "What do these scores mean?\n",
    "Metric\tYour score\tTypical interpretation\n",
    "BLEU\t0.0135\tVery low — almost no n-gram overlap with references\n",
    "ROUGE-L\t0.0924\tAlso quite low — limited overlap in longest common subsequence\n",
    "BERTScore\t0.8318\tPretty good — semantic similarity is fairly strong\n",
    "\n",
    "\n",
    "Reasonability and context:\n",
    "BLEU and ROUGE are surface-level overlap metrics. Scores near zero suggest your generated responses differ a lot word-for-word from the references.\n",
    "\n",
    "BERTScore captures semantic similarity and is more forgiving about wording changes. 0.83 is pretty decent and indicates your model’s output is somewhat close in meaning, even if wording is different.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf8a57e-f839-4eb7-8fca-56a024cdef29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "unsloth_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
