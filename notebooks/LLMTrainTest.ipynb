{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2a6ba2b0-80d1-4198-9011-9ff95941d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "from transformers import TextStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "fc0adaa6-1f1e-4a20-b49a-d4b6555c1862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(member: str, test_size: float = 0.2, seed: int = 42, data_path: str = '/work/users/s/m/smerrill/Albemarle/dataset') -> Tuple[List[dict], List[dict]]:\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and test sets. Synthetic data is always added to the training set.\n",
    "\n",
    "    Parameters:\n",
    "    - member: The name identifier for the board member.\n",
    "    - test_size: Proportion of the real (non-synthetic) data to include in the test split.\n",
    "    - seed: Random seed for reproducibility.\n",
    "    - data_path: Base directory for the dataset files.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple (train_data, test_data)\n",
    "    \"\"\"\n",
    "    real_data, synth_data = [], []\n",
    "\n",
    "    if member == 'acuff':\n",
    "        real_data = load_chat_dataset(os.path.join(data_path, 'kateacuff.txt'))\n",
    "        synth_data = load_chat_dataset(os.path.join(data_path, 'synth_kateacuff.txt'))\n",
    "    elif member == 'osborne':\n",
    "        real_data = load_chat_dataset(os.path.join(data_path, 'ellenosborne.txt'))\n",
    "        synth_data = load_chat_dataset(os.path.join(data_path, 'synth_ellenosborne.txt'))\n",
    "    elif member == 'paige':\n",
    "        real_data = load_chat_dataset(os.path.join(data_path, 'grahampaige.txt'))\n",
    "        synth_data = load_chat_dataset(os.path.join(data_path, 'synth_grahampaige.txt'))\n",
    "    elif member == 'le':\n",
    "        real_data = load_chat_dataset(os.path.join(data_path, 'judyle.txt'))\n",
    "        synth_data = load_chat_dataset(os.path.join(data_path, 'synth_judyle.txt'))\n",
    "    elif member == 'callsen':\n",
    "        real_data = load_chat_dataset(os.path.join(data_path, 'katrinacallsen.txt'))\n",
    "    elif member == 'oberg':\n",
    "        real_data = load_chat_dataset(os.path.join(data_path, 'davidoberg.txt'))\n",
    "    elif member == 'alcaro':\n",
    "        real_data = load_chat_dataset(os.path.join(data_path, 'jonnoalcaro.txt'))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown member: {member}\")\n",
    "\n",
    "    if not 0 < test_size < 1:\n",
    "        raise ValueError(\"test_size must be a float between 0 and 1.\")\n",
    "\n",
    "    # Shuffle and split only the real data\n",
    "    random.seed(seed)\n",
    "    shuffled_real = real_data.copy()\n",
    "    random.shuffle(shuffled_real)\n",
    "\n",
    "    split_index = int(len(shuffled_real) * (1 - test_size))\n",
    "    train_data = shuffled_real[:split_index] + synth_data\n",
    "    test_data = shuffled_real[split_index:]\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def load_chat_dataset(input_path):\n",
    "    \"\"\"\n",
    "    Load a chat-style message dataset from a JSON or JSONL file.\n",
    "    \n",
    "    Returns:\n",
    "        data (list): A list of message dictionaries.\n",
    "    \"\"\"\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        if input_path.endswith(\".jsonl\"):\n",
    "            data = [json.loads(line) for line in f]\n",
    "        else:  # Assume .json\n",
    "            data = json.load(f)\n",
    "    \n",
    "    print(f\"Dataset loaded from: {input_path}\")\n",
    "    print(f\"Total examples: {len(data)}\")\n",
    "    return data\n",
    "\n",
    "def convert_to_chat_format(data):\n",
    "    result = []\n",
    "    for item in data:\n",
    "        result.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": item['prompt']\n",
    "        })\n",
    "        result.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": item['response']\n",
    "        })\n",
    "    return result\n",
    "\n",
    "def combine_conversations(dataset):\n",
    "    # Combine role and content into a list of dicts (conversation format)\n",
    "    conversations = []\n",
    "    # Assuming each row corresponds to a single message, group by conversation if possible\n",
    "    # If it's just single messages in sequence, we can pair messages by user-assistant or whatever fits your logic\n",
    "    # Here, I'll assume consecutive rows alternate roles and should be paired into conversations\n",
    "\n",
    "    new_data = []\n",
    "    i = 0\n",
    "    while i < len(dataset):\n",
    "        # Take two rows at a time: user + assistant\n",
    "        if i + 1 < len(dataset):\n",
    "            convo = [\n",
    "                {\"role\": dataset[i]['role'], \"content\": dataset[i]['content']},\n",
    "                {\"role\": dataset[i+1]['role'], \"content\": dataset[i+1]['content']}\n",
    "            ]\n",
    "            new_data.append({\"conversations\": convo})\n",
    "            i += 2\n",
    "        else:\n",
    "            # If odd number of rows, last one alone\n",
    "            convo = [{\"role\": dataset[i]['role'], \"content\": dataset[i]['content']}]\n",
    "            new_data.append({\"conversations\": convo})\n",
    "            i += 1\n",
    "\n",
    "    return Dataset.from_list(new_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9f0fe2c2-12ff-470c-891b-5b681395f99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded from: /work/users/s/m/smerrill/Albemarle/dataset/grahampaige.txt\n",
      "Total examples: 896\n",
      "Dataset loaded from: /work/users/s/m/smerrill/Albemarle/dataset/synth_grahampaige.txt\n",
      "Total examples: 50\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split('paige')\n",
    "train_data = convert_to_chat_format(train_data)\n",
    "train_data = combine_conversations(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b58a00-50fd-4f14-972c-11b2a110a873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "a40ea729-35a8-4096-b1ac-e7fc71d5c325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.7: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA A100-PCIE-40GB. Num GPUs = 1. Max memory: 39.495 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu118. CUDA: 8.0. CUDA Toolkit: 11.8. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 2048\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "484b558e-d7bd-4413-8561-59c327cd32a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "ac9eda28-ae6a-47c1-935b-480d9d39acfa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 766/766 [00:00<00:00, 17129.65 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a9ea52-2728-455f-8922-07809440efe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "9df4c343-1230-4780-aa32-4b16744ee5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|██████████| 766/766 [00:01<00:00, 495.96 examples/s]\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Map (num_proc=128): 100%|██████████| 766/766 [00:02<00:00, 351.33 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_data,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-5,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.2,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "a7c45f31-7923-41c1-b4f7-72cb2eaca819",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nSpeaker_1: Hi, Mr. Page.\\ngrahampaige: Hey, Dr. Heston, how are you? Doing well. Good, good. OK, it is now 8.17, and we will proceed with our next item on the agenda. Anti-racism policy update, Dr. Bernard Harrison. Dr. Harrison.\\nSpeaker_1: Ms. Johnson, shall I share my screen? Yes. Thank you, ma'am. You see a full screen?\\nSpeaker_1: Yeah. Very good.\\nSpeaker_1: So are we seeing a full screen? All right. Sorry. You see a full screen? Yep. Yes. OK. So good evening board members, Dr. Haas as well, and the community. I am Bernard Hairston, your assistant superintendent of School and Community Empowerment. I will be joined by Jasmine Fernandez this evening, the project manager of the Anti-Racism Steering Committee and Policy Implementation Team. We will present an overview and update to the Anti-Racism Policy Implementation Plan since our last presentation to the board on February 13th, 2020. We will share how the policy regulations were strategically outlined into a logic model design that had the built-in key activities, key strategies, and progressive outline measurable designs built in. In February, we presented examples of startup initiatives, such as kicking off a student advisory group, launching a review of the equity survey interviews, from teachers of color and steps to identify equity needs assessment tools. Today, what we would like to do is to provide you with some updates on those action steps to these startup initiatives that we presented back in February, as well as some other initiatives. Sorry about that. The anti-racism policy statement is being promoted as part of our division level staff meetings. It is presented with equal value. to our mission and vision statement, our equity mission statement, and the strategic priorities, values, and goals of our school division. And it's presented as a top priority message to staff and the community. It reads, Albemarle County Public Schools is committed to establishing and sustaining an equitable community that achieves the school division's equity mission to end the predictive value of race and ensure each individual student's and staff's success. Yes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\ngrahampaige: That's not appearing. The slide's a little bit off. OK, there it is. It's there now. OK.<|eot_id|>\""
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "e17b7e3f-2cd2-4484-8031-724bbde68d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 grahampaige: That's not appearing. The slide's a little bit off. OK, there it is. It's there now. OK.<|eot_id|>\""
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "6b0938c3-96d6-4fd8-8009-64422e5c6406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 766 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 12,156,928/3,000,000,000 (0.41% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 01:06, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.823400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.542900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.616700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.007100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.845700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.146200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.694100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.898700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.913900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.521600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.665400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.749600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.612100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.145600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.600300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.990300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.261700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.609800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.270000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.471900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.248600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.183600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.198000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.455600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.283300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.101000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.548700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.060900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.197000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.054100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.144300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.031400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.030700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.032700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.462400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.282500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.032300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.016400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.407600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.085000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.014600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.005900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.032800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.023400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.049400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.013700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.520800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.141500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.023500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.573100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.058300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.087400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.027100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.006700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "22ad7439-da1d-4866-bf8f-51e2483de625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': \"Speaker_26: So Mr. Page, I'll turn it back over to you. It sounds like we have consensus to move forward.\\ngrahampaige: Right, we do. So thank you for the out, and before I miss waters wicks leaves. This is just something I heard through the rumor mill so it may not necessarily be true, but I understand that she is only that one of her main incentives for doing this for us is that she expects some new cars over. at Albemarle High School. So we might have to look at some of those one-time funds for her new cars. I'm not sure really what it is. But that's what the rumor mill was saying. I think it's one per high school, Mr. Page.\\nSpeaker_1: One per high school.\\ngrahampaige: Oh, OK. I thought that was only at Albemarle. But OK, one per high school. So Monticello Western and Albemarle. Yes, sir. Thank you, Ms. Watters-Wicks. We'll see what we can do.\\nSpeaker_1: OK. I think the students would like really fast ones.\",\n",
       "  'role': 'user'},\n",
       " {'content': 'grahampaige: OK. All right. Thank you. Okay, our next item is the 2022 work session topics. And this will be led by Mishma.',\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['conversations'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "8f52e9bb-8aac-45e3-89a0-539d7d9fbb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK. I think the students would like really fast ones.\n",
      "\n",
      "OK. I think the students would like really fast ones.\n",
      "OK. I think the students would like really fast ones.\n",
      "\n",
      " OK. I think the students would like really fast ones.\n",
      " \n",
      "OK. I think the students would like really fast ones.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Speaker_26: So Mr. Page, I'll turn it back over to you. It sounds like we have consensus to move forward.\\ngrahampaige: Right, we do. So thank you for the out, and before I miss waters wicks leaves. This is just something I heard through the rumor mill so it may not necessarily be true, but I understand that she is only that one of her main incentives for doing this for us is that she expects some new cars over. at Albemarle High School. So we might have to look at some of those one-time funds for her new cars. I'm not sure really what it is. But that's what the rumor mill was saying. I think it's one per high school, Mr. Page.\\nSpeaker_1: One per high school.\\ngrahampaige: Oh, OK. I thought that was only at Albemarle. But OK, one per high school. So Monticello Western and Albemarle. Yes, sir. Thank you, Ms. Watters-Wicks. We'll see what we can do.\\nSpeaker_1: OK. I think the students would like really fast ones.\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "62fc7053-05ec-4abd-ad78-4768b85100a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape: torch.Size([1, 278])\n",
      "Token IDs: [128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 5887, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 83136, 62, 1627, 25, 2100, 4491, 13, 5874, 11, 358, 3358, 2543, 433, 1203, 927, 311, 499, 13, 1102, 10578, 1093, 584, 617, 24811, 311, 3351, 4741, 627, 911, 1494, 23465, 7404, 25, 10291, 11, 584, 656, 13, 2100, 9901, 499, 369, 279, 704, 11, 323, 1603, 358, 3194, 21160, 289, 5908, 11141, 13, 1115, 374, 1120, 2555, 358, 6755, 1555, 279, 59001, 2606, 779, 433, 1253, 539, 14647, 387, 837, 11, 719, 358, 3619, 430, 1364, 374, 1193, 430, 832, 315, 1077, 1925, 36580, 369, 3815, 420, 369, 603, 374, 430, 1364, 25283, 1063, 502, 9515, 927, 13, 520, 32672, 336, 277, 273, 5234, 6150, 13, 2100, 584, 2643, 617, 311, 1427, 520, 1063, 315, 1884, 832, 7394, 10736, 369, 1077, 502, 9515, 13, 358, 2846, 539, 2771, 2216, 1148, 433, 374, 13, 2030, 430, 596, 1148, 279, 59001, 2606, 574, 5605, 13, 358, 1781, 433, 596, 832, 824, 1579, 2978, 11, 4491, 13, 5874, 627, 83136, 62, 16, 25, 3861, 824, 1579, 2978, 627, 911, 1494, 23465, 7404, 25, 8840, 11, 10619, 13, 358, 3463, 430, 574, 1193, 520, 32672, 336, 277, 273, 13, 2030, 10619, 11, 832, 824, 1579, 2978, 13, 2100, 9995, 292, 4896, 11104, 323, 32672, 336, 277, 273, 13, 7566, 11, 28146, 13, 9930, 499, 11, 16450, 13, 468, 10385, 13299, 5908, 13, 1226, 3358, 1518, 1148, 584, 649, 656, 627, 83136, 62, 16, 25, 10619, 13, 358, 1781, 279, 4236, 1053, 1093, 2216, 5043, 6305, 13, 128009, 128006, 78191, 128007, 271]\n",
      "Decoded Text: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Speaker_26: So Mr. Page, I'll turn it back over to you. It sounds like we have consensus to move forward.\n",
      "grahampaige: Right, we do. So thank you for the out, and before I miss waters wicks leaves. This is just something I heard through the rumor mill so it may not necessarily be true, but I understand that she is only that one of her main incentives for doing this for us is that she expects some new cars over. at Albemarle High School. So we might have to look at some of those one-time funds for her new cars. I'm not sure really what it is. But that's what the rumor mill was saying. I think it's one per high school, Mr. Page.\n",
      "Speaker_1: One per high school.\n",
      "grahampaige: Oh, OK. I thought that was only at Albemarle. But OK, one per high school. So Monticello Western and Albemarle. Yes, sir. Thank you, Ms. Watters-Wicks. We'll see what we can do.\n",
      "Speaker_1: OK. I think the students would like really fast ones.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokens: ['<|begin_of_text|>', '<|start_header_id|>', 'system', '<|end_header_id|>', 'ĊĊ', 'Cut', 'ting', 'ĠKnowledge', 'ĠDate', ':', 'ĠDecember', 'Ġ', '202', '3', 'Ċ', 'Today', 'ĠDate', ':', 'Ġ', '26', 'ĠJuly', 'Ġ', '202', '4', 'ĊĊ', '<|eot_id|>', '<|start_header_id|>', 'user', '<|end_header_id|>', 'ĊĊ', 'Speaker', '_', '26', ':', 'ĠSo', 'ĠMr', '.', 'ĠPage', ',', 'ĠI', \"'ll\", 'Ġturn', 'Ġit', 'Ġback', 'Ġover', 'Ġto', 'Ġyou', '.', 'ĠIt', 'Ġsounds', 'Ġlike', 'Ġwe', 'Ġhave', 'Ġconsensus', 'Ġto', 'Ġmove', 'Ġforward', '.Ċ', 'gr', 'ah', 'ampa', 'ige', ':', 'ĠRight', ',', 'Ġwe', 'Ġdo', '.', 'ĠSo', 'Ġthank', 'Ġyou', 'Ġfor', 'Ġthe', 'Ġout', ',', 'Ġand', 'Ġbefore', 'ĠI', 'Ġmiss', 'Ġwaters', 'Ġw', 'icks', 'Ġleaves', '.', 'ĠThis', 'Ġis', 'Ġjust', 'Ġsomething', 'ĠI', 'Ġheard', 'Ġthrough', 'Ġthe', 'Ġrumor', 'Ġmill', 'Ġso', 'Ġit', 'Ġmay', 'Ġnot', 'Ġnecessarily', 'Ġbe', 'Ġtrue', ',', 'Ġbut', 'ĠI', 'Ġunderstand', 'Ġthat', 'Ġshe', 'Ġis', 'Ġonly', 'Ġthat', 'Ġone', 'Ġof', 'Ġher', 'Ġmain', 'Ġincentives', 'Ġfor', 'Ġdoing', 'Ġthis', 'Ġfor', 'Ġus', 'Ġis', 'Ġthat', 'Ġshe', 'Ġexpects', 'Ġsome', 'Ġnew', 'Ġcars', 'Ġover', '.', 'Ġat', 'ĠAlb', 'em', 'ar', 'le', 'ĠHigh', 'ĠSchool', '.', 'ĠSo', 'Ġwe', 'Ġmight', 'Ġhave', 'Ġto', 'Ġlook', 'Ġat', 'Ġsome', 'Ġof', 'Ġthose', 'Ġone', '-time', 'Ġfunds', 'Ġfor', 'Ġher', 'Ġnew', 'Ġcars', '.', 'ĠI', \"'m\", 'Ġnot', 'Ġsure', 'Ġreally', 'Ġwhat', 'Ġit', 'Ġis', '.', 'ĠBut', 'Ġthat', \"'s\", 'Ġwhat', 'Ġthe', 'Ġrumor', 'Ġmill', 'Ġwas', 'Ġsaying', '.', 'ĠI', 'Ġthink', 'Ġit', \"'s\", 'Ġone', 'Ġper', 'Ġhigh', 'Ġschool', ',', 'ĠMr', '.', 'ĠPage', '.Ċ', 'Speaker', '_', '1', ':', 'ĠOne', 'Ġper', 'Ġhigh', 'Ġschool', '.Ċ', 'gr', 'ah', 'ampa', 'ige', ':', 'ĠOh', ',', 'ĠOK', '.', 'ĠI', 'Ġthought', 'Ġthat', 'Ġwas', 'Ġonly', 'Ġat', 'ĠAlb', 'em', 'ar', 'le', '.', 'ĠBut', 'ĠOK', ',', 'Ġone', 'Ġper', 'Ġhigh', 'Ġschool', '.', 'ĠSo', 'ĠMont', 'ic', 'ello', 'ĠWestern', 'Ġand', 'ĠAlb', 'em', 'ar', 'le', '.', 'ĠYes', ',', 'Ġsir', '.', 'ĠThank', 'Ġyou', ',', 'ĠMs', '.', 'ĠW', 'atters', '-W', 'icks', '.', 'ĠWe', \"'ll\", 'Ġsee', 'Ġwhat', 'Ġwe', 'Ġcan', 'Ġdo', '.Ċ', 'Speaker', '_', '1', ':', 'ĠOK', '.', 'ĠI', 'Ġthink', 'Ġthe', 'Ġstudents', 'Ġwould', 'Ġlike', 'Ġreally', 'Ġfast', 'Ġones', '.', '<|eot_id|>', '<|start_header_id|>', 'assistant', '<|end_header_id|>', 'ĊĊ']\n"
     ]
    }
   ],
   "source": [
    "# inputs is a tensor, shape: (batch_size, seq_length)\n",
    "print(\"Inputs shape:\", inputs.shape)\n",
    "\n",
    "# Get first example token IDs as a list\n",
    "input_ids_list = inputs[0].tolist()\n",
    "\n",
    "# Decode back to text\n",
    "decoded_text = tokenizer.decode(input_ids_list)\n",
    "\n",
    "print(\"Token IDs:\", input_ids_list)\n",
    "print(\"Decoded Text:\", decoded_text)\n",
    "\n",
    "# Show tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids_list)\n",
    "print(\"Tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "579dacff-134e-4b90-866c-5cf6d8da4b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 1 ---\n",
      "<|begin_of_text|><|begin_of_text|><|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Speaker_26: So Mr. Page, I'll turn it back over to you. It sounds like we have consensus to move forward.\n",
      "grahampaige: Right, we do. So thank you for the out, and before I miss waters wicks leaves. This is just something I heard through the rumor mill so it may not necessarily be true, but I understand that she is only that one of her main incentives for doing this for us is that she expects some new cars over. at Albemarle High School. So we might have to look at some of those one-time funds for her new cars. I'm not sure really what it is. But that's what the rumor mill was saying. I think it's one per high school, Mr. Page.\n",
      "Speaker_1: One per high school.\n",
      "grahampaige: Oh, OK. I thought that was only at Albemarle. But OK, one per high school. So Monticello Western and Albemarle. Yes, sir. Thank you, Ms. Watters-Wicks. We'll see what we can do.\n",
      "Speaker_1: OK. I think the students would like really fast ones.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "grahampaige: Grahampage!  grahampage!\n",
      " Speaker _3_: Aha ha\n",
      "speaker (_4_) : \n",
      " speaker _(5) ):\n",
      "   ahhaha ahahaha)a  \n",
      "    (speakr)(ers erer.) (\n",
      "     r)r )(ter \n",
      "\n",
      "no response from them...then someone speaks up...\n",
      "\n",
      "Mr P page, \"That’s all folks\" “ THAT'S ALL FOLKS”\n",
      "\n",
      "(There are also other characters in room )... they speak simultaneously when nobody has spoken yet..they start speaking.\n",
      "\n",
      "Aaakkaaa haaaaaaa:\n",
      "Heheeee-heeeee...\n",
      "Hoo-ooo-oouuuu..\n",
      "Zzzz-ZZZ-zhhhhr-hhmmmphmmppf!!\n",
      "Tee-tie-TIII-teiii-i!! Mrr-m-me-M-MMMe-e-emmme!!!??\n",
      "Bbaaabbb-bbiiibbiisss -sss-stttttaaiiinnt.....\n",
      "Kkk-kkikkik kakk\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import TextStreamer\n",
    "\n",
    "model.eval()\n",
    "streamer = TextStreamer(tokenizer)\n",
    "\n",
    "sample_inputs = [\n",
    "    # No <|begin_of_text|> here\n",
    "    \"\"\"<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Speaker_26: So Mr. Page, I'll turn it back over to you. It sounds like we have consensus to move forward.\n",
    "grahampaige: Right, we do. So thank you for the out, and before I miss waters wicks leaves. This is just something I heard through the rumor mill so it may not necessarily be true, but I understand that she is only that one of her main incentives for doing this for us is that she expects some new cars over. at Albemarle High School. So we might have to look at some of those one-time funds for her new cars. I'm not sure really what it is. But that's what the rumor mill was saying. I think it's one per high school, Mr. Page.\n",
    "Speaker_1: One per high school.\n",
    "grahampaige: Oh, OK. I thought that was only at Albemarle. But OK, one per high school. So Monticello Western and Albemarle. Yes, sir. Thank you, Ms. Watters-Wicks. We'll see what we can do.\n",
    "Speaker_1: OK. I think the students would like really fast ones.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\ngrahampaige:\"\"\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(sample_inputs):\n",
    "    print(f\"\\n--- Example {i + 1} ---\")\n",
    "\n",
    "    formatted_prompt = f\"<|begin_of_text|>{prompt}\"\n",
    "    inputs = tokenizer(\n",
    "        formatted_prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=4096,\n",
    "    )\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.9,\n",
    "            top_p=0.95,\n",
    "            repetition_penalty=2.1,\n",
    "            streamer=streamer,  # Comment this out if you want to capture the result\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9611214f-6112-4ff7-b619-7efa2a3bb037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed2ff51-e10b-44a5-bcd1-cc3d8089db53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "unsloth_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
