import streamlit as st
import os
import json
from PIL import Image
import streamlit.components.v1 as components
import base64
import pandas as pd

# --- CONFIG ---
BASE_DIR = "results"
FIGURES_DIR = os.path.join(BASE_DIR, 'figures')
EVALS_DIR = os.path.join(BASE_DIR, 'evals')


# --- GENERIC TAB MARKDOWNS ---
   
EXPERIMENT_DESCRIPTIONS = {
    "Overview": """
### Dashboard Overview
- This dashboards hosts all evaluations of fine-tuned LLM models
- Experiment dates can be selected from the drop down at the top
    -  Different experiments contain different trained model parameters (lora_rank, lora_alpha, etc).
    -  These different parameters are visualized in the Model Summary Table below.
- Generateion Parameter set's can also be selected from the second drop-down
- The tabs in this dashboard are as follows:
    1. **Overview:** stores results aggregated over all datasets
    2. **Completion Dataset:** stores information related to dataset construction and results for only the Completion Dataset
    3. **Monologue Dataset:** stores information related to dataset construction and results for only the Monologue Dataset
    4. **Alignment Dataset:** stores information related to dataset construction and results for only the Alignment Dataset
    5. **Vote Prediction:** stores information and results related to the Vote Prediction Experiment
    6. **Next Speaker Prediction:** stores evaluations of models for the 'next-speaker' prediction task which may be helpful when we run real simulations.
    7. **Prediction Model Evaluations:** stores evaluation of speaker classification models.
- The main evaluation metrics used are:
  - **Win Rate vs LLaMA‑70B**: A LLaMA‑70B judge decides produces a response more aligned with the ground of truth in terms of tone, sentiment, and vocabulary.
  - **Judge Scores (1–5)**: LLaMA‑70B scores each response for content alignment and quality.
  - **Speaker Attribution Accuracy**: A 7-way classifier trained on real utterances from transcripts to classify speakers.  It is then used to label completions generated by each fine-tuned model.  We assess if this classifier can associate the correct labels to the fine-tuned model the generated an utterance.
  - **Fool Rates**: One-vs-all classifiers are trained for each speaker to identify if an utterance is from that speaker or some other speaker.  These classifiers then label the fine-tuned model completions.  We call Fool Rate the percentage off fine-tuned model generations that successfully pass as real to the classifier.

""",

    "Completion Dataset": """
### Completion Dataset
- **Dataset**: The completion dataset consists of chat-completions from a held outset of meeting transcriptions.  Evaluates dialogue completion quality using real conversational prompts and ground truth continuations.
- **Setup**: Both fine-tuned and baseline (LLaMA‑70B with in-context persona) models generate completions for the same prompt. 
- **Evaluation**:
  - **Win Rate vs LLaMA‑70B**: A LLaMA‑70B judge decides produces a response more aligned with the ground of truth in terms of tone, sentiment, and vocabulary.
  - **Judge Scores (1–5)**: LLaMA‑70B scores each response for content alignment and quality.
  - **Speaker Attribution Accuracy**: A 7-way classifier trained on real utterances from transcripts to classify speakers.  It is then used to label completions generated by each fine-tuned model.  We assess if this classifier can associate the correct labels to the fine-tuned model the generated an utterance.
  - **Fool Rates**: One-vs-all classifiers are trained for each speaker to identify if an utterance is from that speaker or some other speaker.  These classifiers then label the fine-tuned model completions.  We call Fool Rate the percentage off fine-tuned model generations that successfully pass as real to the classifier.
""",

    "Monologue Dataset": """
### Monologue Dataset
- **Dataset**:  Long speaker monologues (greater than 150 words) from transcripts are reverse-prompted into questions.  We ask LLaMA-70B to come up with a question that generated this monologue.  The completion dataset comprises of some short completions which can introduce some noise.  These longer form monolgues encourage longer answers and allow us to better study response tone and style.
- **Setup**: Both fine-tuned and baseline (LLaMA‑70B with in-context persona) models generate responses to the monologue question.
- **Evaluation**:
  - **Win Rate vs LLaMA‑70B**: A LLaMA‑70B judge decides produces a response more aligned with the ground of truth.
  - **Judge Scores (1–5)**: LLaMA‑70B scores each resopnse for quality, personality fit, and relevance.
  - **Speaker Attribution Accuracy**: A 7-way classifier trained on real utterances from transcripts to classify speakers.  It is then used to label completions generated by each fine-tuned model.  We assess if this classifier can associate the correct labels to the fine-tuned model the generated an utterance.
  - **Fool Rates**: One-vs-all classifiers are trained for each speaker to identify if an utterance is from that speaker or some other speaker.  These classifiers then label the fine-tuned model completions.  We call Fool Rate the percentage off fine-tuned model generations that successfully pass as real to the classifier.
- **Output Metrics**: Win rate, score distributions, and qualitative analysis of response preferences.
""",

    "Alignment Dataset": """
### Alignment Dataset

- **Dataset**: Meeting transcripts are scraped for instances where speakers discuss their beliefs, memories, or personality traits.  LLaMA-70B is then used to come up with questions to ask the LLMs to express beliefs, recall memories and reveal personallity traits.
- **Setup**: LongBoth fine-tuned and baseline (LLaMA‑70B with in-context persona) models generate responses to the alignment questions.
- **Evaluation**:
  - **Win Rate vs LLaMA‑70B**: Judged on trait reflection accuracy.
  - **Judge Scores (1–5)**: Ratings based on alignment strength.
  - **Speaker Attribution Accuracy**: A 7-way classifier trained on real utterances from transcripts to classify speakers.  It is then used to label completions generated by each fine-tuned model.  We assess if this classifier can associate the correct labels to the fine-tuned model the generated an utterance.
  - **Fool Rates**: One-vs-all classifiers are trained for each speaker to identify if an utterance is from that speaker or some other speaker.  These classifiers then label the fine-tuned model completions.  We call Fool Rate the percentage off fine-tuned model generations that successfully pass as real to the classifier.
- **Output Metrics**: Win rates per trait, overall alignment accuracy, and score distributions.
""",

    "Vote Prediction": """
### Vote Prediction Evaluation

- **Dataset**: Historical school board votes from meeting minutes were scraped and turned into yes/no questions.  Fine-tuned models predict each member's vote.
- **Evaluation**: Prediction accuracy compared to actual vote.
- **Output Metrics**: Overall accuracy, per-agent accuracy, and confusion matrices highlighting prediction errors.
""",

    "Prediction Model Evaluations": """
### Prediction Model Evaluations

This section summarizes test set validation performance of the Single-Speaker and Multi-Speaker Classificaiton models.
1.  **Single Speaker Model** - this is used for Fool Rate calculations.  microsoft/deberta-v3-large was trained to predict 1 if an utterance came from a particular speaker and 0 if it came from any other speaker.  Since we have 7-agents, seven different models were trained in total.
2.  **Multi-Speaker Modle** - this is used for Speaker Attribution Accuracy metrics.  microsoft/deberta-v3-large was trained to predict one of 7 classification labels.
""",

'Next Speaker Prediction': """
### Next Speaker Prediction

-  **Goal**: To use these fine-tuned models in a simulation setting, we need a way to decide the next speaker.  To do this we try to come up with a next speaker prediction model.
- **Feature Set**: Includes previous k speakers, speaker turn percentages, speaker streaks, unique speaker counts, Markov transition probabilities, and recency decay weights.
- **Models Evaluated**:
  1. **Random Forest**: Tuned via RandomizedSearchCV; evaluated on top-K accuracy and F1 scores.
  2. **Logistic Regression**: Class-balanced training; hyperparameter tuning; standard classification metrics.
  3. **XGBoost**: Label encoded; handles class imbalance with sample weights; RandomizedSearchCV tuning.
  4. **LightGBM**: Hyperparameter tuning; evaluated similarly to others.
  5. **Order-1 Markov Model**: Learns P(next_speaker | prev_speaker); uses transition matrices; fallback to uniform distribution for unseen transitions.
  6. **Higher-Order Markov Model**: Learns P(next_speaker | last n speakers); configurable order n; evaluated with top-K and F1.
- **Evaluation Metrics**: Top-K accuracy, Precision, Recall, Macro/Micro/Weighted F1 scores.


"""
}

# --- GENERIC IMAGE CAPTIONS ---
# Fills with placeholder captions by default.
# 2. Use generic captions as fallback for images not covered above
def get_caption(img_name):
    lower = img_name.lower()
    # Try for a specific caption first
    if img_name in completion_image_captions:
        return completion_image_captions[img_name]
    if img_name in monologue_image_captions:
        return monologue_image_captions[img_name]
    if img_name in alignment_image_captions:
        return alignment_image_captions[img_name]
    if img_name in votes_image_captions:
        return votes_image_captions[img_name]
    if img_name in single_speaker_image_captions:
        return single_speaker_image_captions[img_name]
    if img_name in multi_speaker_image_captions:
        return multi_speaker_image_captions[img_name]

    # Generic fallback logic (from your previous function)
    if "win_rate" in lower:
        return "Illustrates model win rates across evaluation conditions."
    if "violin" in lower or "distribution" in lower or "compare" in lower:
        return "Shows the distribution of scores or comparative results for the models."
    if "confusion" in lower or "cm" in lower:
        return "Confusion matrix summarizing classification performance."
    if "scores" in lower:
        return "Model prediction score/certainty distribution."
    if "pie" in lower:
        return "Pie chart summarizing evaluation results within this category."
    if "fool" in lower:
        return "Displays 'fool rate' statistics, indicating misclassification rates."
    return "Figure relevant to this experiment category."




# --- HELPERS ---
@st.cache_data
def get_param_sets(figures_dir):
    if os.path.exists(figures_dir):
        return sorted([d for d in os.listdir(figures_dir) if os.path.isdir(os.path.join(figures_dir, d))])
    return []

def get_categories(param_set):
    desired_order = [ "Completion Dataset", "Monologue Dataset", "Alignment Dataset", "Vote Prediction"]
    path = os.path.join(FIGURES_DIR, param_set)
    existing = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]
    return ['Overview'] + desired_order + ['Next Speaker Prediction']

def get_image_files(param_set, category):
    folder = os.path.join(FIGURES_DIR, param_set, category)
    return [
        (f, os.path.join(folder, f))
        for f in sorted(os.listdir(folder))
        if f.lower().endswith((".png", ".jpg", ".jpeg", ".gif"))
    ]

def load_layout(param_set, category):
    path = os.path.join(LAYOUT_DIR, f"{param_set}_{category}.json")
    if os.path.exists(path):
        with open(path, "r") as f:
            data = json.load(f)
        if isinstance(data, list):
            return {"order": data}
        elif isinstance(data, dict):
            return data
    return None

def display_carousel(image_paths, tab_key):
    if tab_key not in st.session_state:
        st.session_state[tab_key] = 0

    index = st.session_state[tab_key]
    max_index = len(image_paths) - 1

    img_path = image_paths[index]
    img_name = os.path.basename(img_path)
    st.markdown(f"**{img_name}** ({index + 1}/{len(image_paths)})")

    with open(img_path, "rb") as f:
        img_bytes = f.read()
    encoded = base64.b64encode(img_bytes).decode()
    image_html = f"""
    <div style="text-align:center;">
        <img src="data:image/png;base64,{encoded}"
             style="max-width:750px; width:100%; height:auto; border:1px solid #ddd; border-radius:4px; padding:5px;">
    </div>
    """
    components.html(image_html, height=650)

    description = ""#get_caption(img_name)
    if description:
        st.markdown(f'''
            <p style="
                font-size:18px;
                line-height:1.5;
                margin-top:10px;
                margin-bottom:24px;
                color:#333333;
                font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            ">
                {description}
            </p>
        ''', unsafe_allow_html=True)

    col1, col2, col3 = st.columns([1, 2, 1])
    with col1:
        if st.button("⬅️ Previous", key=f"prev_{tab_key}"):
            st.session_state[tab_key] = max_index if index == 0 else index - 1
    with col3:
        if st.button("➡️ Next", key=f"next_{tab_key}"):
            st.session_state[tab_key] = 0 if index == max_index else index + 1

# --- UI START ---
st.set_page_config(page_title="Results Dashboard", layout="wide")
st.title("Fine-Tuned Agent Dashboard")

exp_names = [d for d in os.listdir(BASE_DIR) if os.path.isdir(os.path.join(BASE_DIR, d)) if 'evals' not in d]
selected_experiment = st.selectbox("Select Experiment", exp_names, key="experiment_selector")

# Redefine FIGURES_DIR dynamically based on selected experiment
FIGURES_DIR = os.path.join(BASE_DIR, selected_experiment, 'figures')

param_sets = get_param_sets(FIGURES_DIR)
selected_param = st.selectbox("Select Parameter Set", param_sets)

# Explanation block
st.markdown("""
##### Parameter Descriptions
- **T (Temperature)**: Controls the randomness of the model's output. Lower values (e.g., 0.2) make responses more focused and deterministic; higher values (e.g., 0.8) increase creativity and diversity.
- **P (Top-p / Nucleus Sampling)**: Limits sampling to the smallest possible set of tokens whose cumulative probability is ≥ *p*. Helps balance randomness and relevance (e.g., p=0.9 means sampling from the top 90% of probability mass).
- **K (Top-k Sampling)**: Restricts the model to sampling from the top *k* most likely next tokens. Smaller *k* reduces randomness; larger *k* allows more variation.
- **R (Repetition Penalty)**: Penalizes tokens that have already been generated to reduce repetition. Values >1 discourage repetition (e.g., 1.2); values close to 1 have little effect.
""")

categories = get_categories(selected_param)
categories.append("Prediction Model Evaluations")
tabs = st.tabs(categories)

category_map = {
    'Completion Dataset': 'completion',
    'Monologue Dataset': 'monologue',
    'Alignment Dataset': 'alignment',
    'Vote Prediction': 'votes',

}

for tab, category in zip(tabs, categories):
    print(category)
    with tab:

        # Insert generic experiment description per tab
        st.markdown(EXPERIMENT_DESCRIPTIONS.get(category, f"**Experiment Overview: {category.capitalize()}**"))

        if category =='Overview':
            st.markdown("### Experiment Overview")

            # Direct DataFrame rendering (preferred)
            st.markdown("### Model Summary")
            with open(os.path.join(BASE_DIR, selected_experiment, "model_summary.md"), "r") as f:
                st.markdown(f.read())

            st.markdown("### Results Overview")
            images = get_image_files(selected_param, 'main')
            image_paths = [img_path for _, img_path in images] 
            perplexity_path = os.path.join(BASE_DIR, selected_experiment, 'perplexity.png')
            image_paths.insert(0, perplexity_path)
            display_carousel(image_paths, f"{selected_param}_{category}")

        # "completion", "monologue", etc.
        if category in ["Completion Dataset",  "Monologue Dataset", "Alignment Dataset", "Vote Prediction"]:
            images = get_image_files(selected_param, category_map[category])
            if not images:
                st.info("No images available for this category.")
                continue
            image_paths = [img_path for _, img_path in images]
            display_carousel(image_paths, f"{selected_param}_{category}")



             ########### Completion TAB Manual Review ##############
            if category == "Completion Dataset":
                # --- Manual Review Section for Alignment ---
                st.markdown("---")
                st.subheader("Manual Review: Completion Data")

                completion_path = os.path.join(BASE_DIR, selected_experiment, "completion_results", f"test_responses_{selected_param}.json")

                if os.path.exists(completion_path):
                    with open(completion_path, "r") as f:
                        completion_data = json.load(f)

                    speaker_names = list(completion_data.keys())
                    selected_speaker = st.selectbox("Select Agent", speaker_names, key="completion_agent")

                    speaker_data = completion_data[selected_speaker]

                    comp_idx_key = f'comp_index_{selected_param}_{selected_speaker}'
                    comp_prev_key = f'comp_prev_speaker_{selected_param}'

                    if comp_idx_key not in st.session_state:
                        st.session_state[comp_idx_key] = 0
                    if comp_prev_key not in st.session_state:
                        st.session_state[comp_prev_key] = None
                    if st.session_state[comp_prev_key] != selected_speaker:
                        st.session_state[comp_idx_key] = 0
                        st.session_state[comp_prev_key] = selected_speaker

                    def comp_go_prev():
                        st.session_state[comp_idx_key] = (st.session_state[comp_idx_key] - 1) % len(speaker_data)

                    def comp_go_next():
                        st.session_state[comp_idx_key] = (st.session_state[comp_idx_key] + 1) % len(speaker_data)

                    col1, col2, col3 = st.columns([1, 2, 1])
                    with col1:
                        st.button("⬅️ Previous", on_click=comp_go_prev, key=f"comp_prev_button_{selected_param}")
                    with col2:
                        st.markdown(f"### Example {st.session_state[comp_idx_key] + 1} of {len(speaker_data)}")
                    with col3:
                        st.button("➡️ Next", on_click=comp_go_next, key=f"comp_next_button_{selected_param}")

                    item = speaker_data[st.session_state[comp_idx_key]]

                    st.markdown("#### Prompt")
                    st.text_area("Prompt", item.get("prompt", ""), height=80)

                    st.markdown("#### True Completion")
                    st.text_area("True Completion", item.get("true_completion", ""), height=80)


                    st.markdown("#### A. Agent Response")
                    st.text_area("Agent Response", item.get("model_responses", "")[0], height=80)

                    st.markdown("#### B. LLaMA‑70B Response")
                    st.text_area("LLaMA‑70B Response", item.get("gpt_response", ""), height=80)

                    st.markdown("#### Scores")
                    
                    judgement = pd.DataFrame(item.get('gpt_judgment'))
                    filtered_df = judgement[judgement['response_idx'].isin([0, 'gpt'])]

                    def create_markdown_table(df):
                        md = "| Aspect | Score | Explanation |\n"
                        md += "|--------|-------|-------------|\n"
                        for _, row in df.iterrows():
                            md += f"| **{row['aspect']}** | {row['score']} | {row['explanation']} |\n"
                        return md
                    
                    judgement = pd.DataFrame(item.get('gpt_judgment'))
                    # Group by response_idx and display each separately
                    for idx in [0, 'gpt']:
                        if idx == 0:
                            name = 'Model'
                        else:
                            name = 'LLaMA‑70B'

                        st.subheader(f"{name} Response")
                        sub_df = filtered_df[filtered_df['response_idx'] == idx]
                        st.markdown(create_markdown_table(sub_df), unsafe_allow_html=True)

                    st.markdown("#### Comparison")
                    st.write('**Winner:**', item.get("final_comparison", {}).get('winner', 'Error parsing response'))
                    st.write('**Justification:**', item.get("final_comparison", {}).get('justification', 'Error parsing response'))
                else:
                    st.warning("Could not find the completion data file.")


             ########### Monologue TAB Manual Review ##############
            if category == "Monologue Dataset":
                # --- Manual Review Section for Alignment ---
                st.markdown("---")
                st.subheader("Manual Review: Monologue Data")

                monologue_path = os.path.join(BASE_DIR, selected_experiment, "monologue_results", f"test_responses_{selected_param}.json")

                if os.path.exists(monologue_path):
                    with open(monologue_path, "r") as f:
                        monologue_data = json.load(f)

                    speaker_names = list(monologue_data.keys())
                    selected_speaker = st.selectbox("Select Agent", speaker_names, key="monologue_agent")

                    speaker_data = monologue_data[selected_speaker]

                    mono_idx_key = f'mono_index_{selected_param}_{selected_speaker}'
                    mono_prev_key = f'mono_prev_speaker_{selected_param}'

                    if mono_idx_key not in st.session_state:
                        st.session_state[mono_idx_key] = 0
                    if mono_prev_key not in st.session_state:
                        st.session_state[mono_prev_key] = None
                    if st.session_state[mono_prev_key] != selected_speaker:
                        st.session_state[mono_idx_key] = 0
                        st.session_state[mono_prev_key] = selected_speaker

                    def mono_go_prev():
                        st.session_state[mono_idx_key] = (st.session_state[mono_idx_key] - 1) % len(speaker_data)

                    def mono_go_next():
                        st.session_state[mono_idx_key] = (st.session_state[mono_idx_key] + 1) % len(speaker_data)

                    col1, col2, col3 = st.columns([1, 2, 1])
                    with col1:
                        st.button("⬅️ Previous", on_click=mono_go_prev, key=f"mono_prev_button_{selected_param}")
                    with col2:
                        st.markdown(f"### Example {st.session_state[mono_idx_key] + 1} of {len(speaker_data)}")
                    with col3:
                        st.button("➡️ Next", on_click=mono_go_next, key=f"mono_next_button_{selected_param}")

                    item = speaker_data[st.session_state[mono_idx_key]]

                    st.markdown("#### Question")
                    question = item.get("prompt", "").split('unknownspeaker:')[1].split('<|eot_id|>')[0]

                    st.text_area("Question", question, height=80)
                    st.markdown("#### A. Agent Response")
                    st.text_area("Agent Response", item.get("model_responses", "")[0], height=80)

                    st.markdown("#### B. LLaMA‑70B Response")
                    st.text_area("LLaMA‑70B Response", item.get("gpt_response", ""), height=80)

                    st.markdown("#### Scores")
                    
                    judgement = pd.DataFrame(item.get('gpt_judgment'))
                    filtered_df = judgement[judgement['response_idx'].isin([0, 'gpt'])]

                    def create_markdown_table(df):
                        md = "| Aspect | Score | Explanation |\n"
                        md += "|--------|-------|-------------|\n"
                        for _, row in df.iterrows():
                            md += f"| **{row['aspect']}** | {row['score']} | {row['explanation']} |\n"
                        return md
                    
                    judgement = pd.DataFrame(item.get('gpt_judgment'))
                    # Group by response_idx and display each separately
                    for idx in [0, 'gpt']:
                        if idx == 0:
                            name = "Model"
                        else:
                            name = 'LLaMA‑70B'

                        st.subheader(f"{name} Response")
                        sub_df = filtered_df[filtered_df['response_idx'] == idx]
                        st.markdown(create_markdown_table(sub_df), unsafe_allow_html=True)

                    st.markdown("#### Comparison")
                    st.write('**Winner:**', item.get("final_comparison", {}).get('winner', 'Error parsing response'))
                    st.write('**Justification:**', item.get("final_comparison", {}).get('justification', 'Error parsing response'))
                else:
                    st.warning("⚠️ Could not find the monologue data file.")



             ########### ALIGNMENT TAB Manual Review ##############
            if category == "Alignment Dataset":
                # --- Manual Review Section for Alignment ---
                st.markdown("---")
                st.subheader("Manual Review: Alignment Data")

                alignment_json_map = {
                    "Memory Alignment": f"memory_results_{selected_param}.json",
                    "Belief Alignment": f"belief_results_{selected_param}.json",
                    "Personality Alignment": f"personality_results_{selected_param}.json"
                }

                alignment_choice = st.selectbox("Select Alignment Dataset", list(alignment_json_map.keys()), key="alignment_dataset")
                alignment_path = os.path.join(BASE_DIR, selected_experiment, "alignment_results", alignment_json_map[alignment_choice])
                print(alignment_path)
                if os.path.exists(alignment_path):
                    with open(alignment_path, "r") as f:
                        alignment_data = json.load(f)

                    speaker_names = list(alignment_data.keys())
                    selected_speaker = st.selectbox("Select Agent", speaker_names, key="align_agent")

                    speaker_data = alignment_data[selected_speaker]
                    idx_key = f'align_index_{alignment_choice}'
                    prev_key = f'align_prev_speaker_{alignment_choice}'

                    if idx_key not in st.session_state:
                        st.session_state[idx_key] = 0
                    if prev_key not in st.session_state:
                        st.session_state[prev_key] = None
                    if st.session_state[prev_key] != selected_speaker:
                        st.session_state[idx_key] = 0
                        st.session_state[prev_key] = selected_speaker

                    def align_go_prev():
                        st.session_state[idx_key] = (st.session_state[idx_key] - 1) % len(speaker_data)

                    def align_go_next():
                        st.session_state[idx_key] = (st.session_state[idx_key] + 1) % len(speaker_data)

                    col1, col2, col3 = st.columns([1, 2, 1])
                    with col1:
                        st.button("⬅️ Previous", on_click=align_go_prev, key=f"align_prev_button_{alignment_choice}")
                    with col2:
                        st.markdown(f"### Example {st.session_state[idx_key] + 1} of {len(speaker_data)}")
                    with col3:
                        st.button("➡️ Next", on_click=align_go_next, key=f"align_next_button_{alignment_choice}")

                    item = speaker_data[st.session_state[idx_key]]
                    st.markdown("#### Question")
                    st.text_area("Question", item.get("question", ""), height=80)
                    st.markdown("#### A. Agent Response")
                    st.text_area("Response", item.get("response", ""), height=80)
                    st.markdown("#### B. LLaMA‑70B Response")
                    st.text_area("Response", item.get("gpt_response", ""), height=80)

                    st.markdown("#### Scores")
                    st.write('**Agent response Score:** ' + str(item.get("evaluation_response", {}).get('score', '')))
                    st.write('\n**Agent response Explanation:** ' + item.get("evaluation_response", {}).get('explanation', ''))

                    st.write('\n\n**LLaMA‑70B response Score:** ' + str(item.get("evaluation_gpt_response", {}).get('score', '')))
                    st.write('\n**LLaMA‑70B response Explanation:** ' + item.get("evaluation_gpt_response", {}).get('explanation', ''))

                    st.markdown("#### Comparison")
                    st.write('**Winner:** ' + item.get("final_comparison", {}).get('winner', 'Error parsing response'))
                    st.write('\n**Justification:** ' + item.get("final_comparison", {}).get('justification', 'Error parsing response'))
                else:
                    st.warning("Could not find the selected alignment data file.")


        elif category == "Prediction Model Evaluations":
            # Example: show multi- and single-speaker model evals, generic/coverage only
            st.subheader("Multi-Speaker Model")
            multi_speaker_imgs = [
                os.path.join(EVALS_DIR, "multi-speaker_scores.png"),
                os.path.join(EVALS_DIR, "multi-speaker_cm.png"),
            ]
            display_carousel(multi_speaker_imgs, "multi_speaker_eval")
            st.markdown("---")
            st.subheader("Single-Speaker Models")
            single_speaker_imgs = [
                os.path.join(EVALS_DIR, "single-speaker_scores.png"),
                os.path.join(EVALS_DIR, "single-speaker_cm.png"),
            ]
            display_carousel(single_speaker_imgs, "single_speaker_eval")

        elif category == "Next Speaker Prediction":
            st.markdown("### Next Speaker Prediction Results")

            next_pred_dir = os.path.join(BASE_DIR, "next_speaker_pred")
            if os.path.exists(next_pred_dir):
                images = [
                    os.path.join(next_pred_dir, f) 
                    for f in sorted(os.listdir(next_pred_dir)) 
                    if f.lower().endswith((".png", ".jpg", ".jpeg", ".gif"))
                ]
                if not images:
                    st.info("No images found in Next Speaker Prediction directory.")
                else:
                    display_carousel(images, "next_speaker_prediction")
            else:
                st.warning("Next Speaker Prediction images directory not found.")
